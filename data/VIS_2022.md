# IEEE VIS 2022
## Papers
<table><tr><th>id</th><th>title</th><th>author</th><th>abstract</th><th>keywords</th><th>type</th><th>session</th><th>award</th><th>video</th><th>detail</th></tr><tr><td>1</td><td>Neuroknitting Beethoven</td><td>varvara guljajeva, Mar Canet Sola</td><td>NeuroKnitting Beethoven was developed to celebrate Ludwig van Beethoven’s 250th anniversary and to re-visit the composer’s classical compositions from an interdisciplinary viewpoint. Suddenly, the public could hear not only an artistic interpretation of the composition but also the musician&#x27;s emotional state, which has resulted in the movement of a circular knitting machine installation, visuals, and plotted pattern to the produced garment in real-time. It means the pianist’s (in Seoul’s performance, it was a monk) affective response to music was captured every second and memorized in the knitted textile pattern, which was sprayed on the yarn before being knitted. High attention level resulted in a dense pattern, and the knitting machine’s speed followed the meditation level. All these processes were real-time and took place simultaneously. Furthermore, the sound-responsive AI-generated visuals were created and displayed alongside the data visualization to accompany brain data visualizations.</td><td></td><td>Associated Event</td><td>VISAP Opening Reception (6:00pm-8:00pm) | -- Art Exhibit Location: Automobile Alley</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1047.html">link</a></td></tr><tr><td>2</td><td>At the Pump</td><td>Joseph Insley</td><td>“at the pump” is an observation of items left behind at various gas stations. The digital print, which measures 360” x 10”, consists of 110 photos collected over the course of about three and a half years. Along with the photos themselves are plots of associated data. This includes information related to when the photo was taken, as well as data related to the content of the image. The gas station is a common shared public space that many people move in and out of, often without giving a thought to those that are there before or after them. By focusing on these items that were left behind, we raise awareness that we are not here alone. And while we may not know or ever directly encounter these other people, our actions can leave an imprint on them, and the environment. The installation will also include the results of an #atthepump Twitter campaign.</td><td></td><td>Associated Event</td><td>VISAP Opening Reception (6:00pm-8:00pm) | -- Art Exhibit Location: Automobile Alley</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1031.html">link</a></td></tr><tr><td>3</td><td>Ray</td><td>weidi zhang</td><td>RAY provides a responsive art experience that re-interprets Rayograph (photogram) – a 20th Century cameraless image-making technique – in the perspective of Artificial Intelligent (AI) surveillance and the changing ontology of images. The system implements Image-to-Image Translation with Conditional Adversarial Networks and a computer vision system to translate human portraits into new images of Rayograph with semantic meanings, which are further developed algorithmically through visualizing in the aesthetics of light painting. RAY bridges intelligent visualization with cameraless photography Rayograph to engage audiences with an interactive poetic experience that conveys meanings.</td><td></td><td>Associated Event</td><td>VISAP Opening Reception (6:00pm-8:00pm) | -- Art Exhibit Location: Automobile Alley</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1078.html">link</a></td></tr><tr><td>4</td><td>Sifting Strands</td><td>Oskar Elek, Weston Mossman, Angus Forbes</td><td>Sifting Strands is an interdisciplinary art project which arose from a wider scientific collaboration between computational researchers and astrophysicists. Sifting Sands brings bio-inspired patterns rooted in optimal transport networks into the aesthetic visual realm. Through it we explore how compter-generated art and live audience can collaborate in the creative act. The core of this work is a generalized simulation of Physarum polycephalum which meaningfully reacts to music, video and scene depth information. The resulting computational graph is implemented in Touch Designer and has been adapted to live performances and interactive installations. Universally, our works bring their audience together by creating a mingling space around the computational artifact.</td><td></td><td>Associated Event</td><td>VISAP Opening Reception (6:00pm-8:00pm) | -- Art Exhibit Location: Automobile Alley</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1085.html">link</a></td></tr><tr><td>5</td><td>Presentation of Self in the Machine</td><td>RAY LC</td><td>The world has been driven apart by recent events, making long distance performative mingling difficult to achieve, especially those employing in-person collaboration between humans and machines. How shall we reclaim a tangible exchange between the US and other parts of the world that has presence and meaning, as opposed to impersonal virtual interactions? We created and choreographed and an art technology performance exchange that would allow viewers in Oklahoma City to immerse themselves in a collaborative narrative space between a dancer in the US and a robot arm in City University of Hong Kong&#x27;s Studio for Narrative Spaces. The performance can be shown in either online or offline form to audiences, who witness the narrative of a dancer and a robot who communicate with each other through movement, sometimes leading one another, sometimes frustrating each other, but always communicating as if each other are present to each other across a 12 hour divide.  We propose an evening 9pm performance in Oklahoma City presented by dancer Mizuho Kappa, remotely choregraphed by RAY LC, which is simultaneously streamed live in the morning 10am at the School of Creative Media in Hong Kong, where a robot arm moves in response to movements undertaken by Mizuho. The live stream from Hong Kong is also shared with Oklahoma City, enabling Mizuho and RAY to interactively alter the dance movements and choreography live through visual comparison with the robot in a collaborative digital space. The robot imitates Mizuho’s head using its hand and her body using its arm. The choreography follows Mizuho as she steps outside the digital realm of the virtual platform and into the physical stage, enticing the robot to dance with her. The robot starts with only block-like movements but eventually learns to mimic her with his body. Still he cannot run around or use hands like Mizuho, and eventually seeks help. Soon Mizuho begins performing actions that the arm is not capable of, such as jumping and lying flat on the ground, leading the robot to wonder on his own: is there also something I can do that the human cannot?</td><td></td><td>Associated Event</td><td>VISAP Opening Reception (6:00pm-8:00pm) | -- Art Exhibit Location: Automobile Alley</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1082.html">link</a></td></tr><tr><td>6</td><td>Affective Learning Objectives for Communicative Visualizations</td><td>Elsie Lee-Robbins, Eytan Adar</td><td>When designing communicative visualizations, we often focus on goals that seek to convey patterns, relations, or comparisons (cognitive learning objectives). We pay less attention to affective intents–those that seek to influence or leverage the audience&#x27;s opinions, attitudes, or values in some way. Affective objectives may range in outcomes from making the viewer care about the subject, strengthening a stance on an opinion, or leading them to take further action. Because such goals are often considered a violation of perceived ‘neutrality’ or are ‘political,’ designers may resist or be unable to describe these intents, let alone formalize them as learning objectives. While there are notable exceptions–such as advocacy visualizations or persuasive cartography–we find that visualization designers rarely acknowledge or formalize affective objectives. Through interviews with visualization designers, we expand on prior work on using learning objectives as a framework for describing and assessing communicative intent. Specifically, we extend and revise the framework to include a set of affective learning objectives. This structured taxonomy can help designers identify and declare their goals and compare and assess designs in a more principled way. Additionally, the taxonomy can enable external critique and analysis of visualizations. We illustrate the use of the taxonomy with a critical analysis of an affective visualization.</td><td></td><td>VIS Full Paper</td><td>VIS Opening (10:45am-11:03am)| Best Papers (11:03am-12:00pm)</td><td></td><td><a href="https://youtu.be/2MJlzAd9Ua0">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1240.html">link</a></td></tr><tr><td>7</td><td>Multiple Forecast Visualizations (MFVs): Trade-offs in Trust and Performance in Multiple COVID-19 Forecast Visualizations</td><td>Lace Padilla, Racquel Fygenson, Spencer C. Castro, Enrico Bertini</td><td>The prevalence of inadequate SARS-COV-2 (COVID-19) responses may indicate a lack of trust in forecasts and risk communication. However, no work has empirically tested how multiple forecast visualization choices impact trust and task-based performance. The three studies presented in this paper (N = 1299) examine how visualization choices impact trust in COVID-19 mortality forecasts and how they influence performance in a trend prediction task. These studies focus on line charts populated with real-time COVID-19 data that varied the number and color encoding of the forecasts and the presence of best/worst-case forecasts. The studies reveal that trust in COVID-19 forecast visualizations initially increases with the number of forecasts and then plateaus after 6-9 forecasts. However, participants were most trusting of visualizations that showed less visual information, including a 95% confidence interval, single forecast, and grayscale encoded forecasts. Participants maintained high trust in intervals labeled with 50% and 25% and did not proportionally scale their trust to the indicated interval size. Despite the high trust, the 95% CI condition was the most likely to evoke predictions that did not correspond with the actual COVID-19 trend. Qualitative analysis of participants’ strategies confirmed that many participants trusted both the simplistic visualizations and those with numerous forecasts. This work provides practical guides for how COVID-19 forecast visualizations influence trust, including recommendations for identifying the range where forecasts balance trade-offs between trust and task-based performance.</td><td></td><td>VIS Full Paper</td><td>VIS Opening (10:45am-11:03am)| Best Papers (11:03am-12:00pm)</td><td></td><td><a href="https://youtu.be/6vQJYh7O3lg">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1636.html">link</a></td></tr><tr><td>8</td><td>Uncertainty-Aware Multidimensional Scaling</td><td>David Hägele, Tim Krake, Daniel Weiskopf</td><td>We present an extension of multidimensional scaling (MDS) to uncertain data, facilitating uncertainty visualization of multidimensional data. Our approach uses local projection operators that map high-dimensional random vectors to low-dimensional space to formulate a generalized stress. In this way, our generic model supports arbitrary distributions and various stress types. We use our uncertainty-aware multidimensional scaling (UAMDS) concept to derive a formulation for the case of normally distributed random vectors and a squared stress. The resulting minimization problem is numerically solved via gradient descent. We complement UAMDS by additional visualization techniques that address the sensitivity and trustworthiness of dimensionality reduction under uncertainty. With several examples, we demonstrate the usefulness of our approach and the importance of uncertainty-aware techniques.</td><td></td><td>VIS Full Paper</td><td>VIS Opening (10:45am-11:03am)| Best Papers (11:03am-12:00pm)</td><td></td><td><a href="https://youtu.be/hMOzEQ1x7jI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1075.html">link</a></td></tr><tr><td>9</td><td>Exploring D3 Implementation Challenges on Stack Overflow</td><td>Leilani Battle, Danni Feng, Kelli Webber</td><td>Visualization languages help to standardize the process of designing effective visualizations, one of the most prominent being D3. However, few researchers have analyzed at scale how users incorporate these languages into existing visualization programming processes, i.e., implementation workflows. In this paper, we present a new method for evaluating visualization languages. Our method emphasizes the experiences of users as observed through the online communities that have sprouted to facilitate public discussion and support around visualization languages. We demonstrate our method by analyzing D3 implementation workflows and challenges discussed on Stack Overflow. Our results show how the visualization community may be limiting its understanding of users&#x27; visualization implementation challenges by ignoring the larger context in which languages such as D3 are used. Based on our findings, we suggest new research directions to enhance the user experience with visualization languages. All our data and code are available at: https://osf.io/fup48/.</td><td>Web mining, visualization language evaluation</td><td>VIS Short Paper</td><td>VIS Opening (10:45am-11:03am)| Best Papers (11:03am-12:00pm)</td><td></td><td><a href="https://youtu.be/RC97-giqs6M">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1097.html">link</a></td></tr><tr><td>10</td><td>KiriPhys: Exploring New Data Physicalization Opportunities</td><td>Foroozan Daneshzand, Charles Perin, Sheelagh Carpendale</td><td>We present KiriPhys, a new type of data physicalization based on kirigami, a traditional Japanese art form that uses
 paper-cutting. Within the kirigami possibilities, we investigate how different aspects of cutting patterns offer opportunities for mapping
 data to both independent and dependent physical variables. As a first step towards understanding the data physicalization opportunities
 in KiriPhys, we conducted a qualitative study in which 12 participants interacted with four KiriPhys examples. Our observations of how
 people interact with, understand, and respond to KiriPhys suggest that KiriPhys: 1) provides new opportunities for interactive, layered
 data exploration, 2) introduces elastic expansion as a new sensation that can reveal data, and 3) offers data mapping possibilities while
 providing a pleasurable experience that stimulates curiosity and engagement</td><td></td><td>VIS Full Paper</td><td>Visualization Opportunities</td><td></td><td><a href="https://youtu.be/nxg0FpRtrXk">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1467.html">link</a></td></tr><tr><td>11</td><td>Supporting Expressive and Faithful Pictorial Visualization Design with Visual Style Transfer</td><td>Yang Shi, Pei Liu, Siji Chen, Mengdi Sun, Nan Cao</td><td>Pictorial visualizations portray data with figurative messages and approximate the audience to the visualization. Previous research on pictorial visualizations has developed authoring tools or generation systems, but their methods are restricted to specific visualization types and templates. Instead, we propose to augment pictorial visualization authoring with visual style transfer, enabling a more extensible approach to visualization design. To explore this, our work presents Vistylist, a design support tool that disentangles the visual style of a source pictorial visualization from its content and transfers the visual style to one or more intended pictorial visualizations. We evaluated Vistylist through a survey of example pictorial visualizations, a controlled user study, and a series of expert interviews. The results of our evaluation indicated that Vistylist is useful for creating expressive and faithful pictorial visualizations.</td><td></td><td>VIS Full Paper</td><td>Visualization Opportunities</td><td></td><td><a href="https://youtu.be/008mZfvIO-E">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1197.html">link</a></td></tr><tr><td>12</td><td>Self-Supervised Color-Concept Association via Image Colorization</td><td>Ruizhen Hu, Ziqi Ye, Bin Chen, Oliver van Kaick, Hui Huang</td><td>The interpretation of colors in visualizations is facilitated when the assignments between colors and concepts in the visualizations match human’s expectations, implying that the colors can be interpreted in a semantic manner. However, manually creating a dataset of suitable associations between colors and concepts for use in visualizations is costly, as such associations would have to be collected from humans for a large variety of concepts. To address the challenge of collecting this data, we introduce a method to extract color-concept associations automatically from a set of concept images. While the state-of-the-art method extracts associations from data with supervised learning, we developed a self-supervised method based on colorization that does not require the preparation of ground truth color-concept associations. Our key insight is that a set of images of a concept should be sufficient for learning color-concept associations, since humans also learn to associate colors to concepts mainly from past visual input. Thus, we propose to use an automatic colorization method to extract statistical models of the color-concept associations that appear in concept images. Specifically, we take a colorization model pre-trained on ImageNet and fine-tune it on the set of images associated with a given concept, to predict pixel-wise probability distributions in Lab color space for the images. Then, we convert the predicted probability distributions into color ratings for a given color library and aggregate them for all the images of a concept to obtain the final color-concept associations. We evaluate our method using four different evaluation metrics and via a user study. Experiments show that, although the state-of-the-art method based on supervised learning with user-provided ratings is more effective at capturing relative associations, our self-supervised method obtains overall better results according to metrics like Earth Mover’s Distance (EMD) and Entropy Difference (ED), which are closer to human perception of color distributions.</td><td></td><td>VIS Full Paper</td><td>Visualization Opportunities</td><td></td><td><a href="https://youtu.be/AU9NoTlEh8U">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1221.html">link</a></td></tr><tr><td>13</td><td>Cultivating Visualization Literacy for Children through Curiosity and Play</td><td>S. Sandra Bae, Rishi Vanukuru, Ruhan Yang, Peter Gyory, Ran Zhou, Ellen Yi-Luen Do, Danielle Albers Szafir</td><td>Fostering data visualization literacy (DVL) as part of childhood education could lead to a more data literate society. However, most work in DVL for children relies on a more formal educational context (i.e., a teacher-led approach) that limits children&#x27;s engagement with data to classroom-based environments and, consequently, children&#x27;s ability to ask questions about and explore data on topics they find personally meaningful. We explore how a curiosity-driven, child-led approach can provide more agency to children when they are authoring data visualizations. This paper explores how informal learning with crafting physicalizations through play and curiosity may foster increased literacy and engagement with data. Employing a constructionist approach, we designed a do-it-yourself toolkit made out of everyday materials (e.g., paper, cardboard, mirrors) that enables children to create, customize, and personalize three different interactive visualizations (bar, line, pie). We used the toolkit as a design probe in a series of in-person workshops with 5 children (6 to 11-year-olds) and interviews with 5 educators. Our observations reveal that the toolkit helped children creatively engage and interact with visualizations. Children with prior knowledge of data visualization reported the toolkit serving as more of an authoring tool that they envision using in their daily lives, while children with little to no experience found the toolkit as an engaging introduction to data visualization. Our study demonstrates the potential of using the constructionist approach to cultivate children&#x27;s DVL through curiosity and play.</td><td></td><td>VIS Full Paper</td><td>Visualization Opportunities</td><td></td><td><a href="https://youtu.be/wo0mK3qOQzI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1616.html">link</a></td></tr><tr><td>14</td><td>Roboviz: A Game-Centered Project for Information Visualization Education</td><td>Eytan Adar, Elsie Lee-Robbins</td><td>Due to their pedagogical advantages, large final projects in information visualization courses have become standard practice. Students take on a client--real or simulated--a dataset, and a vague set of goals to create a complete visualization or visual analytics product. Unfortunately, many projects suffer from ambiguous goals, over or under-constrained client expectations, and data constraints that have students spending their time on non-visualization problems (e.g., data cleaning). These are important skills, but are often secondary course objectives, and unforeseen problems can majorly hinder students. We created an alternative for our information visualization course: Roboviz, a real-time game for students to play by building a visualization-focused interface. By designing the game mechanics around four different data types, the project allows students to create a wide array of interactive visualizations. Student teams play against their classmates with the objective to collect the most (good) robots. The flexibility of the strategies encourages variability, a range of approaches, and solving wicked design constraints. We describe the construction of this game and report on student projects over two years. We further show how the game mechanics can be extended or adapted to other game-based projects.</td><td></td><td>VIS Full Paper</td><td>Visualization Opportunities</td><td></td><td><a href="https://youtu.be/8SrT3nkfjn4">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1237.html">link</a></td></tr><tr><td>15</td><td>Relaxed Dot Plots: Faithful Visualization of Samples and Their Distribution</td><td>Nils Rodrigues, Christoph Schulz, Sören Döring, Daniel Baumgartner, Tim Krake, Daniel Weiskopf</td><td>We introduce relaxed dot plots as an improvement of nonlinear dot plots for unit visualization. Our plots produce more faithful data representations and reduce moiré effects. Their contour is based on a customized kernel frequency estimation to match the shape of the distribution of underlying data values. Previous nonlinear layouts introduce column-centric nonlinear scaling of dot diameters for visualization of high-dynamic-range data with high peaks. We provide a mathematical approach to convert that column-centric scaling to our smooth envelope shape. This formalism allows us to use linear, root, and logarithmic scaling to find ideal dot sizes. Our method iteratively relaxes the dot layout for more correct and aesthetically pleasing results. To achieve this, we modified Lloyd&#x27;s algorithm with additional constraints and heuristics. We evaluate the layouts of relaxed dot plots against a previously existing nonlinear variant and show that our algorithm produces less error regarding the underlying data while establishing the blue noise property that works against moiré effects. Further, we analyze the readability of our relaxed plots in three crowd-sourced experiments. The results indicate that our proposed technique surpasses traditional dot plots.</td><td></td><td>VIS Full Paper</td><td>Visualization Opportunities</td><td></td><td><a href="https://youtu.be/7D8MsG7t3CA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1621.html">link</a></td></tr><tr><td>16</td><td>Multivariate Probabilistic Range Queries for Scalable Interactive 3D Visualization</td><td>Amani Waleed Ageeli, Alberto Jaspe-Villanueva, Ronell Sicat, Florian Mannuss, Peter Rautek, Markus Hadwiger</td><td>Large-scale scientific data, such as weather and climate simulations, often comprise a large number of attributes for each data sample, like temperature, pressure, humidity, and many more. Interactive visualization and analysis require filtering according to any desired combination of attributes, in particular logical AND operations, which is challenging for large data and many attributes. Many general data structures for this problem are built for and scale with a fixed number of attributes, and scalability of joint queries with arbitrary attribute subsets remains a significant problem. We propose a flexible probabilistic framework for multivariate range queries that decouples all attribute dimensions via projection, allowing any subset of attributes to be queried with full efficiency. Moreover, our approach is output-sensitive, mainly scaling with the cardinality of the query result rather than with the input data size. This is particularly important for joint attribute queries, where the query output is usually much smaller than the whole data set. Additionally, our approach can split query evaluation between user interaction and rendering, achieving much better scalability for interactive visualization than the previous state of the art. Furthermore, even when a multi-resolution strategy is used for visualization, queries are jointly evaluated at the finest data granularity, because our framework does not limit query accuracy to a fixed spatial subdivision.</td><td></td><td>VIS Full Paper</td><td>Dealing with Scale, Space, and Dimension</td><td></td><td><a href="https://youtu.be/yrQRYjA_NTg">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1406.html">link</a></td></tr><tr><td>17</td><td>Dual Space Coupling Model Guided Overlap-free Scatterplot</td><td>zeyu li, RuiZhi Shi, Yan Liu, Shizhuo Long, Ziheng Guo, Shichao Jia, Jiawan Zhang</td><td>The overdraw problem of scatterplots seriously interferes with the visual tasks. Existing methods, such as data sampling, node dispersion, subspace mapping, and visual abstraction, cannot guarantee the correspondence and consistency between the data points that reflect the intrinsic original data distribution and the corresponding visual units that reveal the presented data distribution, thus failing to obtain an overlap-free scatterplot with unbiased and lossless data distribution. A dual space coupling model is proposed in this paper to represent the complex bilateral relationship between data space and visual space theoretically and analytically. Under the guidance of the model, an overlap-free scatterplot method is developed through integration of the following: a geometry-based data transformation algorithm, namely DistributionTranscriptor; an efficient spatial mutual exclusion guided view transformation algorithm, namely PolarPacking; an overlap-free oriented visual encoding configuration model and a radius adjustment tool, namely $f_{r_{draw}}$. Our method can ensure complete and accurate information transfer between the two spaces, maintaining consistency between the newly created scatterplot and the original data distribution on global and local features. Quantitative evaluation proves our remarkable progress on computational efficiency compared with the state-of-the-art methods. Three applications involving pattern enhancement, interaction improvement, and overdraw mitigation of trajectory visualization demonstrate the broad prospects of our method.</td><td></td><td>VIS Full Paper</td><td>Dealing with Scale, Space, and Dimension</td><td></td><td><a href="https://youtu.be/ESQAl7o6BnE">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1394.html">link</a></td></tr><tr><td>18</td><td>Measuring Effects of Spatial Visualization and Domain on Visualization Task Performance: A Comparative Study</td><td>Sara Tandon, Alfie Abdul-Rahman, Rita Borgo</td><td>Understanding one’s audience is foundational to creating high impact visualization designs. However, individual differences and cognitive abilities influence interactions with information visualization. Different user needs and abilities suggest that an individual’s background could influence cognitive performance and interactions with visuals in a systematic way. This study builds on current research in domain-specific visualization and cognition to address if domain and spatial visualization ability combine to affect performance on information visualization tasks. We measure spatial visualization and visual task performance between those with tertiary education and professional profile in business, law &amp; political science, and math &amp; computer science. We conducted an online study with 90 participants using an established psychometric test to assess spatial visualization ability, and bar chart layouts rotated along Cartesian and polar coordinates to assess performance on spatially rotated data. Accuracy and response times varied with domain across chart types and task difficulty. We found that accuracy and time correlate with spatial visualization level, and education in math &amp; computer science can indicate higher spatial visualization. Additionally, we found that motivational differences between domains could contribute to increased levels of accuracy. Our findings indicate discipline not only affects user needs and interactions with data visualization, but also cognitive traits. Our results can advance inclusive practices in visualization design and add to knowledge in domain-specific visual research that can empower designers across disciplines to create effective visualizations.</td><td></td><td>VIS Full Paper</td><td>Dealing with Scale, Space, and Dimension</td><td></td><td><a href="https://youtu.be/iQVkv1GjArs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1124.html">link</a></td></tr><tr><td>19</td><td>Local Latent Representation based on Geometric Convolution for Particle Data Feature Exploration</td><td>Haoyu Li, Han-Wei Shen</td><td>Feature related particle data analysis plays an important role in many scientific applications such as fluid simulations, cosmology simulations and molecular dynamics. Compared to conventional methods that use hand-crafted feature descriptors, some recent studies focus on transforming the data into a new latent space, where features are easier to be identified, compared and extracted. However, it is challenging to transform particle data into latent representations, since the convolution neural networks used in prior studies require the data presented in regular grids. In this paper, we adopt Geometric Convolution, a neural network building block designed for 3D point clouds, to create latent representations for scientific particle data. These latent representations capture both the particle positions and their physical attributes in the local neighborhood so that features can be extracted by clustering in the latent space, and tracked by applying tracking algorithms such as mean-shift. We validate the extracted features and tracking results from our approach using datasets from three applications and show that they are comparable to the methods that define hand-crafted features for each specific dataset.</td><td>Data transformation, Particle data, Feature extraction and tracking, Deep learning</td><td>VIS Full Paper</td><td>Dealing with Scale, Space, and Dimension</td><td></td><td><a href="https://youtu.be/6n-a8bzRtVY">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9735308.html">link</a></td></tr><tr><td>20</td><td>Graphical Enhancements for Effective Exemplar Identification in Contextual Data Visualizations</td><td>Xinyu Zhang, Shenghui Cheng, Klaus Mueller</td><td>An exemplar is an entity that represents a desirable instance in a multi-attribute configuration space. It offers certain strengths in some of its attributes without unduly compromising the strengths in other attributes. Exemplars are frequently sought after in real life applications, such as systems engineering, investment banking, drug advisory, product marketing and many others. We study a specific method for the visualization of multi-attribute configuration spaces, the Data Context Map (DCM), for its capacity in enabling users to identify proper exemplars. The DCM produces a 2D embedding where users can view the data objects in the context of the data attributes. We ask whether certain graphical enhancements can aid users to gain a better understanding of the attribute-wise tradeoffs and so select better exemplar sets.  We conducted several user studies for three different graphical designs, namely iso-contour, value-shaded topographic rendering and terrain topographic rendering, and compare these with a baseline DCM display. As a benchmark we use an exemplar set generated via Pareto optimization which has similar goals but unlike humans can operate in the native high-dimensional data space. Our study finds that the two topographic maps are statistically superior to both the iso-contour and the DCM baseline display.</td><td>High-dimensional data, multivariate data, contextual displays, exemplar generation, decision support, configuration space</td><td>VIS Full Paper</td><td>Dealing with Scale, Space, and Dimension</td><td></td><td><a href="https://youtu.be/VOHaFonlMiQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9765327.html">link</a></td></tr><tr><td>21</td><td>Effectiveness Error: Measuring and Improving RadViz Visual Effectiveness</td><td>Marco Angelini, Graziano Blasilli, Simone Lenti, Alessia Palleschi, Giuseppe Santucci</td><td>RadViz contributes to multidimensional analysis by using 2D points for encoding data elements and interpreting them alongthe original data dimensions. For these characteristics it is used in different application domains, like clustering, anomaly detection, and software visualization. However, it is likely that using the dimension arrangement that comes with the data will produce a plot that leads users to make inaccurate conclusions about points values and data distribution. This paper attacks this problem without altering the original RadViz design: it defines, for both a single point and a set of points, the metric of effectiveness error, and uses it to define the objective function of a dimension arrangement strategy, arguing that minimizing it increases the overall RadViz visual quality. This paper investigated the intuition that reducing the effectiveness error is beneficial for other well-known RadViz problems, like points clumping toward the center, many-to-one plotting of non-proportional points, and cluster separation. It presents an algorithm that reduces to zero the effectiveness error for a single point and a heuristic that approximates the dimension arrangement minimizing the effectiveness error for an arbitrary set of points. A set of experiments based on 21 real datasets has been performed, with the goals of analyzing the advantages of reducing the effectiveness error, comparing the proposed dimension arrangement strategy with other related proposals, and investigating the heuristic accuracy. The Effectiveness Error metric, the algorithm, and the heuristic presented in this paper have been made available in a d3.jsplugin at  https://aware-diag-sapienza.github.io/d3-radviz.</td><td>Dimensionality reduction, RadViz, dimension arrangement, visual quality metrics.</td><td>VIS Full Paper</td><td>Dealing with Scale, Space, and Dimension</td><td></td><td><a href="https://youtu.be/FO932FPZseM">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9514468.html">link</a></td></tr><tr><td>22</td><td>A Unified Understanding of Deep NLP Models for Text Classification</td><td>Zhen Li, Xiting Wang, Weikai Yang, Jing Wu, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun, Hui Zhang, Shixia Liu</td><td>The rapid development of deep natural language processing (NLP) models for text classification has led to an urgent need for a unified understanding of these models proposed individually. Existing methods cannot meet the need for understanding different models in one framework due to the lack of a unified measure for explaining both low-level (e.g., words) and high-level (e.g., phrases) features. We have developed a visual analysis tool, DeepNLPVis, to enable a unified understanding of NLP models for text classification. The key idea is a mutual information-based measure, which provides quantitative explanations on how each layer of a model maintains the information of input words in a sample. We model the intra- and inter-word information at each layer measuring the importance of a word to the final prediction as well as the relationships between words, such as the formation of phrases. A multi-level visualization, which consists of a corpus-level, a sample-level, and a word-level visualization, supports the analysis from the overall training set to individual samples. Two case studies on classification tasks and comparison between models demonstrate that DeepNLPVis can help users effectively identify potential problems caused by samples and model architectures and then make informed improvements.</td><td>Explainable AI, visual debugging, visual analytics, deep NLP model, information-based interpretation</td><td>VIS Full Paper</td><td>Text, Language, and Image Data</td><td></td><td><a href="https://youtu.be/92gSgHp4jS4">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9801603.html">link</a></td></tr><tr><td>23</td><td>EBBE-Text: Explaining Neural Networks by Exploring Text Classification Decision Boundaries</td><td>Alexis Delaforge, Jérôme Azé, Sandra Bringay, Caroline Mollevi, Arnaud Sallaberry, Maximilien Servajean</td><td>While neural networks (NN) have been successfully applied to many NLP tasks, the way they function is often difficult to interpret. In this article, we focus on binary text classification via NNs and propose a new tool, which includes a visualization of the decision boundary and the distances of data elements to this boundary. This tool increases the interpretability of NN. Our approach uses two innovative views: (1) an overview of the text representation space and (2) a local view allowing data exploration around the decision boundary for various localities of this representation space. These views are integrated into a visual platform, EBBE-Text, which also contains state-of-the-art visualizations of NN representation spaces and several kinds of information obtained from the classification process. The various views are linked through numerous interactive functionalities that enable easy exploration of texts and classification results via the various complementary views. A user study shows the effectiveness of the visual encoding and a case study illustrates the benefits of using our tool for the analysis of the classifications obtained with several recent NNs and two datasets.</td><td>Artificial neural networks, Data visualization, Computational modeling, Natural language processing, Predictive models, Task analysis, Deep learning, Binary text classification, decision boundary, deep learning, interpretability, neural networks, representation space, visual analytics</td><td>VIS Full Paper</td><td>Text, Language, and Image Data</td><td></td><td><a href="https://youtu.be/xYETETj4WJI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9801527.html">link</a></td></tr><tr><td>24</td><td>LegalVis: Exploring and Inferring Precedent Citations in Legal Documents</td><td>Lucas E. Resck, Jean R. Ponciano, Luis Gustavo Nonato, Jorge Poco</td><td>To reduce the number of pending cases and conflicting rulings in the Brazilian Judiciary, the National Congress amended the Constitution, allowing the Brazilian Supreme Court (STF) to create binding precedents (BPs), i.e., a set of understandings that both Executive and lower Judiciary branches must follow. The STF&#x27;s justices frequently cite the 58 existing BPs in their decisions, and it is of primary relevance that judicial experts could identify and analyze such citations. To assist in this problem, we propose LegalVis, a web-based visual analytics system designed to support the analysis of legal documents that cite or could potentially cite a BP. We model the problem of identifying potential citations (i.e., non-explicit) as a classification problem. However, a simple score is not enough to explain the results; that is why we use an interpretability machine learning method to explain the reason behind each identified citation. For a compelling visual exploration of documents and BPs, LegalVis comprises three interactive visual components: the first presents an overview of the data showing temporal patterns, the second allows filtering and grouping relevant documents by topic, and the last one shows a document&#x27;s text aiming to interpret the model&#x27;s output by pointing out which paragraphs are likely to mention the BP, even if not explicitly specified. We evaluated our identification model and obtained an accuracy of 96%; we also made a quantitative and qualitative analysis of the results. The usefulness and effectiveness of LegalVis were evaluated through two usage scenarios and feedback from six domain experts.</td><td>Legal Documents, Visual Analytics, Brazilian Legal System, Natural Language Processing</td><td>VIS Full Paper</td><td>Text, Language, and Image Data</td><td></td><td><a href="https://youtu.be/JyAi8dTUqCM">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9716779.html">link</a></td></tr><tr><td>25</td><td>DeHumor: Visual Analytics for Decomposing Humor</td><td>Xingbo Wang, Yao Ming, Tongshuang Wu, Haipeng Zeng, Yong Wang, Huamin Qu</td><td>Despite being a critical communication skill, grasping humor is challenging—a successful use of humor requires a mixture of both engaging content build-up and an appropriate vocal delivery (e.g., pause). Prior studies on computational humor emphasize the textual and audio features immediately next to the punchline, yet overlooking longer-term context setup. Moreover, the theories are usually too abstract for understanding each concrete humor snippet. To fill in the gap, we develop DeHumor, a visual analytical system for analyzing humorous behaviors in public speaking. To intuitively reveal the building blocks of each concrete example, DeHumor decomposes each humorous video into multimodal features and provides inline annotations of them on the video script. In particular, to better capture the build-ups, we introduce content repetition as a complement to features introduced in theories of computational humor and visualize them in a context linking graph. To help users locate the punchlines that have the desired features to learn, we summarize the content (with keywords) and humor feature statistics on an augmented time matrix. With case studies on stand-up comedy shows and TED talks, we show that DeHumor is able to highlight various building blocks of humor examples. In addition, expert interviews with communication coaches and humor researchers demonstrate the effectiveness of DeHumor for multimodal humor analysis of speech content and vocal delivery.</td><td>Humor, Context, Multimodal Features, Visualization</td><td>VIS Full Paper</td><td>Text, Language, and Image Data</td><td></td><td><a href="https://youtu.be/DNkXtGCHMbI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9488285.html">link</a></td></tr><tr><td>26</td><td>Interactive and visual prompt engineering for ad-hoc task adaptation with large language models</td><td>Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin Hoover, Johanna Beyer, Hanspeter Pfister, Alexander Rush</td><td>State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases.</td><td></td><td>VIS Full Paper</td><td>Text, Language, and Image Data</td><td></td><td><a href="https://youtu.be/gmi6Coca9OI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1651.html">link</a></td></tr><tr><td>27</td><td>A Hybrid In Situ Approach for Cost Efficient Image Database Generation</td><td>Valentin Bruder, Matthew Larsen, Thomas Ertl, Hank Childs, Steffen Frey</td><td>The visualization of results while the simulation is running is increasingly common in extreme scale computing environments. We present a novel approach for in situ generation of image databases to achieve cost savings on supercomputers. Our approach, a hybrid between traditional inline and in transit techniques, dynamically distributes visualization tasks between simulation nodes and visualization nodes, using probing as a basis to estimate rendering cost. Our hybrid design differs from previous works in that it creates opportunities to minimize idle time from four fundamental types of inefficiency: variability, limited scalability, overhead, and rightsizing. We demonstrate our results by comparing our method against both inline and in transit methods for a variety of configurations, including two simulation codes and a scaling study that goes above 19K cores. Our findings show that our approach is superior in many configurations. As in situ visualization becomes increasingly ubiquitous, we believe our technique could lead to significant amounts of reclaimed cycles on supercomputers.</td><td>Visualization, High performance computing, In situ</td><td>VIS Full Paper</td><td>Text, Language, and Image Data</td><td></td><td><a href="https://youtu.be/GqdbZ-Jzj6o">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9765476.html">link</a></td></tr><tr><td>28</td><td>Revealing the Semantics of Data Wrangling Scripts With COMANTICS</td><td>Kai Xiong, Zhongsu Luo, Siwei Fu, Yongheng Wang, Mingliang Xu, Yingcai Wu</td><td>Data workers usually seek to understand the semantics of data wrangling scripts in various scenarios, such as code debugging, reusing, and maintaining. However, the understanding is challenging for novice data workers due to the variety of programming languages, functions, and parameters. Based on the observation that differences between input and output tables highly relate to the type of data transformation, we outline a design space including 103 characteristics to describe table differences. Then, we develop COMANTICS, a three-step pipeline that automatically detects the semantics of data transformation scripts. The first step focuses on the detection of table differences for each line of wrangling code. Second, we incorporate a characteristic-based component and a Siamese convolutional neural network-based component for the detection of transformation types. Third, we derive the parameters of each data transformation by employing a &quot;slot filling&quot; strategy. We design experiments to evaluate the performance of COMANTICS. Further, we assess its flexibility using three example applications in different domains.</td><td></td><td>VIS Full Paper</td><td>Transforming Tabular Data and Grammars</td><td></td><td><a href="https://youtu.be/cdawykJH3Wk">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1346.html">link</a></td></tr><tr><td>29</td><td>Visualizing the Scripts of Data Wrangling with SOMNUS</td><td>Kai Xiong, Siwei Fu, Guoming Ding, Zhongsu Luo, Rong Yu, Wei Chen, Hujun Bao, Yingcai Wu</td><td>Data workers use various scripting languages for data transformation, such as SAS, R, and Python. However, understanding intricate code pieces requires advanced programming skills, which hinders data workers from grasping the idea of data transformation at ease. Program visualization is beneficial for debugging and education and has the potential to illustrate transformations intuitively and interactively. In this paper, we explore visualization design for demonstrating the semantics of code pieces in the context of data transformation. First, to depict individual data transformations, we structure a design space by two primary dimensions, i.e., key parameters to encode and possible visual channels to be mapped. Then, we derive a collection of 23 glyphs that visualize the semantics of transformations. Next, we design a pipeline, named Somnus, that provides an overview of the creation and evolution of data tables using a provenance graph. At the same time, it allows detailed investigation of individual transformations. User feedback on Somnus is positive. Our study participants achieved better accuracy with less time using Somnus, and preferred it over carefully-crafted textual description. Further, we provide two example applications to demonstrate the utility and versatility of Somnus.</td><td>Program understanding, data transformation, visualization design</td><td>VIS Full Paper</td><td>Transforming Tabular Data and Grammars</td><td></td><td><a href="https://youtu.be/1MxZ9icQ_nM">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9693232.html">link</a></td></tr><tr><td>30</td><td>Rigel: Transforming Tabular Data By Declarative Mapping</td><td>Ran Chen, Di Weng, Yanwei Huang, Xinhuan Shu, Jiayi Zhou, Guodao Sun, Yingcai Wu</td><td>We present Rigel, an interactive system for rapid transformation of tabular data. Rigel implements a new declarative mapping approach that formulates the data transformation procedure as direct mappings from data to the row, column, and cell channels of the target table. To construct such mappings, Rigel allows users to directly drag data attributes from input data to these three channels and indirectly drag or type data values in a spreadsheet, and possible mappings that do not contradict these interactions are recommended to achieve efficient and straightforward data transformation. The recommended mappings are generated by enumerating and composing data variables based on the row, column, and cell channels, thereby revealing the possibility of alternative tabular forms and facilitating open-ended exploration in many data transformation scenarios, such as designing tables for presentation. In contrast to existing systems that transform data by composing operations (like transposing and pivoting), Rigel requires less prior knowledge on these operations, and constructing tables from the channels is more efficient and results in less ambiguity than generating operation sequences as done by the traditional by-example approaches. User study results demonstrated that Rigel is significantly less demanding in terms of time and interactions and suits more scenarios compared to the state-of-the-art by-example approach. A gallery of diverse transformation cases is also presented to show the potential of Rigel&#x27;s expressiveness.</td><td></td><td>VIS Full Paper</td><td>Transforming Tabular Data and Grammars</td><td></td><td><a href="https://youtu.be/r79WulbdBMo">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1171.html">link</a></td></tr><tr><td>31</td><td>HiTailor: Interactive Transformation and Visualization for Hierarchical Tabular Data</td><td>Guozheng Li, Runfei Li, Zicheng Wang, Chi Harold Liu, Min Lu, Guoren Wang</td><td>Tabular visualization techniques integrate visual representations with tabular data to avoid additional cognitive load caused by splitting users&#x27; attention. However, most of the existing studies focus on simple flat tables instead of hierarchical tables, whose complex structure limits the expressiveness of visualization results and affects users&#x27; efficiency in visualization construction. We present HiTailor, a technique for presenting and exploring hierarchical tables. HiTailor constructs an abstract model, which defines row/column headings as biclustering and hierarchical structures. Based on our abstract model, we identify three pairs of operators, Swap/Transpose, ToStacked/ToLinear, Fold/Unfold, for transformations of hierarchical tables to support users&#x27; comprehensive explorations. After transformation, users can specify a cell or block of interest in hierarchical tables as a TableUnit for visualization, and HiTailor recommends other related TableUnits according to the abstract model using different mechanisms. We demonstrate the usability of the HiTailor system through a comparative study and a case study with domain experts, showing that HiTailor can present and explore hierarchical tables from different viewpoints. HiTailor is available at https://github.com/bitvis2021/HiTailor.</td><td></td><td>VIS Full Paper</td><td>Transforming Tabular Data and Grammars</td><td></td><td><a href="https://youtu.be/Eem27BmZfXs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1413.html">link</a></td></tr><tr><td>32</td><td>Animated Vega-Lite: Unifying Animation with a Grammar of Interactive Graphics</td><td>Jonathan Zong, Josh M. Pollock, Dylan Wootton, Arvind Satyanarayan</td><td>We present Animated Vega-Lite, a set of extensions to Vega-Lite that model animated visualizations as time-varying data queries. In contrast to alternate approaches for specifying animated visualizations, which prize a highly expressive design space, Animated Vega-Lite prioritizes unifying animation with the language’s existing abstractions for static and interactive visualizations to enable authors to smoothly move between or combine these modalities. Thus, to compose animation with static visualizations, we represent time as an encoding channel. Time encodings map a data field to animation keyframes, providing a lightweight specification for animations without interaction. To compose animation and interaction, we also represent time as an event stream; Vega-Lite selections, which provide dynamic data queries, are now driven not only by input events but by timer ticks as well. We evaluate the expressiveness of our approach through a gallery of diverse examples that demonstrate coverage over taxonomies of both interaction and animation. We also critically reflect on the conceptual affordances and limitations of our contribution by interviewing five expert developers of existing animation grammars. These reflections highlight the key motivating role of in-the-wild examples, and identify three central tradeoffs: the language design process, the types of animated transitions supported, and how the systems model keyframes.</td><td></td><td>VIS Full Paper</td><td>Transforming Tabular Data and Grammars</td><td></td><td><a href="https://youtu.be/3awOHEVAjME">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1138.html">link</a></td></tr><tr><td>33</td><td>No Grammar to Rule Them All: A Survey of JSON-style DSLs for Visualization</td><td>Andrew M McNutt</td><td>There has been substantial growth in the use of JSON-based grammars, as well as other standard data serialization languages, to create visualizations. Each of these grammars serves a purpose: some focus on particular computational tasks (such as animation), some are concerned with certain chart types (such as maps), and some target specific data domains (such as ML). Despite the prominence of this interface form, there has been little detailed analysis of the characteristics of these languages. In this study, we survey and analyze the design and implementation of 57 JSON-style DSLs for visualization. We analyze these languages supported by a collected corpus of examples for each DSL (consisting of 4395 instances) across a variety of axes organized into concerns related to domain, conceptual model, language relationships, affordances, and general practicalities. We identify tensions throughout these areas, such as between formal and colloquial specifications, among types of users, and within the composition of languages. Through this work, we seek to support language implementers by elucidating the choices, opportunities, and tradeoffs in visualization DSL design.</td><td></td><td>VIS Full Paper</td><td>Transforming Tabular Data and Grammars</td><td></td><td><a href="https://youtu.be/eudQcdPhXDU">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1045.html">link</a></td></tr><tr><td>34</td><td>Real-Time Visualization of Large-Scale Geological Models with Nonlinear Feature-Preserving  Levels of Detail</td><td>Ronell Sicat, Mohamed Ibrahim, Amani Ageeli, Florian Mannuss, Peter Rautek, Markus Hadwiger</td><td>The rapidly growing size and complexity of 3D geological models has increased the need for level-of-detail techniques and compact encodings to facilitate interactive visualization. For large-scale hexahedral meshes, state-of-the-art approaches often employ wavelet schemes for level of detail as well as for data compression. Here, wavelet transforms serve two  purposes: (1) they achieve substantial compression for data reduction; and (2) the multiresolution encoding provides levels of detail for visualization. However, in coarser detail levels, important geometric features, such as geological faults, often get too smoothed out or lost, due to linear translation invariant filtering. The same is true for attribute features, such as discontinuities in porosity or permeability.We present a novel, integrated approach addressing both purposes above, while preserving critical data features of both model geometry and its attributes. Our first major contribution is that we completely decouple the computation of levels of detail from data compression, and perform nonlinear filtering in a high-dimensional data space jointly representing the geological model geometry with its attributes. Computing detail levels in this space enables us to jointly preserve features in both geometry and attributes. While designed in a general way, our framework specifically employs joint bilateral filters, computed efficiently on a high-dimensional permutohedral grid. For data compression, after the computation of all detail levels, each level is separately encoded with a standard wavelet transform. Our second major contribution is a compact GPU data structure for the encoded mesh and attributes that enables direct real-time GPU visualization without prior decoding.</td><td>Geological model visualization, Structured hexahedral meshes, Multiresolution representations and visualization, GPU data structures and rendering</td><td>VIS Full Paper</td><td>Spatial Data</td><td></td><td><a href="https://youtu.be/wTBCsUvvJMs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9576578.html">link</a></td></tr><tr><td>35</td><td>CosmoVis: An Interactive Visual Analysis Tool for Exploring Hydrodynamic Cosmological Simulations</td><td>David Abramov, Joseph N. Burchett, Oskar Elek, Cameron Hummels, J. Xavier Prochaska, Angus G. Forbes</td><td>We introduce CosmoVis, an open source web-based visualization tool for the interactive analysis of massive hydrodynamic cosmological simulation data. CosmoVis was designed in close collaboration with astrophysicists to enable researchers and citizen scientists to share and explore these datasets, and to use them to investigate a range of scientific questions. CosmoVis visualizes many key gas, dark matter, and stellar attributes extracted from the source simulations, which typically consist of complex data structures multiple terabytes in size, often requiring extensive data wrangling. CosmoVis introduces a range of features to facilitate real-time analysis of these simulations, including the use of &quot;virtual skewers,&quot; simulated analogues of absorption line spectroscopy that act as spectral probes piercing the volume of gaseous cosmic medium. We explain how such synthetic spectra can be used to gain insight into the source datasets and to make functional comparisons with observational data. Furthermore, we identify the main analysis tasks that CosmoVis enables and present implementation details of the software interface and the client-server architecture. We conclude by providing details of three contemporary scientific use cases that were conducted by domain experts using the software and by documenting expert feedback from astrophysicists at different career levels.</td><td>Astrovis, astrographics, cosmological simulations, astronomy, astrophysics, virtual spectrography</td><td>VIS Full Paper</td><td>Spatial Data</td><td></td><td><a href="https://youtu.be/R0W7w0P_igU">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9736650.html">link</a></td></tr><tr><td>36</td><td>Dynamic Mode Decomposition for Large-Scale Coherent Structure Extraction in Shear Flows</td><td>Duong Nguyen, Panruo Wu, Rodolfo Ostilla Monico, Guoning Chen</td><td>Large-scale structures have been observed in many shear flows which are the fluid generated between two surfaces moving with different velocity. A better understanding of the physics of the structures (especially large-scale structures) in shear flows will help explain a diverse range of physical phenomena and improve our capability of modeling more complex turbulence flows. Many efforts have been made in order to capture such structures; however, conventional methods have their limitations, such as arbitrariness in parameter choice or specificity to certain setups. To address this challenge, we propose to use Multi-Resolution Dynamic Mode Decomposition (mrDMD), for large-scale structure extraction in shear flows. In particular, we show that the slow-motion DMD modes are able to reveal large-scale structures in shear flows that also have slow dynamics. In most cases, we find that the slowest DMD mode and its reconstructed flow can sufficiently capture the large-scale dynamics in the shear flows, which leads to a parameter-free strategy for large-scale structure extraction. Effective visualization of the large-scale structures can then be produced with the aid of the slowest DMD mode. To speed up the computation of mrDMD, we provide a fast GPU-based implementation. We also apply our method to some non-shear flows that need not behave quasi-linearly to demonstrate the limitation of our strategy of using the slowest DMD mode. For non-shear flows, we show that multiple modes from different levels of mrDMD may be needed to sufficiently characterize the flow behavior.</td><td>Flow visualization, Shear Flows, Dynamic Mode Decomposition</td><td>VIS Full Paper</td><td>Spatial Data</td><td></td><td><a href="https://youtu.be/Gk5PAxkDybc">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9599597.html">link</a></td></tr><tr><td>37</td><td>A Comparison of Spatiotemporal Visualizations for 3D Urban Analytics</td><td>Roberta Mota, Fabio Miranda, Julio Daniel Silva, Marius Horga, Marcos Lage, Luis Ceferino, Usman Raza Alim, Ehud Sharlin, Nivan Ferreira</td><td>Recent technological innovations have led to an increase in the availability of 3D urban data, such as shadow, noise, solar potential, and earthquake simulations. These spatiotemporal datasets create opportunities for new visualizations to engage experts from different domains to study the dynamic behavior of urban spaces in this under explored dimension. However, designing 3D spatiotemporal urban visualizations is challenging, as it requires visual strategies to support analysis of time-varying data referent to the city geometry. Although different visual strategies have been used in 3D urban visual analytics, the question of how effective these visual designs are at supporting spatiotemporal analysis on building surfaces remains open. To investigate this, in this paper we first contribute a series of analytical tasks elicited after interviews with practitioners from three urban domains. We also contribute a quantitative user study comparing the effectiveness of four representative visual designs used to visualize 3D spatiotemporal urban data: spatial juxtaposition, temporal juxtaposition, linked view, and embedded view. Participants performed a series of tasks that required them to identify extreme values on building surfaces over time. Tasks varied in granularity for both space and time dimensions. Our results demonstrate that participants were more accurate using plot-based visualizations (linked view, embedded view) but faster using color-coded visualizations (spatial juxtaposition, temporal juxtaposition). Our results also show that, with increasing task complexity, plot-based visualizations perform better in preserving efficiency (time, accuracy) compared to color-coded visualizations. Based on our findings, we present a set of takeaways with design recommendations for 3D spatiotemporal urban visualizations for researchers and practitioners. Lastly, we report on a series of interviews with four practitioners, and their feedback and suggestions for further work on the visualizations to support 3D spatiotemporal urban data analysis.</td><td></td><td>VIS Full Paper</td><td>Spatial Data</td><td></td><td><a href="https://youtu.be/qgoNzM9SeUw">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1239.html">link</a></td></tr><tr><td>38</td><td>EpiMob: Interactive Visual Analytics of Citywide Human Mobility Restrictions for Epidemic Control</td><td>Chuang Yang, Zhiwen Zhang, Zipei Fan, Renhe Jiang, Quanjun Chen, Xuan Song, Ryosuke Shibasaki.</td><td>The outbreak of coronavirus disease (COVID-19) has swept across more than 180 countries and territories since late January 2020. As a worldwide emergency response, governments have implemented various measures and policies, such as self-quarantine, travel restrictions, work from home, and regional lockdown, to control the spread of the epidemic. These countermeasures seek to restrict human mobility because COVID-19 is a highly contagious disease that is spread by human-to-human transmission. Medical experts and policymakers have expressed the urgency to effectively evaluate the outcome of human restriction policies with the aid of big data and information technology. Thus, based on big human mobility data and city POI data, an interactive visual analytics system called Epidemic Mobility (EpiMob) was designed in this study. The system interactively simulates the changes in human mobility and infection status in response to the implementation of a certain restriction policy or a combination of policies (e.g., regional lockdown, telecommuting, screening). Users can conveniently designate the spatial and temporal ranges for different mobility restriction policies. Then, the results reflecting the infection situation under different policies are dynamically displayed and can be flexibly compared and analyzed in depth. Multiple case studies consisting of interviews with domain experts were conducted in the largest metropolitan area of Japan (i.e., Greater Tokyo Area) to demonstrate that the system can provide insight into the effects of different human mobility restriction policies for epidemic control, through measurements and comparisons.</td><td>Human mobility simulation, epidemic control, visual analytics, interactive system, big trajectory data</td><td>VIS Full Paper</td><td>Spatial Data</td><td></td><td><a href="https://youtu.be/A-UlJvsU1R8">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9750868.html">link</a></td></tr><tr><td>39</td><td>On-Tube Attribute Visualization for Multivariate Trajectory Data</td><td>Benjamin Russig, David Groß, Raimund Dachselt, Stefan Gumhold</td><td>Stylized tubes are an established visualization primitive for line data as encountered in many scientific fields, ranging from characteristic lines in flow fields, fiber tracks reconstructed from diffusion tensor imaging, to trajectories of moving objects as they arise from cyber-physical systems in many engineering disciplines. Typical challenges include large data set sizes demanding for efficient rendering techniques as well as a large number of attributes that cannot be mapped simultaneously to the basic visual attributes provided by a tube-based visualization. In this work, we tackle both challenges with a new on-tube visualization approach. We improve recent work on high-quality GPU ray casting of Hermite spline tubes supporting ambient occlusion and extend it by a new layered procedural texturing technique. In the proposed framework, a large number of data set attributes can be mapped simultaneously to a variety of glyphs and plots that are embedded in texture space and organized in layers. Efficient rendering with minimal data transfer is achieved by generating the glyphs procedurally and drawing them in a deferred shading pass. We integrated these techniques in a prototype visualization tool that facilitates flexible mapping of data set attributes to visual tube and glyph attributes. We studied our approach on a variety of example data from different fields and found it to provide a highly adaptable and extensible toolbox to quickly craft tailor-made tube-based trajectory visualizations.</td><td></td><td>VIS Full Paper</td><td>Spatial Data</td><td></td><td><a href="https://youtu.be/0uB3HGtLuO4">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1291.html">link</a></td></tr><tr><td>40</td><td>Fiber Uncertainty Visualization for Bivariate Data With Parametric and Nonparametric Noise Models</td><td>Tushar M. Athawale, Chris R. Johnson, Sudhanshu Sane, David Pugmire</td><td>Visualization and analysis of multivariate data and their uncertainty are top research challenges in data visualization. Constructing fiber surfaces is a popular technique for multivariate data visualization that generalizes the idea of level-set visualization for univariate data to multivariate data. In this paper, we present a statistical framework to quantify positional probabilities of fibers extracted from uncertain bivariate fields. Specifically, we extend the state-of-the-art Gaussian models of uncertainty for bivariate data to other parametric distributions (e.g., uniform and Epanechnikov) and more general nonparametric probability distributions (e.g., histograms and kernel density estimation) and derive corresponding spatial probabilities of fibers. In our proposed framework, we leverage Green’s theorem for closed-form computation of fiber probabilities when bivariate data are assumed to have independent parametric and nonparametric noise. Additionally, we present a nonparametric approach combined with numerical integration to study the positional probability of fibers when bivariate data are assumed to have correlated noise. For uncertainty analysis, we visualize the derived probability volumes for fibers via volume rendering and extracting level sets based on probability thresholds. We present the utility of our proposed techniques via experiments on synthetic and simulation datasets.</td><td></td><td>VIS Full Paper</td><td>Uncertainty</td><td></td><td><a href="https://youtu.be/33pVyJ9bUqc">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1307.html">link</a></td></tr><tr><td>41</td><td>Evaluating the Use of Uncertainty Visualisations for Imputations of Data Missing At Random in Scatterplots</td><td>Abhraneel Sarma, Shunan Guo, Jane Hoffswell, Ryan Rossi, Fan Du, Eunyee Koh, Matthew Kay</td><td>Most real-world datasets contain missing values yet most exploratory data analysis (EDA) systems only support visualising data points with complete cases. This omission may potentially lead the user to biased analyses and insights. Imputation techniques can help estimate the value of a missing data point, but introduces additional uncertainty. In this work, we investigate the effects of visualising imputed values in charts using different ways of representing data imputations and imputation uncertainty—no imputation, mean, 95% confidence intervals, probability density plots, gradient intervals, and hypothetical outcome plots. We focus on scatterplots, which is a commonly used chart type, and conduct a crowdsourced study with 202 participants. We measure users’ bias and precision in performing two tasks—estimating average and detecting trend—and their self-reported confidence in performing these tasks. Our results suggest that, when estimating averages, uncertainty representations may reduce bias but at the cost of decreasing precision. When estimating trend, only hypothetical outcome plots may lead to a small probability of reducing bias while increasing precision. Participants in every uncertainty representation were less certain about their response when compared to the baseline. The findings point towards potential trade-offs in using uncertainty encodings for datasets with a large number of missing values. This paper and the associated analysis materials are available at: https://osf.io/q4y5r/</td><td></td><td>VIS Full Paper</td><td>Uncertainty</td><td></td><td><a href="https://youtu.be/ey3Jy2iW-bI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1121.html">link</a></td></tr><tr><td>42</td><td>Dispersion vs Disparity: Hiding Variability Can Encourage Stereotyping When Visualizing Social Outcomes</td><td>Eli Holder, Cindy Xiong</td><td>Visualization research often focuses on perceptual accuracy or helping readers interpret key messages. However, we know very little about how chart designs might influence readers&#x27; perceptions of the people behind the data. Specifically, could designs interact with readers&#x27; social cognitive biases in ways that perpetuate harmful stereotypes? For example, when analyzing social inequality, bar charts are a popular choice to present outcome disparities between race, gender, or other groups. But bar charts may encourage deficit thinking, the perception that outcome disparities are caused by groups’ personal strengths or deficiencies, rather than external factors. These faulty personal attributions can then reinforce stereotypes about the groups being visualized. We conducted four experiments examining design choices that influence attribution biases (and therefore deficit thinking). Crowdworkers viewed visualizations depicting social outcomes that either mask variability in data, such as bar charts or dot plots, or emphasize variability in data, such as jitter plots or prediction intervals. They reported their agreement with both personal and external explanations for the visualized disparities. Overall, when participants saw visualizations that hide within-group variability, they agreed more with personal explanations. When they saw visualizations that emphasize within-group variability, they agreed less with personal explanations. These results demonstrate that data visualizations about social inequity can be misinterpreted in harmful ways and lead to stereotyping. Design choices can influence these biases: Hiding variability tends to increase stereotyping while emphasizing variability reduces it.</td><td></td><td>VIS Full Paper</td><td>Uncertainty</td><td></td><td><a href="https://youtu.be/kI3NcukbVsA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1062.html">link</a></td></tr><tr><td>43</td><td>Communicating Uncertainty in Digital Humanities Visualization Research</td><td>Georgia Panagiotidou, Houda Lamqaddam, Jeroen Poblome, Koenraad Brosens, Katrien Verbert, Andrew Vande Moere</td><td>Due to their historical nature, humanistic data encompass multiple sources of uncertainty. While humanists are accustomed to handling such uncertainty with their established methods, they are cautious of visualizations that appear overly objective and fail to communicate this uncertainty. To design more trustworthy visualizations for humanistic research, therefore, a deeper understanding of its relation to uncertainty is needed. We systematically reviewed 126 publications from digital humanities literature that use visualization as part of their research process, and examined how uncertainty was handled and represented in their visualizations. Crossing these dimensions with the visualization type and use, we identified that uncertainty originated from multiple steps in the research process from the source artifacts to their datafication. We also noted how besides known uncertainty coping strategies, such as excluding data and evaluating its effects, humanists also embraced uncertainty as a separate dimension important to retain. By mapping how the visualizations encoded uncertainty, we identified four approaches that varied in terms of explicitness and customization. This work thus contributes with two empirical taxonomies of uncertainty and it’s corresponding coping strategies, as well as with the foundation of a research agenda for uncertainty visualization in the digital humanities. Our findings further the synergy among humanists and visualization researchers, and ultimately contribute to the development of more trustworthy, uncertainty-aware visualizations.</td><td></td><td>VIS Full Paper</td><td>Uncertainty</td><td></td><td><a href="https://youtu.be/mqfGYPMD8gE">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1269.html">link</a></td></tr><tr><td>44</td><td>Fuzzy Spreadsheet: Understanding and Exploring Uncertainties in Tabular Calculations</td><td>Vaishali Dhanoa, Conny Walchshofer, Andreas Hinterreiter, Eduard Gröller, Marc Streit</td><td>Spreadsheet-based tools provide a simple yet effective way of calculating values, which makes them the number-one choice for building and formalizing simple models for budget planning and many other applications. A cell in a spreadsheet holds one specific value and gives a discrete, over precise view of the underlying model. Therefore, spreadsheets are of limited use when investigating the inherent uncertainties of such models and answering what-if questions. Existing extensions typically require a complex modeling process that cannot easily be embedded in a tabular layout. In Fuzzy Spreadsheet, a cell can hold and display a distribution of values. This integrated uncertainty-handling immediately conveys sensitivity and robustness information. The fuzzification of the cells enables calculations not only with precise values but also with distributions, and probabilities. We conservatively added and carefully crafted visuals to maintain the look and feel of a traditional spreadsheet while facilitating what-if analyses. Given a user-specified reference cell, Fuzzy Spreadsheet automatically extracts and visualizes contextually relevant information, such as impact, uncertainty, and degree of neighborhood, for the selected and related cells. To evaluate its usability and the perceived mental effort required, we conducted a user study. The results show that our approach outperforms traditional spreadsheets in terms of answer correctness, response time, and perceived mental effort in almost all tasks tested.</td><td>Uncertainty visualization, tabular data, spreadsheet augmentation</td><td>VIS Full Paper</td><td>Uncertainty</td><td></td><td><a href="https://youtu.be/Il7gn_FHanU">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9566799.html">link</a></td></tr><tr><td>45</td><td>A Survey of Perception-Based Visualization Studies by Task</td><td>Ghulam Jilani Quadri, Paul Rosen</td><td>Knowledge of human perception has long been incorporated into visualizations to enhance their quality and effectiveness. The last decade, in particular, has shown an increase in perception-based visualization research studies. With all of this recent progress, the visualization community lacks a comprehensive guide to contextualize their results. In this report, we provide a systematic and comprehensive review of research studies on perception related to visualization. This survey reviews perception-focused visualization studies since 1980 and summarizes their research developments focusing on low-level tasks, further breaking techniques down by visual encoding and visualization type. In particular, we focus on how perception is used to evaluate the effectiveness of visualizations, to help readers understand and apply the principles of perception of their visualization designs through a task-optimized approach. We concluded our report with a summary of the weaknesses and open research questions in the area.</td><td>Visualization, perception, graphical perception, visual analytics tasks, evaluation, survey.</td><td>VIS Full Paper</td><td>Understanding and Modeling How People Respond to Visualizations</td><td></td><td><a href="https://youtu.be/Hrgd7gmnfYc">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9492011.html">link</a></td></tr><tr><td>46</td><td>BeauVis: A Validated Scale for Measuring the Aesthetic Pleasure of Visual Representations</td><td>Tingying He, Petra Isenberg, Raimund Dachselt, Tobias Isenberg</td><td>We developed and validated a rating scale to assess the aesthetic pleasure (or beauty) of a visual data representation: the BeauVis scale. With our work we offer researchers and practitioners a simple instrument to compare the visual appearance of different visualizations, unrelated to data or context of use. Our rating scale can, for example, be used to accompany results from controlled experiments or be used as informative data points during in-depth qualitative studies. Given the lack of an aesthetic pleasure scale dedicated to visualizations, researchers have mostly chosen their own terms to study or compare the aesthetic pleasure of visualizations. Yet, many terms are possible and currently no clear guidance on their effectiveness regarding the judgment of aesthetic pleasure exists. To solve this problem, we engaged in a multi-step research process to develop the first validated rating scale specifically for judging the aesthetic pleasure of a visualization (osf.io/fxs76). Our final BeauVis scale consists of five items, “enjoyable,” “likable,” “pleasing,” “nice,” and “appealing.” Beyond this scale itself, we contribute (a) a systematic review of the terms used in past research to capture aesthetics, (b) an investigation with visualization experts who suggested terms to use for judging the aesthetic pleasure of a visualization, and (c) a confirmatory survey in which we used our terms to study the aesthetic pleasure of a set of 3 visualizations.</td><td></td><td>VIS Full Paper</td><td>Understanding and Modeling How People Respond to Visualizations</td><td></td><td><a href="https://youtu.be/k9iC6typxYA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1219.html">link</a></td></tr><tr><td>47</td><td>Photosensitive Accessibility for Interactive Data Visualizations</td><td>Laura South, Michelle A. Borkin</td><td>Accessibility guidelines place restrictions on the use of animations and interactivity on webpages to lessen the likelihood of webpages inadvertently producing sequences with flashes, patterns, or color changes that may trigger seizures for individuals with photosensitive epilepsy. Online data visualizations often incorporate elements of animation and interactivity to create a narrative, engage users, or encourage exploration. These design guidelines have been empirically validated by perceptual studies in visualization literature, but the impact of animation and interaction in visualizations on users with photosensitivity, who may experience seizures in response to certain visual stimuli, has not been considered. We systematically gathered and tested 1,132 interactive and animated visualizations for seizure-inducing risk using established methods and found that currently available methods for determining photosensitive risk are not reliable when evaluating interactive visualizations, as risk scores varied significantly based on the individual interacting with the visualization. To address this issue, we introduce a theoretical model defining the degree of control visualization designers have over three determinants of photosensitive risk in potentially seizure-inducing sequences: the size, frequency, and color of flashing content. Using an analysis of 375 visualizations hosted on bl.ocks.org, we created a theoretical model of photosensitive risk in visualizations by arranging the photosensitive risk determinants according to the degree of control visualization authors have over whether content exceeds photosensitive accessibility thresholds. We then use this model to propose a new method of testing for photosensitive risk that focuses on elements of visualizations that are subject to greater authorial control – and are therefore more robust to variations in the individual user – producing more reliable risk assessments than existing methods when applied to interactive visualizations. A full copy of this paper and all study materials are available at https://osf.io/8kzmg/.</td><td></td><td>VIS Full Paper</td><td>Understanding and Modeling How People Respond to Visualizations</td><td></td><td><a href="https://youtu.be/QSFUOVRGHzU">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1217.html">link</a></td></tr><tr><td>48</td><td>Unifying Effects of Direct and Relational Associations for Visual Communication</td><td>Melissa A. Schoenlein, Johnny Campos, Kevin Lande, Laurent Lessard, Karen Schloss</td><td>People have expectations about how colors map to concepts in visualizations, and they are better at interpreting visualizations that match their expectations. Traditionally, studies on these expectations (inferred mappings) distinguished distinct factors relevant for visualizations of categorical vs. continuous information. Studies on categorical information focused on direct associations (e.g., mangos are associated with yellows) whereas studies on continuous information focused on relational associations (e.g., darker colors map to larger quantities; dark-is-more bias). We unite these two areas within a single framework of assignment inference. Assignment inference is the process by which people infer mappings between perceptual features and concepts represented in encoding systems. Observers infer globally optimal assignments by maximizing the “merit,” or “goodness,” of each possible assignment. Previous work on assignment inference focused on visualizations of categorical information. We extend this approach to visualizations of continuous data by (a) broadening the notion of merit to include relational associations and (b) developing a method for combining multiple (sometimes conflicting) sources of merit to predict people’s inferred mappings. We developed and tested our model on data from experiments in which participants interpreted colormap data visualizations, representing fictitious data about environmental concepts (sunshine, shade, wild fire, ocean water, glacial ice). We found both direct and relational associations contribute independently to inferred mappings. These results can be used to optimize visualization design to facilitate visual communication.</td><td></td><td>VIS Full Paper</td><td>Understanding and Modeling How People Respond to Visualizations</td><td></td><td><a href="https://youtu.be/nwkT-szaoGE">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1115.html">link</a></td></tr><tr><td>49</td><td>A Scanner Deeply: Predicting Gaze Heatmaps on Visualizations Using Crowdsourced Eye Movement Data</td><td>Sungbok Shin, Sunghyo Chung, Sanghyun Hong, Niklas Elmqvist</td><td>Visual perception is a key component of data visualization. Much prior empirical work uses eye movement as a proxy to understand human visual perception. Diverse apparatus and techniques have been proposed to collect eye movements, but there is still no optimal approach. In this paper, we review 30 prior works for collecting eye movements based on three axes: (1) the tracker technology used to measure eye movements; (2) the image stimulus shown to participants; and (3) the collection methodology used to gather the data. Based on this taxonomy, we employ a webcam-based eyetracking approach using task-specific visualizations as the stimulus. The low technology requirement means that virtually anyone can participate, thus enabling us to collect data at large scale using crowdsourcing: approximately 12,000 samples in total. Choosing visualization images as stimulus means that the eye movements will be specific to perceptual tasks associated with visualization. We use these data to propose a SCANNER DEEPLY, a virtual eyetracker model that, given an image of a visualization, generates a gaze heatmap for that image. We employ a computationally efficient, yet powerful convolutional neural network for our model. We compare the results of our work with results from the DVS model and a neural network trained on the Salicon dataset. The analysis of our gaze patterns enables us to understand how users grasp the structure of visualized data. We also make our stimulus dataset of visualization images available as part of this paper’s contribution.</td><td></td><td>VIS Full Paper</td><td>Understanding and Modeling How People Respond to Visualizations</td><td></td><td><a href="https://youtu.be/W_5DWEkUPeA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1448.html">link</a></td></tr><tr><td>50</td><td>Studying Early Decision Making with Progressive Bar Charts</td><td>Ameya B Patil, Gaëlle Richer, Dominik Moritz, Christopher Jermaine, Jean-Daniel Fekete</td><td>We conduct a user study to quantify and compare user performance for a value comparison task using four bar chart designs, where the bars show the mean values of data loaded progressively and updated every second (progressive bar charts). Progressive visualization divides different stages of the visualization pipeline—data loading, processing, and visualization—into iterative animated steps to limit the latency when loading large amounts of data. An animated visualization appearing quickly, unfolding, and getting more accurate with time, enables users to make early decisions. However, intermediate mean estimates are computed only on partial data and may not have time to converge to the true means, potentially misleading users and resulting in incorrect decisions. To address this issue, we propose two new designs visualizing the history of values in progressive bar charts, in addition to the use of confidence intervals. We comparatively study four progressive bar chart designs: with/without confidence intervals, and using near-history representation with/without confidence intervals, on three realistic data distributions. We evaluate user performance based on the percentage of correct answers (accuracy), response time, and user confidence. Our results show that, overall, users can make early and accurate decisions with 92% accuracy using only 18% of the data, regardless of the design. We find that our proposed bar chart design with only near-history is comparable to bar charts with only confidence intervals in performance, and the qualitative feedback we received indicates a preference for designs with history.</td><td></td><td>VIS Full Paper</td><td>Understanding and Modeling How People Respond to Visualizations</td><td></td><td><a href="https://youtu.be/GiSmTXxoD_0">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1254.html">link</a></td></tr><tr><td>51</td><td>StrategyAtlas: Strategy Analysis for Machine Learning Interpretability</td><td>Dennis Collaris, Jarke J. van Wijk</td><td>Businesses in high-risk environments have been reluctant to adopt modern machine learning approaches due to their complex and uninterpretable nature. Most current solutions provide local, instance-level explanations, but this is insufficient for understanding the model as a whole. In this work, we show that strategy clusters (i.e., groups of data instances that are treated distinctly by the model) can be used to understand the global behavior of a complex ML model. To support effective exploration and understanding of these clusters, we introduce StrategyAtlas, a system designed to analyze and explain model strategies. Furthermore, it supports multiple ways to utilize these strategies for simplifying and improving the reference model. In collaboration with a large insurance company, we present a use case in automatic insurance acceptance, and show how professional data scientists were enabled to understand a complex model and improve the production model based on these insights.</td><td>Machine learning, Visual analytics, Explainable AI</td><td>VIS Full Paper</td><td>Interpreting Machine Learning</td><td></td><td><a href="https://youtu.be/PYHvadNTUj4">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9695246.html">link</a></td></tr><tr><td>52</td><td>VDL-Surrogate: A View-Dependent Latent-based Model for Parameter Space Exploration of Ensemble Simulations</td><td>Neng Shi, Jiayi Xu, Haoyu Li, Hanqi Guo, Jonathan Woodring, Han-Wei Shen</td><td>We propose VDL-Surrogate, a view-dependent neural-network-latent-based surrogate model for parameter space exploration of ensemble simulations that allows high-resolution visualizations and user-specified visual mappings. Surrogate-enabled parameter space exploration allows domain scientists to preview simulation results without having to run a large number of computationally costly simulations. Limited by computational resources, however, existing surrogate models may not produce previews with sufficient resolution for visualization and analysis. To improve the efficient use of computational resources and support high-resolution exploration, we perform ray casting from different viewpoints to collect samples and produce compact latent representations. This latent encoding process reduces the cost of surrogate model training while maintaining the output quality. In the model training stage, we select viewpoints to cover the whole viewing sphere and train corresponding VDL-Surrogate models for the selected viewpoints. In the model inference stage, we predict the latent representations at previously selected viewpoints and decode the latent representations to data space. For any given viewpoint, we make interpolations over decoded data at selected viewpoints and generate visualizations with user-specified visual mappings. We show the effectiveness and efficiency of VDL-Surrogate in cosmological and ocean simulations with quantitative and qualitative evaluations. Source code is publicly available at \url{https://github.com/trainsn/VDL-Surrogate}.</td><td></td><td>VIS Full Paper</td><td>Interpreting Machine Learning</td><td></td><td><a href="https://youtu.be/YCBLJDno87I">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1696.html">link</a></td></tr><tr><td>53</td><td>ConceptExplainer: Interactive Explanation for Deep Neural Networks from a Concept Perspective</td><td>Jinbin Huang, Aditi Mishra, Bum Chul Kwon, Chris Bryan</td><td>Traditional deep learning interpretability methods which are suitable for model users cannot explain network behaviors at the global level and are inflexible at providing fine-grained explanations. As a solution, concept-based explanations are gaining attention due to their human intuitiveness and their flexibility to describe both global and local model behaviors. Concepts are groups of similarly meaningful pixels that express a notion, embedded within the network’s latent space and have commonly been hand-generated, but have recently been discovered by automated approaches. Unfortunately, the magnitude and diversity of discovered concepts makes it difficult to navigate and make sense of the concept space. Visual analytics can serve a valuable role in bridging these gaps by enabling structured navigation and exploration of the concept space to provide concept-based insights of model behavior to users. To this end, we design, develop, and validate ConceptExplainer, a visual analytics system that enables people to interactively probe and explore the concept space to explain model behavior at the instance/class/global level. The system was developed via iterative prototyping to address a number of design challenges that model users face in interpreting the behavior of deep learning models. Via a rigorous user study, we validate how ConceptExplainer supports these challenges. Likewise, we conduct a series of usage scenarios to demonstrate how the system supports the interactive analysis of model behavior across a variety of tasks and explanation granularities, such as identifying concepts that are important to classification, identifying bias in training data, and understanding how concepts can be shared across diverse and seemingly dissimilar classes.</td><td></td><td>VIS Full Paper</td><td>Interpreting Machine Learning</td><td></td><td><a href="https://youtu.be/EvArXDWxCXI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1074.html">link</a></td></tr><tr><td>54</td><td>SliceTeller : A Data Slice-Driven Approach for Machine Learning Model Validation</td><td>Xiaoyu Zhang, Jorge H Piazentin Ono, Huan Song, Liang Gou, Kwan-Liu Ma, Liu Ren</td><td>Real-world machine learning applications need to be thoroughly evaluated to meet critical product requirements for model release, to ensure fairness for different groups or individuals, and to achieve a consistent performance in various scenarios. For example, in autonomous driving, an object classification model should achieve high detection rates under different conditions of weather, distance, etc. Similarly, in the financial setting, credit-scoring models must not discriminate against minority groups. These conditions or groups are called as “Data Slices”. In product MLOps cycles, product developers must identify such critical data slices and adapt models to mitigate data slice problems. Discovering where models fail, understanding why they fail, and mitigating these problems, are therefore essential tasks in the MLOps life-cycle. In this paper, we present SliceTeller, a novel tool that allows users to debug, compare and improve machine learning models driven by critical data slices. SliceTeller automatically discovers problematic slices in the data, helps the user understand why models fail. More importantly, we present an efficient algorithm, SliceBoosting, to estimate trade-offs when prioritizing the optimization over certain slices. Furthermore, our system empowers model developers to compare and analyze different model versions during model iterations, allowing them to choose the model version best suitable for their applications. We evaluate our system with three use cases, including two real-world use cases of product development, to demonstrate the power of SliceTeller in the debugging and improvement of product-quality ML models.</td><td></td><td>VIS Full Paper</td><td>Interpreting Machine Learning</td><td></td><td><a href="https://youtu.be/MXCaUK-RBr4">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1320.html">link</a></td></tr><tr><td>55</td><td>Calibrate: Interactive Analysis of Probabilistic Model Output</td><td>Peter Xenopoulos, João Rulff, Luis Gustavo Nonato, Brian Barr, Claudio Silva</td><td>Analyzing classification model performance is a crucial task for machine learning practitioners. While practitioners often use count-based metrics derived from confusion matrices, like accuracy, many applications, such as weather prediction, sports betting, or patient risk prediction, rely on a classifier&#x27;s predicted probabilities rather than predicted labels. In these instances, practitioners are concerned with producing a calibrated model, that is, one which outputs probabilities that reflect those of the true distribution. Model calibration is often analyzed visually, through static reliability diagrams, however, the traditional calibration visualization may suffer from a variety of drawbacks due to the strong aggregations it necessitates. Furthermore, count-based approaches are unable to sufficiently analyze model calibration. We present Calibrate, an interactive reliability diagram that addresses the aforementioned issues. Calibrate constructs a reliability diagram that is resistant to drawbacks in traditional approaches, and allows for interactive subgroup analysis and instance-level inspection. We demonstrate the utility of Calibrate through use cases on both real-world and synthetic data. We further validate Calibrate by presenting the results of a think-aloud experiment with data scientists who routinely analyze model calibration.</td><td></td><td>VIS Full Paper</td><td>Interpreting Machine Learning</td><td></td><td><a href="https://youtu.be/B55HitnGlw4">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1657.html">link</a></td></tr><tr><td>56</td><td>Visualizing Ensemble Predictions of Music Mood</td><td>Zelin Ye, Min Chen</td><td>Music mood classification has been a challenging problem in comparison with other music classification problems (e.g., genre, composer, or period). One solution for addressing this challenge is to use an ensemble of machine learning models. In this paper, we show that visualization techniques can effectively convey the popular prediction as well as uncertainty at different music sections along the temporal axis while enabling the analysis of individual ML models in conjunction with their application to different musical data. In addition to the traditional visual designs, such as stacked line graph, ThemeRiver, and pixel-based visualization, we introduce a new variant of ThemeRiver, called ``dual-flux ThemeRiver&#x27;&#x27;, which allows viewers to observe and measure the most popular prediction more easily than stacked line graph and ThemeRiver. Together with pixel-based visualization, dual-flux ThemeRiver plots can also assist in model-development workflows, in addition to annotating music using ensemble model predictions.</td><td></td><td>VIS Full Paper</td><td>Interpreting Machine Learning</td><td></td><td><a href="https://youtu.be/AyWvx7mJSXw">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1319.html">link</a></td></tr><tr><td>57</td><td>Breaking the Fourth Wall of Data Stories through Interaction</td><td>Yang Shi, Tian Gao, Xiaohan Jiao, Nan Cao</td><td>Interaction is increasingly integrating into data stories to support data exploration and explanation. Interaction can also be combined with the narrative device, breaking the fourth wall (BTFW), to build a deeper connection between readers and data stories. BTFW interaction directly addresses readers by requiring their input. Such user input is then integrated into the narrative or visuals of data stories to encourage readers to inspect the stories more closely. In this work, we explore the design patterns of BTFW interaction commonly used in data stories. Six design patterns were identified through the analysis of 58 high-quality data stories collected from a range of online sources. Specifically, the data stories were categorized using a coding framework, including the input of BTFW interaction provided by readers and the output of BTFW interaction generated by data stories to respond to the input. To explore the benefits as well as concerns of using BTFW interaction, we conducted a three-session user study including the reading, interview, and recall sessions. The results of our user study suggested that BTFW interaction has a positive impact on self-story connection, user engagement, and information recall. We also discussed design implications to address the possible negative effects on the interactivity-comprehensibility balance, information privacy, and the learning curve of interaction brought by BTFW interaction.</td><td></td><td>VIS Full Paper</td><td>Storytelling</td><td></td><td><a href="https://youtu.be/gqdC0w04wxY">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1198.html">link</a></td></tr><tr><td>58</td><td>Erato: Cooperative Data Story Editing via Fact Interpolation</td><td>Mengdi Sun, Ligan Cai, Weiwei Cui, Yanqiu Wu, Yang Shi, Nan Cao</td><td>As an effective form of narrative visualization, visual data stories are widely used in data-driven storytelling to communicate complex insights and support data understanding. Although important, they are difficult to create, as a variety of interdisciplinary skills, such as data analysis and design, are required. In this work, we introduce Erato, a human-machine cooperative data story editing system, which allows users to generate insightful and fluent data stories together with the computer. Specifically, Erato only requires a number of keyframes provided by the user to briefly describe the topic and structure of a data story. Meanwhile, our system leverages a novel interpolation algorithm to help users insert intermediate frames between the keyframes to smooth the transition. We evaluated the effectiveness and usefulness of the Erato system via a series of evaluations including a Turing test, a controlled user study, a performance validation, and interviews with three expert users. The evaluation results showed that the proposed interpolation technique was able to generate coherent story content and help users create data stories more efficiently.</td><td></td><td>VIS Full Paper</td><td>Storytelling</td><td></td><td><a href="https://youtu.be/Luv5dRwLLnw">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1495.html">link</a></td></tr><tr><td>59</td><td>Geo-Storylines: Integrating Maps into Storyline Visualizations</td><td>Golina Hulstein, Vanessa Peña-Araya, Anastasia Bezerianos</td><td>Storyline visualizations are a powerful way to compactly visualize how the relationships between people evolve over time. Real-world relationships often also involve space, for example the cities that two political rivals visited together or alone over the years. By default, Storyline visualizations only show implicitly geospatial co-occurrence between people (drawn as lines), by bringing their lines together. Even the few designs that do explicitly show geographic locations only do so in abstract ways (e.g. annotations) and do not communicate geospatial information, such as the direction or extent of their political campains. We introduce Geo-Storylines, a collection of visualisation designs that integrate geospatial context into Storyline visualizations, using different strategies for compositing time and space. Our contribution is twofold. First, we present the results of a sketching workshop with 11 participants, that we used to derive a design space for integrating maps into Storylines. Second, by analyzing the strengths and weaknesses of the potential designs of the design space in terms of legibility and ability to scale to multiple relationships, we extract the three most promising: Time Glyphs, Coordinated Views, and Map Glyphs. We compare these three techniques first in a controlled study with 18 participants, under five different geospatial tasks and two maps of different complexity. We additionally collected informal feedback about their usefulness from domain experts in data journalism. Our results indicate that, as expected, detailed performance depends on the task. Nevertheless, Coordinated Views remain a highly effective and preferred technique across the board.</td><td></td><td>VIS Full Paper</td><td>Storytelling</td><td></td><td><a href="https://youtu.be/S_WISD78cfs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1180.html">link</a></td></tr><tr><td>60</td><td>Roslingifier: Semi-Automated Storytelling for Animated Scatterplots</td><td>Minjeong Shin, Joohee Kim, Yunha Han, Lexing Xie, Mitchell Whitelaw, Bum Chul Kwon, Sungahn Ko, Niklas Elmqvist</td><td>We present Roslingifier, a data-driven storytelling method for animated scatterplots. Like its namesake, Hans Rosling (1948--2017), a professor of public health and a spellbinding public speaker, Roslingifier turns a sequence of entities changing over time---such as countries and continents with their demographic data---into an engaging narrative telling the story of the data. This data-driven storytelling method with an in-person presenter is a new genre of storytelling technique and has never been studied before. In this paper, we aim to define a design space for this new genre---data presentation---and provide a semi-automated authoring tool for helping presenters create quality presentations. From an in-depth analysis of video clips of presentations using interactive visualizations, we derive three specific techniques to achieve this: natural language narratives, visual effects that highlight events, and temporal branching that changes playback time of the animation. Our implementation of the Roslingifier method is capable of identifying and clustering significant movements, automatically generating visual highlighting and a narrative for playback, and enabling the user to customize. From two user studies, we show that Roslingifier allows users to effectively create engaging data stories and the system features help both presenters and viewers find diverse insights.</td><td>Data-driven storytelling, narrative visualization, Hans Rosling, Gapminder, Trendalyzer</td><td>VIS Full Paper</td><td>Storytelling</td><td></td><td><a href="https://youtu.be/Qh97Mb4WYm0">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9695173.html">link</a></td></tr><tr><td>61</td><td>Nanotilus: Generator of Immersive Guided-Tours in Crowded 3D Environments</td><td>Ruwayda Alharbi, Ondˇrej Strnad, Laura R. Luidolt, Manuela Waldner, David Kouˇril, Ciril Bohak, Tobias Klein, Eduard Groller, Ivan Viola</td><td>Immersive virtual reality environments are gaining popularity for studying and exploring crowded three-dimensional structures. When reaching very high structural densities, the natural depiction of the scene produces impenetrable clutter and requires visibility and occlusion management strategies for exploration and orientation. Strategies developed to address the crowdedness in desktop applications, however, inhibit the feeling of immersion. They result in nonimmersive, desktop-style outside-in viewing in virtual reality. This paper proposes Nanotilus---a new visibility and guidance approach for very dense environments that generates an endoscopic inside-out experience instead of outside-in viewing, preserving the immersive aspect of virtual reality. The approach consists of two novel, tightly coupled mechanisms that control scene sparsification simultaneously with camera path planning. The sparsification strategy is localized around the camera and is realized as a multi-scale, multi-shell, variety-preserving technique. When Nanotilus dives into the structures to capture internal details residing on multiple scales, it guides the camera using depth-based path planning. In addition to sparsification and path planning, we complete the tour generation with an animation controller, textual annotation, and text-to-visualization conversion. We demonstrate the generated guided tours on mesoscopic biological models -- SARS-CoV-2 and HIV. We evaluate the Nanotilus experience with a baseline outside-in sparsification and navigational technique in a formal user study with 29 participants. While users can maintain a better overview using the outside-in sparsification, the study confirms our hypothesis that Nanotilus leads to stronger engagement and immersion.</td><td>VR immersive, Visibility management, Path planning, Storytelling, Visualization</td><td>VIS Full Paper</td><td>Storytelling</td><td></td><td><a href="https://youtu.be/c_QMG1BK1G0">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9645360.html">link</a></td></tr><tr><td>62</td><td>How Do Viewers Synthesize Conflicting Information from Data Visualizations?</td><td>Prateek Mantri, Hariharan Subramonyam, Audrey Michal, Cindy Xiong</td><td>Scientific knowledge develops through cumulative discoveries that build on, contradict, contextualize, or correct prior findings. Scientists and journalists often communicate these incremental findings to lay people through visualizations and text (e.g., the positive and negative effects of caffeine intake). Consequently, readers need to integrate diverse and contrasting evidence from multiple sources to form opinions or make decisions. However, the underlying mechanism for synthesizing information from multiple visualizations remains under-explored. To address this knowledge gap, we conducted a series of four experiments (N = 1166) in which participants synthesized empirical evidence from a pair of line charts presented sequentially. In Experiment 1, we administered a baseline condition with charts depicting no specific context where participants held no strong belief. To test for the generalizability, we introduced real-world scenarios to our visualizations in Experiment 2 and added accompanying text descriptions similar to online news articles or blog posts in Experiment 3. In all three experiments, we varied the relative direction and magnitude of line slopes within the chart pairs. We found that participants tended to weigh the positive slope more when the two charts depicted relationships in the opposite direction (e.g., one positive slope and one negative slope). Participants tended to weigh the less steep slope more when the two charts depicted relationships in the same direction (e.g., both positive). Through these experiments, we characterize participants&#x27; synthesis behaviors depending on the relationship between the information they viewed, contribute to theories describing underlying cognitive mechanisms in information synthesis, and describe design implications for data storytelling.</td><td></td><td>VIS Full Paper</td><td>Storytelling</td><td></td><td><a href="https://youtu.be/Un-d8rKSqrs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1103.html">link</a></td></tr><tr><td>63</td><td>RankAxis: Towards a Systematic Combination of Projection and Ranking in Multi-Attribute Data Exploration</td><td>Qiangqiang Liu, Yukun Ren, Zhihua Zhu, Dai Li, Xiaojuan Ma, Quan Li</td><td>Projection and ranking are frequently used analysis techniques in multi-attribute data exploration. Both families of techniques help analysts with tasks such as identifying similarities between observations and determining ordered subgroups, and have shown good performances in multi-attribute data exploration. However, they often exhibit problems such as distorted projection layouts, obscure semantic interpretations, and non-intuitive effects produced by selecting a subset of (weighted) attributes. Moreover, few studies have attempted to combine projection and ranking into the same exploration space to complement each other&#x27;s strengths and weaknesses. For this reason, we propose RankAxis, a visual analytics system that systematically combines projection and ranking to facilitate the mutual interpretation of these two techniques and jointly support multi-attribute data exploration. A real-world case study, expert feedback, and a user study demonstrate the efficacy of RankAxis.</td><td></td><td>VIS Full Paper</td><td>Interactive Dimensionality (High Dimensional Data)</td><td></td><td><a href="https://youtu.be/DmA0g8UlNjU">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1525.html">link</a></td></tr><tr><td>64</td><td>PC-Expo: A Metrics-Based Interactive Axes Reordering Method for Parallel Coordinate Displays</td><td>Anjul Kumar Tyagi, Tyler Estro, Geoff Kuenning, Erez Zadok, Klaus Mueller</td><td>Parallel coordinate plots (PCPs) have been widely used for high-dimensional (HD) data storytelling because they allow presenting a large number of dimensions without distortions. The axes ordering in PCP presents a particular story from the data based
 on the user perception of PCP polylines. Existing works focus on directly optimizing for PCP axes ordering based on some common analysis tasks like clustering, neighborhood, and correlation. However, direct optimization for PCP axes based on these common properties is restrictive because it does not account for multiple properties occurring between the axes, and for local properties that occur in small regions in the data. Also, many of these techniques do not support the human-in-the-loop (HIL) paradigm, which is crucial (i) for explainability and (ii) in cases where no single reordering scheme fits the users’ goals. To alleviate these problems, we
 present PC-Expo, a real-time visual analytics framework for all-in-one PCP line pattern detection, and axes reordering. We studied the connection of line patterns in PCPs with different data analysis tasks and datasets. PC-Expo expands prior work on PCP axes
 reordering by developing real-time, local detection schemes for the 12 most common analysis tasks (properties). Users can choose the story they want to present with PCPs by optimizing directly over their choice of properties. These properties can be ranked, or
 combined using individual weights, creating a custom optimization scheme for axes reordering. Users can control the granularity at which they want to work with their detection scheme in the data, allowing exploration of local regions. PC-Expo also supports HIL axes reordering via local-property visualization, which shows the regions of granular activity for every axis pair. Local-property visualization is helpful for PCP axes reordering based on multiple properties, when no single reordering scheme fits the user goals. A comprehensive evaluation done with real users and diverse datasets confirms the efficacy of PC-Expo in data storytelling with PCPs.</td><td></td><td>VIS Full Paper</td><td>Interactive Dimensionality (High Dimensional Data)</td><td></td><td><a href="https://youtu.be/yQD5nKklvN8">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1631.html">link</a></td></tr><tr><td>65</td><td>Incorporation of Human Knowledge into Data Embeddings to Improve Pattern Significance and Interpretability</td><td>Jie Li, Chunqi Zhou</td><td>Embedding is a common technique for analyzing multi-dimensional data. However, the embedding projection cannot always form significant and interpretable visual structures that foreshadow underlying data patterns. We propose an approach that incorporates human knowledge into data embeddings to improve pattern significance and interpretability. The core idea is (1) externalizing tacit human knowledge as explicit sample labels and (2) adding a classification loss in the embedding network to encode samples’ classes. The approach pulls samples of the same class with similar data features closer in the projection, leading to more compact (significant) and class-consistent (interpretable) visual structures. We give an embedding network with a customized classification loss to implement the idea and integrate the network into a visualization system to form a workflow that supports flexible class creation and pattern exploration. Patterns found on open datasets in case studies, subjects’ performance in a user study, and quantitative experiment results illustrate the general usability and effectiveness of the approach.</td><td></td><td>VIS Full Paper</td><td>Interactive Dimensionality (High Dimensional Data)</td><td></td><td><a href="https://youtu.be/oQyBQv530RA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1359.html">link</a></td></tr><tr><td>66</td><td>Interactive Visual Cluster Analysis by Contrastive Dimensionality Reduction</td><td>Jiazhi Xia, Linquan Huang, Weixing Lin, Xin Zhao, Jing Wu, Yang Chen, Ying Zhao, Wei Chen</td><td>We propose a contrastive dimensionality reduction approach (CDR) for interactive visual cluster analysis. Although dimensionality reduction of high-dimensional data is widely used in visual cluster analysis in conjunction with scatterplots, there are several limitations on effective visual cluster analysis. First, it is non-trivial for an embedding to present clear visual cluster separation when keeping neighborhood structures. Second, as cluster analysis is a subjective task, user steering is required. However, it is also non-trivial to enable interactions in dimensionality reduction. To tackle these problems, we introduce contrastive learning into dimensionality reduction for high-quality embedding. We then redefine the gradient of the loss function to the negative pairs to enhance the visual cluster separation of embedding results. Based on the contrastive learning scheme, we employ link-based interactions to steer embeddings. After that, we implement a prototype visual interface that integrates the proposed algorithms and a set of visualizations. Quantitative experiments demonstrate that CDR outperforms existing techniques in terms of preserving correct neighborhood structures and improving visual cluster separation. The ablation experiment demonstrates the effectiveness of gradient redefinition. The user study verifies that CDR outperforms t-SNE and UMAP in the task of cluster identification. We also showcase two use cases on real-world datasets to present the effectiveness of link-based interactions.</td><td></td><td>VIS Full Paper</td><td>Interactive Dimensionality (High Dimensional Data)</td><td></td><td><a href="https://youtu.be/E-TnYjcNyW4">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1350.html">link</a></td></tr><tr><td>67</td><td>Predicting User Preferences of Dimensionality Reduction Embedding Quality</td><td>Cristina Morariu, Adrien Bibal, Rene Cutura, Benoit Frenay, Michael Sedlmair</td><td>A plethora of dimensionality reduction techniques have emerged over the past decades, leaving researchers and analysts with a wide variety of choices for reducing their data, all the more so given some techniques come with additional hyper-parametrization (e.g., t-SNE, UMAP, etc.). Recent studies are showing that people often use dimensionality reduction as a black-box regardless of the specific properties the method itself preserves. Hence, evaluating and comparing 2D embeddings is usually qualitatively decided, by setting embeddings side-by-side and letting human judgment decide which embedding is the best. In this work, we propose a quantitative way of evaluating embeddings, that nonetheless places human perception at the center. We run a comparative study, where we ask people to select &quot;good&#x27;&#x27; and &quot;misleading&#x27;&#x27; views between scatterplots of low-dimensional embeddings of image datasets, simulating the way people usually select embeddings. We use the study data as labels for a set of quality metrics for a supervised machine learning model whose purpose is to discover and quantify what exactly people are looking for when deciding between embeddings. With the model as a proxy for human judgments, we use it to rank embeddings on new datasets, explain why they are relevant, and quantify the degree of subjectivity when people select preferred embeddings.</td><td></td><td>VIS Full Paper</td><td>Interactive Dimensionality (High Dimensional Data)</td><td></td><td><a href="https://youtu.be/NGWxpprFM1A">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1447.html">link</a></td></tr><tr><td>68</td><td>VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks</td><td>Erick Cuenca, Arnaud Sallaberry, Dino Ienco, Pascal Poncelet</td><td>Many real world data can be modeled by a graph with a set of nodes interconnected to each other by multiple relationships. Such a rich graph is called multilayer graph or network. Providing useful visualization tools to support the query process for such graphs is challenging. Although many approaches have addressed the visual query construction, few efforts have been done to provide a contextualized exploration of query results and suggestion strategies to refine the original query. This is due to several issues such as i) the size of the graphs ii) the large number of retrieved results and iii) the way they can be organized to facilitate their exploration. In this paper, we present VERTIGo, a novel visual platform to query, explore and support the analysis of large multilayer graphs. VERTIGo provides coordinated views to navigate and explore the large set of retrieved results at different granularity levels. In addition, the proposed system supports the refinement of the query by visual suggestions to guide the user through the exploration process. Two examples and a user study demonstrate how VERTIGo can be used to perform visual analysis query, exploration, and suggestion) on real world multilayer networks.</td><td>Visual Querying System, Visual Pattern Suggestion, Multilayer Networks</td><td>VIS Full Paper</td><td>Interactive Dimensionality (High Dimensional Data)</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9382912.html">link</a></td></tr><tr><td>69</td><td>Probablement, Wahrscheinlich, Likely ? A Cross-Language Study of How People Verbalize Probabilities in Icon Array Visualizations</td><td>Noëlle Rakotondravony, Yiren Ding, Lane Harrison</td><td>Visualizations today are used across a wide range of languages and cultures. Yet the extent to which language impacts how we reason about data and visualizations remains unclear. In this paper, we explore the intersection of visualization and language through a cross-language study on estimative probability tasks with icon-array visualizations. Across Arabic, English, French, German, and Mandarin, n = 50 participants per language both chose probability expressions — e.g. likely, probable — to describe icon-array visualizations (Vis-to-Expression), and drew icon-array visualizations to match a given expression (Expression-to-Vis). Results suggest that there is no clear one-to-one mapping of probability expressions and associated visual ranges between languages. Several translated expressions fell significantly above or below the range of the corresponding English expressions. Compared to other languages, French and German respondents appear to exhibit high levels of consistency between the visualizations they drew and the words they chose. Participants across languages used similar words when describing scenarios above 80% chance, with more variance in expressions targeting mid-range and lower values. We discuss how these results suggest potential differences in the expressiveness of language as it relates to visualization interpretation and design goals, as well as practical implications for translation efforts and future studies at the intersection of languages, culture, and visualization. Experiment data, source code, and analysis scripts are available at the following repository: https://osf.io/g5d4r/</td><td></td><td>VIS Full Paper</td><td>Natural Language Interaction</td><td></td><td><a href="https://youtu.be/65LbvtWInkg">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1653.html">link</a></td></tr><tr><td>70</td><td>FlowNL: Asking the Flow Data in Natural Languages</td><td>Jieying Huang, Yang Xi, Junnan Hu, Jun Tao</td><td>Flow visualization is essentially a tool to answer domain experts&#x27; questions about flow fields using rendered images. Static flow visualization approaches require domain experts to raise their questions to visualization experts, who develop specific techniques to extract and visualize the flow structures of interest. Interactive visualization approaches allow domain experts to ask the system directly through the visual analytic interface, which provides flexibility to support various tasks. However, in practice, the visual analytic interface may require extra learning effort, which often discourages domain experts and limits its usage in real-world scenarios. In this paper, we propose FlowNL, a novel interactive system with a natural language interface. FlowNL allows users to manipulate the flow visualization system using plain English, which greatly reduces the learning effort. We develop a natural language parser to interpret user intention and translate textual input into a declarative language. We design the declarative language as an intermediate layer between the natural language and the programming language specifically for flow visualization. The declarative language provides selection and composition rules to derive relatively complicated flow structures from primitive objects that encode various kinds of information about scalar fields, flow patterns, regions of interest, connectivities, etc. We demonstrate the effectiveness of FlowNL using multiple usage scenarios and an empirical evaluation.</td><td></td><td>VIS Full Paper</td><td>Natural Language Interaction</td><td></td><td><a href="https://youtu.be/sY9o7WDRbGU">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1577.html">link</a></td></tr><tr><td>71</td><td>Comparison Conundrum and the Chamber of Visualizations: An Exploration of How Language Influences Visual Design</td><td>Aimen Gaba, Vidya Setlur, Arjun Srinivasan, Jane Hoffswell, Cindy Xiong</td><td>The language for expressing comparisons is often complex and nuanced, making supporting natural language-based visual comparison a non-trivial task. To better understand how people reason about comparisons in natural language, we explore a design space of utterances for comparing data entities. We identified different parameters of comparison utterances that indicate what is being compared (i.e., data variables and attributes) as well as how these parameters are specified (i.e., explicitly or implicitly). We conducted a user study with sixteen data visualization experts and non-experts to investigate how they designed visualizations for comparisons in our design space. Based on the rich set of visualization techniques observed, we extracted key design features from the visualizations and synthesized them into a subset of sixteen representative visualization designs. We then conducted a follow-up study to validate user preferences for the sixteen representative visualizations corresponding to utterances in our design space. Findings from these studies suggest guidelines and future directions for designing natural language interfaces and recommendation tools to better support natural language comparisons in visual analytics.</td><td></td><td>VIS Full Paper</td><td>Natural Language Interaction</td><td></td><td><a href="https://youtu.be/yVpPhEb2BNg">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1056.html">link</a></td></tr><tr><td>72</td><td>Explaining with Examples: Lessons Learned from Crowdsourced Introductory Description of Information Visualizations</td><td>Leni Yang; Cindy Xiong; Jason K. Wong; Aoyu Wu; Huamin Qu</td><td>Data visualizations have been increasingly used in oral presentations to communicate data patterns to the general public. Clear verbal introductions of visualizations to explain how to interpret the visually encoded information are essential to convey the takeaways and avoid misunderstandings. We contribute a series of studies to investigate how to effectively introduce visualizations to the audience with varying degrees of visualization literacy. We begin with understanding how people are introducing visualizations. We crowdsource 110 introductions of visualizations and categorize them based on their content and structures. From these crowdsourced introductions, we identify different introduction strategies and generate a set of introductions for evaluation. We conduct experiments to systematically compare the effectiveness of different introduction strategies across four visualizations with 1,080 participants. We find that introductions explaining visual encodings with concrete examples are the most effective. Our study provides both qualitative and quantitative insights into how to construct effective verbal introductions of visualizations in presentations, inspiring further research in data storytelling.</td><td>narrative visualization, oral presentation, introduction</td><td>VIS Full Paper</td><td>Natural Language Interaction</td><td></td><td><a href="https://youtu.be/xDxo8mFnBb8">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9615008.html">link</a></td></tr><tr><td>73</td><td>Towards Natural Language-Based Visualization Authoring</td><td>Yun Wang, Zhitao Hou, Jiaqi Wang, Tongshuang Wu, Leixian Shen, He Huang, Haidong Zhang, Dongmei Zhang</td><td>A key challenge to visualization authoring is the process of getting familiar with the complex user interfaces of authoring tools. Natural Language Interface (NLI) presents promising benefits due to its learnability and usability. However, supporting NLIs for authoring tools requires expertise in natural language processing, while existing NLIs are mostly designed for visual analytic workflow. In this paper, we propose an authoring-oriented NLI pipeline by introducing a structured representation of users’ visualization editing intents, called editing actions, based on a formative study and an extensive survey on visualization construction tools. The editing actions are executable, and thus decouple natural language interpretation and visualization applications as an intermediate layer. We implement a deep learning-based NL interpreter to translate NL utterances into editing actions. The interpreter is reusable and extensible across authoring tools. The authoring tools only need to map the editing actions into tool-specific operations. To illustrate the usages of the NL interpreter, we implement an Excel chart editor and a proof-of-concept authoring tool, VisTalk. We conduct a user study with VisTalk to understand the usage patterns of NL-based authoring systems. Finally, we discuss observations on how users author charts with natural language, as well as implications for future research.</td><td></td><td>VIS Full Paper</td><td>Natural Language Interaction</td><td></td><td><a href="https://youtu.be/tri-4oWpAAg">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1260.html">link</a></td></tr><tr><td>74</td><td>Striking a Balance: Reader Takeaways and Preferences when Integrating Text and Charts</td><td>Chase Stokes, Vidya Setlur, Bridget Cogley, Arvind Satyanarayan, Marti Hearst</td><td>While visualizations are an effective way to represent insights about information, they rarely stand alone. When designing a visualization, text is often added to provide additional context and guidance for the reader. However, there is little experimental evidence to guide designers as to what is the right amount of text to show within a chart, what its qualitative properties should be, and where it should be placed. Prior work also shows variation in personal preferences for charts versus textual representations. In this paper, we explore several research questions about the relative value of textual components of visualizations. 302 participants ranked univariate line charts containing varying amounts of text, ranging from no text (except for the axes) to a written paragraph with no visuals. Participants also described what information they could take away from line charts with text having varying semantic content. We find that heavily annotated charts were not penalized. In fact, participants preferred the charts with the largest number of textual annotations over charts with fewer annotations or text alone. We also find effects of semantic content. For instance, the text that describes statistical or relational components of a chart leads to more takeaways regarding statistics or relational comparisons than text describing elemental or encoded components. Finally, we found different effects for the semantic levels based on the placement of the text on the chart; some kinds of information are best placed in the title, while others should be placed closer to the data. These results have important implications for chart design guidelines and future work pertaining to the combination of text and charts.</td><td></td><td>VIS Full Paper</td><td>Natural Language Interaction</td><td></td><td><a href="https://youtu.be/H8kyOnmvXyc">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1024.html">link</a></td></tr><tr><td>75</td><td>Scientometric Analysis of Interdisciplinary Collaboration and Gender Trends in 30 Years of IEEE VIS Publications</td><td>Ali Sarvghad, Rolando Franqui-Nadal, Rebecca Reznik-Zellen, Ria Chawla, Narges Mahyar</td><td>We present the results of a scientometric analysis of 30 years of IEEE VIS publications between 1990-2020, in which we conducted a multifaceted analysis of interdisciplinary collaboration and gender composition among authors. To this end, we curated BiblioVIS, a bibliometric dataset that contains rich metadata about IEEE VIS publications, including 3032 papers and 6113 authors. One of the main factors differentiating BiblioVIS from similar datasets is the authors&#x27; gender and discipline data, which we inferred through iterative rounds of computational and manual processes. Our analysis shows that, by and large, inter-institutional and interdisciplinary collaboration has been steadily growing over the past 30 years. However, interdisciplinary research was mainly between a few fields, including Computer Science, Engineering and Technology, and Medicine and Health disciplines. Our analysis of gender shows steady growth in women&#x27;s authorship. Despite this growth, the gender distribution is still highly skewed, with men dominating (~75%) of this space. Our predictive analysis of gender balance shows that if the current trends continue, gender parity in the visualization field will not be reached before the third quarter of the century (~2070). Our primary goal in this work is to call the visualization community&#x27;s attention to the critical topics of collaboration, diversity, and gender. Our research offers critical insights through the lens of diversity and gender to help accelerate progress towards a more diverse and representative research community.</td><td>Scientometric, IEEE VIS Publications, Gender, Co-authorship, Collaboration, Interdisciplinary, Inter-institutional</td><td>VIS Full Paper</td><td>Reflecting on Academia and our Field</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9733942.html">link</a></td></tr><tr><td>76</td><td>Thirty-Two Years of IEEE VIS: Authors, Fields of Study and Citations</td><td>Hongtao Hao, Yumian Cui, Zhengxiang Wang, Yea-Seul Kim</td><td>The IEEE VIS Conference (VIS) recently rebranded itself as a unified conference and officially positioned itself within the discipline of Data Science. Driven by this movement, we investigated (1) who contributed to VIS, and (2) where VIS stands in the scientific world. We examined the authors and fields of study of 3,240 VIS publications in the past 32 years based on data collected from OpenAlex and IEEE Xplore, among other sources. We also examined the citation flows from referenced papers (i.e., those referenced in VIS) to VIS, and from VIS to citing papers (i.e., those citing VIS). We found that VIS has been becoming increasingly popular and collaborative. The number of publications, of unique authors, and of participating countries have been steadily growing. Both cross-country collaborations, and collaborations between educational and non-educational affiliations, namely “cross-type collaborations&#x27;&#x27;, are increasing. The dominance of the US is decreasing, and authors from China are now an important part of VIS. In terms of author affiliation types, VIS is increasingly dominated by authors from universities. We found that the topics, inspirations, and influences of VIS research is limited such that (1) VIS, and their referenced and citing papers largely fall into the Computer Science domain, and (2) citations flow mostly between the same set of subfields within Computer Science. Our citation analyses showed that award-winning VIS papers had higher citations. Interactive visualizations, replication data, source code and supplementary material are available at https://32vis.hongtaoh.com and https://osf.io/zkvjm.</td><td></td><td>VIS Full Paper</td><td>Reflecting on Academia and our Field</td><td></td><td><a href="https://youtu.be/6tZqOB_wwFk">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1364.html">link</a></td></tr><tr><td>77</td><td>SD^2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance</td><td>Zhichun Guo, Jun Tao, Siming Chen, Nitesh V. Chawla, Chaoli Wang</td><td>Comprehensively evaluating and comparing researchers&#x27; academic performance is complicated due to the intrinsic complexity of scholarly data. Different scholarly evaluation tasks often require the publication and citation data to be investigated in various manners. In this paper, we present an interactive visualization framework, SD^2, to enable flexible data partition and composition to support various analysis requirements within a single system. SD^2 features the hierarchical histogram, a novel visual representation for flexibly slicing and dicing the data, allowing different aspects of scholarly performance to be studied and compared. We also leverage the state-of-the-art set visualization technique to select individual researchers or combine multiple scholars for comprehensive visual comparison. We conduct multiple rounds of expert evaluation to study the effectiveness and usability of SD^2 and revise the design and system implementation accordingly. The effectiveness of SD^2 is demonstrated via multiple usage scenarios with each aiming to answer a specific, commonly raised question.</td><td>Scholarly performance, publication, citation, hierarchical histogram, visual analytics.</td><td>VIS Full Paper</td><td>Reflecting on Academia and our Field</td><td></td><td><a href="https://youtu.be/A1YbPZsvBFw">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9747941.html">link</a></td></tr><tr><td>78</td><td>In Defence of Visual Analytics Systems: Replies to Critics</td><td>Aoyu Wu, Dazhen Deng, Furui Cheng, Yingcai Wu, Shixia Liu, Huamin Qu</td><td>The last decade has witnessed many visual analytics (VA) systems that make successful applications to wide-ranging domains like urban analytics and explainable AI. However, their research rigor and contributions have been extensively challenged within the visualization community. We come in defence of VA systems by contributing two interview studies for gathering critics and responses to those criticisms. First, we interview 24 researchers to collect criticisms the review comments on their VA work. Through an iterative coding and refinement process, the interview feedback is summarized into a list of 36 common criticisms. Second, we interview 17 researchers to validate our list and collect their responses, thereby discussing implications for defending and improving the scientific values and rigor of VA systems. We highlight that the presented knowledge is deep, extensive, but also imperfect, provocative, and controversial, and thus recommend reading with an inclusive and critical eye. We hope our work can provide thoughts and foundations for conducting VA research and spark discussions to promote the research field forward more rigorously and vibrantly.</td><td></td><td>VIS Full Paper</td><td>Reflecting on Academia and our Field</td><td></td><td><a href="https://youtu.be/KJMstdzDEeY">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1344.html">link</a></td></tr><tr><td>79</td><td>Visualization Design Practices in a Crisis: Behind the Scenes with COVID-19 Dashboard Creators</td><td>Yixuan Zhang, Joseph D Gaggiano, Yifan Sun, Neha Kumar, Clio Andris, Andrea G Parker</td><td>During the COVID-19 pandemic, a number of data visualizations were created to inform the public about the rapidly evolving crisis. Data dashboards, a form of information dissemination used during the pandemic, have facilitated this process by visualizing statistics regarding the number of COVID-19 cases over time. Prior work on COVID-19 visualizations has primarily focused on the design and evaluation of specific visualization systems from technology-centered perspectives. However, little is known about what occurs behind the scenes during the visualization creation processes, given the complex sociotechnical contexts in which they are embedded. Yet, such ecological knowledge is necessary to help characterize the nuances and trajectories of visualization design practices in the wild, as well as generate insights into how creators come to understand and approach visualization design on their own terms and for their own situated purposes. In this research, we conducted a qualitative interview study among dashboard creators from federal agencies, state health departments, mainstream news media outlets, and other organizations that created (often widely-used) COVID-19 dashboards to answer the following questions: how did visualization creators engage in COVID-19 dashboard design, and what tensions, conflicts, and challenges arose during this process? Our findings detail the trajectory of design practices---from creation to expansion, maintenance, and termination---that are shaped by the complex interplay between design goals, tools and technologies, labor, emerging crisis contexts, and public engagement. We particularly examined the tensions between designers and the general public involved in these processes. These conflicts, which often materialized due to a divergence between public demands and standing policies, centered around the type and amount of information to be visualized, how public perceptions shape and are shaped by visualization design, and the strategies utilized to deal with (potential) misinterpretations and misuse of visualizations. Our findings and lessons learned shed light on new ways of thinking in visualization design, focusing on the bundled activities that are invariably involved in human and nonhuman participation throughout the entire trajectory of design practice.</td><td></td><td>VIS Full Paper</td><td>Reflecting on Academia and our Field</td><td></td><td><a href="https://youtu.be/Ub0_LUxcVHA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1152.html">link</a></td></tr><tr><td>80</td><td>Understanding how Designers Find and Use Data Visualization Examples</td><td>Hannah K. Bako, Xinyi Liu, Leilani Battle, Zhicheng Liu</td><td>Examples are useful for inspiring ideas and facilitating implementation in visualization design. However, there is little understanding of how visualization designers use examples, and how computational tools may support such activities. In this paper, we contribute an exploratory study of current practices in incorporating visualization examples. We conducted semi-structured interviews with 15 university students and 15 professional designers. Our analysis focus on two core design activities: searching for examples and utilizing examples. We characterize observed strategies and tools for performing these activities, as well as major challenges that hinder designers’ current workflows. In addition, we identify themes that cut across these two activities: criteria for determining example usefulness, curation practices, and design fixation. Given our findings, we discuss the implications for visualization design and authoring tools and highlight critical areas for future research.</td><td></td><td>VIS Full Paper</td><td>Reflecting on Academia and our Field</td><td></td><td><a href="https://youtu.be/yoS0dYyllqI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1456.html">link</a></td></tr><tr><td>81</td><td>The State of the Art in BGP Visualization Tools: A Mapping of Visualization Techniques to Cyberattack Types</td><td>Justin Raynor, Tarik Crnovrsanin, Sara Di Bartolomeo, Laura South, David Saffo, Cody Dunne</td><td>Internet routing is largely dependent on Border Gateway Protocol (BGP). However, BGP does not have any inherent authentication or integrity mechanisms that help make it secure. Effective security is challenging or infeasible to implement due to high costs, policy employment in these distributed systems, and unique routing behavior. Visualization tools provide an attractive alternative in lieu of traditional security approaches. Several BGP security visualization tools have been developed as a stop-gap in the face of ever-present BGP attacks. Even though the target users, tasks, and domain remain largely consistent across such tools, many diverse visualization designs have been proposed. The purpose of this study is to provide an initial formalization of methods and visualization techniques for BGP cybersecurity analysis. Using PRISMA guidelines, we provide a systematic review and survey of 29 BGP visualization tools with their tasks, implementation techniques, and attacks and anomalies that they were intended for. We focused on BGP visualization tools as the main inclusion criteria to best capture the visualization techniques used in this domain while excluding solely algorithmic solutions and other detection tools that do not involve user interaction or interpretation. We take the unique approach of connecting (1) the actual BGP attacks and anomalies used to validate existing tools with (2) the techniques employed to detect them. In this way, we contribute an analysis of which techniques can be used for each attack type. Furthermore, we can see the evolution of visualization solutions in this domain as new attack types are discovered. This systematic review provides the groundwork for future designers and researchers building visualization tools for providing BGP cybersecurity, including an understanding of the state-of-the-art in this space and an analysis of what techniques are appropriate for each attack type. Our novel security visualization survey methodology---connecting visualization techniques with appropriate attack types---may also assist future researchers conducting systematic reviews of security visualizations. All supplemental materials are available at https://osf.io/tupz6/.</td><td></td><td>VIS Full Paper</td><td>Infrastructure Management</td><td></td><td><a href="https://youtu.be/HRA0KS38DSQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1006.html">link</a></td></tr><tr><td>82</td><td>RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics</td><td>Longfei Chen, Yang Ouyang, Haipeng Zhang, Suting Hong, Quan Li</td><td>Restructuring the regional industrial structure (RIS) has the potential to halt economic recession and achieve revitalization. Understanding the current status and dynamics of RIS will greatly assist in studying and evaluating the current industrial structure. Previous studies have focused on qualitative and quantitative research to rationalize RIS from a macroscopic perspective. Although recent studies have traced information at the industrial enterprise level to complement existing research from a micro perspective, the ambiguity of the underlying variables contributing to the industrial sector and its composition, the dynamic nature, and the large number of multivariant features of RIS records have obscured a deep and fine-grained understanding of RIS. To this end, we propose an interactive visualization system, RISeer, which is based on interpretable machine learning models and enhanced visualizations designed to identify the evolutionary patterns of the RIS and facilitate inter-regional inspection and comparison. Two case studies confirm the effectiveness of our approach, and feedback from experts indicates that RISeer helps them to gain a fine-grained understanding of the dynamics and evolution of the RIS.</td><td></td><td>VIS Full Paper</td><td>Infrastructure Management</td><td></td><td><a href="https://youtu.be/GwkjlVpyIuc">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1329.html">link</a></td></tr><tr><td>83</td><td>PMU Tracker: A Visualization Platform for Epicentric Event Propagation Analysis in the Power Grid</td><td>Anjana Arunkumar, Andrea Pinceti, Lalitha Sankar, Chris Bryan</td><td>The electrical power grid is a critical infrastructure, with disruptions in transmission having severe repercussions on daily activities, across multiple sectors. To identify, prevent, and mitigate such events, power grids are being refurbished as ‘smart’ systems that include the widespread deployment of GPS-enabled phasor measurement units (PMUs). PMUs provide fast, precise, and time-synchronized measurements of voltage and current, enabling real-time wide-area monitoring and control. However, the potential benefits of PMUs, for analyzing grid events like abnormal power oscillations and load fluctuations, are hindered by the fact that these sensors produce large, concurrent volumes of noisy data. In this paper, we describe working with power grid engineers to investigate how this problem can be addressed from a visual analytics perspective. As a result, we have developed PMU Tracker, an event localization tool that supports power grid operators in visually analyzing and identifying power grid events and tracking their propagation through the power grid’s network. As a part of the PMU Tracker interface, we develop a novel visualization technique which we term an epicentric cluster dendrogram, which allows operators to analyze the effects of an event as it propagates outwards from a source location. We robustly validate PMU Tracker with: (1) a usage scenario demonstrating how PMU Tracker can be used to analyze anomalous grid events, and (2) case studies with power grid operators using a real-world interconnection dataset. Our results indicate that PMU Tracker effectively supports the analysis of power grid events; we also demonstrate and discuss how PMU Tracker’s visual analytics approach can be generalized to other domains composed of time-varying networks with epicentric event characteristics.</td><td></td><td>VIS Full Paper</td><td>Infrastructure Management</td><td></td><td><a href="https://youtu.be/VfjyqEqoitQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1161.html">link</a></td></tr><tr><td>84</td><td>ECoalVis: Visual Analysis of Control Strategies in Coal-fired Power Plants</td><td>Shuhan Liu, Di Weng, Yuan Tian, Zikun Deng, Haoran Xu, Xiangyu Zhu, Honglei Yin, Xianyuan Zhan, Yingcai Wu</td><td>Improving the efficiency of coal-fired power plants has numerous benefits. The control strategy is one of the major factors affecting such efficiency. However, due to the complex and dynamic environment inside the power plants, it is hard to extract and evaluate control strategies and their cascading impact across massive sensors. Existing manual and data-driven approaches cannot well support the analysis of control strategies because these approaches are time-consuming and do not scale with the complexity of the power plant systems. Three challenges were identified: a) interactive extraction of control strategies from large-scale dynamic sensor data, b) intuitive visual representation of cascading impact among the sensors in a complex power plant system, and c) time-lag-aware analysis of the impact of control strategies on electricity generation efficiency. By collaborating with energy domain experts, we addressed these challenges with ECoalVis, a novel interactive system for experts to visually analyze the control strategies of coal-fired power plants extracted from historical sensor data. The effectiveness of the proposed system is evaluated with two usage scenarios on a real-world historical dataset and received positive feedback from experts.</td><td></td><td>VIS Full Paper</td><td>Infrastructure Management</td><td></td><td><a href="https://youtu.be/MJtqd5u-h54">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1143.html">link</a></td></tr><tr><td>85</td><td>A Visual Analytics System for Improving Attention-based Traffic Forecasting Models</td><td>Seungmin Jin, Hyunwook Lee, Cheonbok Park, Hyeshin Chu, Yunwon Tae, Jaegul Choo, Sungahn Ko</td><td>With deep learning (DL) outperforming conventional methods for different tasks, much effort has been devoted to utilizing DL in various domains. Researchers and developers in the traffic domain have also designed and improved DL models for forecasting tasks such as estimation of traffic speed and time of arrival. However, there exist many challenges in analyzing DL models due to the black-box property of DL models and complexity of traffic data (i.e., spatio-temporal dependencies). Collaborating with domain experts, we design a visual analytics system, AttnAnalyzer, that enables users to explore how DL models make predictions by allowing effective spatio-temporal dependency analysis. The system incorporates dynamic time warping (DTW) and Granger causality tests for computational spatio-temporal dependency analysis while providing map, table, line chart, and pixel views to assist user to perform dependency and model behavior analysis. For the evaluation, we present three case studies showing how AttnAnalyzer can effectively explore model behaviors and improve model performance in two different road networks. We also provide domain expert feedback.</td><td></td><td>VIS Full Paper</td><td>Infrastructure Management</td><td></td><td><a href="https://youtu.be/JiObcRiyv9Q">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1491.html">link</a></td></tr><tr><td>86</td><td>RCMVis: A Visual Analytics System for Route Choice Modeling</td><td>DongHwa Shin, Jaemin Jo, Bohyoung Kim, Hyunjoo Song, Shin-Hyung Cho, Jinwook Seo</td><td>We present RCMVis, a visual analytics system to support interactive Route Choice Modeling analysis. It aims to model which characteristics of routes, such as distance and the number of traffic lights, affect travelers&#x27; route choice behaviors and how much they affect the choice during their trips. Through close collaboration with domain experts, we designed a visual analytics framework for Route Choice Modeling. The framework supports three interactive analysis stages: exploration, modeling, and reasoning. In the exploration stage, we help analysts interactively explore trip data from multiple origin-destination (OD) pairs and choose a subset of data they want to focus on. To this end, we provide coordinated multiple OD views with different foci that allow analysts to inspect, rank, and compare OD pairs in terms of their multidimensional attributes. In the modeling stage, we integrate a k-medoids clustering method and a path-size logit model into our system to enable analysts to model route choice behaviors from trips with support for feature selection, hyperparameter tuning, and model comparison. Finally, in the reasoning stage, we help analysts rationalize and refine the model by selectively inspecting the trips that strongly support the modeling result. For evaluation, we conducted a case study and interviews with domain experts. The domain experts discovered unexpected insights from numerous modeling results, allowing them to explore the hyperparameter space more effectively to gain better results. In addition, they gained OD- and road-level insights into which data mainly supported the modeling result, enabling further discussion of the model.</td><td>route choice modeling, urban planning, trajectory data, origin-destination, visual analytics</td><td>VIS Full Paper</td><td>Infrastructure Management</td><td></td><td><a href="https://youtu.be/pfBWVUlQun4">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9632413.html">link</a></td></tr><tr><td>87</td><td>Sporthesia: Augmenting Sports Videos Using Natural Language</td><td>Zhutian Chen, Qisen Yang, Xiao Xie, Johanna Beyer, Haijun Xia, Yingcai Wu, Hanspeter Pfister</td><td>Augmented sports videos, which combine visualizations and video effects to present data in actual scenes, can communicate insights engagingly and thus have been increasingly popular for sports enthusiasts around the world. Yet, creating augmented sports videos remains a challenging task, requiring considerable time and video editing skills. On the other hand, sports insights are often communicated using natural languages, such as in commentaries, oral presentations, and articles, but usually lack visual cues. Thus, this work aims to facilitate the creation of augmented sports videos by enabling analysts to directly create visualizations embedded in videos using insights expressed in natural language. To achieve this goal, we proposed a three-step approach – 1) detecting visualizable entities in the text, 2) mapping these entities into visualizations, and 3) scheduling these visualizations to play with the video – and analyzed 155 sports video clips and the accompanying commentaries for accomplishing these steps. Informed by our analysis, we have designed and implemented Sporthesia, a proof-of-concept system that takes racket-based sports videos and textual commentaries as the input and outputs augmented videos. We demonstrate Sporthesia’s applicability in two exemplar scenarios, i.e., authoring augmented sports videos using text and augmenting historical sports videos based on auditory comments. A technical evaluation shows that Sporthesia achieves high accuracy (F1-score of 0.9) in detecting visualizable entities in the text. An expert evaluation with eight sports analysts suggests high utility, effectiveness, and satisfaction with our language-driven authoring method and provides insights for future improvement and opportunities.</td><td></td><td>VIS Full Paper</td><td>Sports Vis</td><td></td><td><a href="https://youtu.be/IV42XpeiaoQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1238.html">link</a></td></tr><tr><td>88</td><td>OBTracker: Visual Analytics of Off-ball Movements in Basketball</td><td>Yihong Wu, Dazhen Deng, Xiao Xie, Moqi He, Jie Xu, Hongzeng Zhang, Hui Zhang, Yingcai Wu</td><td>In a basketball play, players who are not in possession of the ball (i.e., off-ball players) can still effectively contribute to the team’s offense, such as making a sudden move to create scoring opportunities. Analyzing the movements of off-ball players can thus facilitate the development of effective strategies for coaches. However, common basketball statistics (e.g., points and assists) primarily focus on what happens around the ball and are mostly result-oriented, making it challenging to objectively assess and fully understand the contributions of off-ball movements. To address these challenges, we collaborate closely with domain experts and summarize the multi-level requirements for off-ball movement analysis in basketball. We first establish an assessment model to quantitatively evaluate the offensive contribution of an off-ball movement considering both the position of players and the team cooperation. Based on the model, we design and develop a visual analytics system called OBTracker to support the multifaceted analysis of off-ball movements. OBTracker enables users to identify the frequency and effectiveness of off-ball movement patterns and learn the performance of different off-ball players. A tailored visualization based on the Voronoi diagram is proposed to help users interpret the contribution of off-ball movements from a temporal perspective. We conduct two case studies based on the tracking data from NBA games and demonstrate the effectiveness and usability of OBTracker through expert feedback.</td><td></td><td>VIS Full Paper</td><td>Sports Vis</td><td></td><td><a href="https://youtu.be/iKKbQScLptg">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1083.html">link</a></td></tr><tr><td>89</td><td>Visualization in Motion: A Research Agenda and Two Evaluations</td><td>Lijie Yao, Anastasia Bezerianos, Romain Vuillemot, Petra Isenberg</td><td>We contribute a research agenda for visualization in motion and two experiments to understand how well viewers can read data from moving visualizations. We define visualizations in motion as visual data representations that are used in contexts that exhibit relative motion between a viewer and an entire visualization. Sports analytics, video games, wearable devices, or data physicalizations are example contexts that involve different types of relative motion between a viewer and a visualization. To analyze the opportunities and challenges for designing visualization in motion, we show example scenarios and outline a first research agenda. Motivated primarily by the prevalence of and opportunities for visualizations in sports and video games we started to investigate a small aspect of our research agenda: the impact of two important characteristics of motion---speed and trajectory on a stationary viewer&#x27;s ability to read data from moving donut and bar charts. We found that increasing speed and trajectory complexity did negatively affect the accuracy of reading values from the charts and that bar charts were more negatively impacted. In practice, however, this impact was small: both charts were still read fairly accurately.</td><td>Situated Visualization, Visualization in Motion, Research Agenda, Empirical Study</td><td>VIS Full Paper</td><td>Sports Vis</td><td></td><td><a href="https://youtu.be/t0j6YT_DJAE">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9802784.html">link</a></td></tr><tr><td>90</td><td>RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports</td><td>Jiang Wu, Dongyu Liu, Ziyang Guo, Yingcai Wu</td><td>Experts in racket sports like tennis and badminton use tactical analysis to gain insight into competitors&#x27; playing styles. Many data-driven methods apply pattern mining to racket sports data — which is often recorded as multivariate event sequences — to uncover sports tactics. However, tactics obtained in this way are often inconsistent with those deduced by experts through their domain knowledge, which can be confusing to those experts. This work introduces RASIPAM, a RAcket-Sports Interactive PAttern Mining system, which allows experts to incorporate their knowledge into data mining algorithms to discover meaningful tactics interactively. RASIPAM consists of a constraint-based pattern mining algorithm that responds to the analysis demands of experts: Experts provide suggestions for finding tactics in intuitive written language, and these suggestions are translated into constraints to run the algorithm. RASIPAM further introduces a tailored visual interface that allows experts to compare the new tactics with the original ones and decide whether to apply a given adjustment. This interactive workflow iteratively progresses until experts are satisfied with all tactics. We conduct a quantitative experiment to show that our algorithm supports real-time interaction. Two case studies in tennis and in badminton respectively, each involving two domain experts, are conducted to show the effectiveness and usefulness of the system.</td><td></td><td>VIS Full Paper</td><td>Sports Vis</td><td></td><td><a href="https://youtu.be/Wr956aLgcmU">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1041.html">link</a></td></tr><tr><td>91</td><td>Tac-Trainer: A Visual Analytics System for IoT-based Racket Sports Training</td><td>Jiachen Wang, Ji Ma, Kangping Hu, Zheng Zhou, Hui Zhang, Xiao Xie, Yingcai Wu</td><td>Conventional racket sports training highly relies on coaches&#x27; knowledge and experience, leading to biases in the guidance. To solve this problem, smart wearable devices based on Internet of Things technology (IoT) have been extensively investigated to support data-driven training. Considerable studies introduced methods to extract valuable information from the sensor data collected by IoT devices. However, the information cannot provide actionable insights for coaches due to the large data volume and high data dimensions. We proposed an IoT + VA framework, Tac-Trainer, to integrate the sensor data, the information, and coaches&#x27; knowledge to facilitate racket sports training. Tac-Trainer consists of four components: device configuration, data interpretation, training optimization, and result visualization. These components collect trainees&#x27; kinematic data through IoT devices, transform the data into attributes and indicators, generate training suggestions, and provide an interactive visualization interface for exploration, respectively. We further discuss new research opportunities and challenges inspired by our work from two perspectives, VA for IoT and IoT for VA.</td><td></td><td>VIS Full Paper</td><td>Sports Vis</td><td></td><td><a href="https://youtu.be/HZ-wn30pEzA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1031.html">link</a></td></tr><tr><td>92</td><td>The Quest for Omnioculars: Embedded Visualization for Augmenting Basketball Game Viewing Experiences</td><td>Tica Lin, Zhutian Chen, Yalong Yang, Daniele Chiappalupi, Johanna Beyer, Hanspeter Pfister</td><td>Sports game data is becoming increasingly complex, often consisting of multivariate data such as player performance stats, historical team records, and athletes’ positional tracking information. While numerous visual analytics systems have been developed for sports analysts to derive insights, few tools target fans to improve their understanding and engagement of sports data during live games. By presenting extra data in the actual game views, embedded visualization has the potential to enhance fans’ game-viewing experience. However, little is known about how to design such kinds of visualizations embedded into live games. In this work, we present a user-centered design study of developing interactive embedded visualizations for basketball fans to improve their live game-watching experiences. We first conducted a formative study to characterize basketball fans’ in-game analysis behaviors and tasks. Based on our findings, we propose a design framework to inform the design of embedded visualizations based on specific data-seeking contexts. Following the design framework, we present five novel embedded visualization designs targeting five representative contexts identified by the fans, including shooting, offense, defense, player evaluation, and team comparison. We then developed Omnioculars, an interactive basketball game-viewing prototype that features the proposed embedded visualizations for fans’ in-game data analysis. We evaluated Omnioculars in a simulated basketball game with basketball fans. The study results suggest that our design supports personalized in-game data analysis and enhances game understanding and engagement.</td><td></td><td>VIS Full Paper</td><td>Sports Vis</td><td></td><td><a href="https://youtu.be/6jdNmoT9fAA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1223.html">link</a></td></tr><tr><td>93</td><td>ClinicalPath: a Visualization tool to Improve the Evaluation of Electronic Health Records in Clinical Decision-Making</td><td>Claudio D. G. Linhares, Daniel M. Lima, Jean R. Ponciano, Mauro M. Olivatto, Marco A. Gutierrez, Jorge Poco, Caetano Traina Jr., Agma J. M. Traina</td><td>Physicians work at a very tight schedule and need decision-making support tools to help on improving and doing their work in a timely and dependable manner. Examining piles of sheets with test results and using systems with little visualization support to provide diagnostics is daunting, but that is still the usual way for the physicians&#x27; daily procedure, especially in developing countries. Electronic Health Records systems have been designed to keep the patients&#x27; history and reduce the time spent analyzing the patient&#x27;s data. However, better tools to support decision-making are still needed. In this paper, we propose \systemname, a visualization tool for users to track a patient&#x27;s clinical path through a series of tests and data, which can aid in treatments and diagnoses. Our proposal is focused on patient&#x27;s data analysis, presenting the test results and clinical history longitudinally. Both the visualization design and the system functionality were developed in close collaboration with experts in the medical domain to ensure a right fit of the technical solutions and the real needs of the professionals. We validated the proposed visualization based on case studies and user assessments through tasks based on the physician&#x27;s daily activities. Our results show that our proposed system improves the physicians&#x27; experience in decision-making tasks, made with more confidence and better usage of the physicians&#x27; time, allowing them to take other needed care for the patients.</td><td>Information Visualization, Interactive Visualizations, Human-Computer Interaction, Electronic Health Records</td><td>VIS Full Paper</td><td>Visual Analytics of Health Data</td><td></td><td><a href="https://youtu.be/GcrZzVz-WVs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9779066.html">link</a></td></tr><tr><td>94</td><td>Visual Assistance in Development and Validation of Bayesian Networks for Clinical Decision Support</td><td>Juliane Müller-Sielaff, Seyed Behnam Beladi, Stephanie W. Vrede, Monique Meuschke, Peter J.F. Lucas, Johanna M.A. Pijnenborg, Steffen Oeltze-Jafra</td><td>The development and validation of Clinical Decision Support Models (CDSM) based on Bayesian networks (BN) is commonly done in a collaborative work between medical researchers providing the domain expertise and computer scientists developing the decision support model. Although modern tools provide facilities for data-driven model generation, domain experts are required to validate the accuracy of the learned model and to provide expert knowledge for fine-tuning it while computer scientists are needed to integrate this knowledge in the learned model (hybrid modeling approach). This generally time-expensive procedure hampers CDSM generation and updating. To address this problem, we developed a novel interactive visual approach allowing medical researchers with less knowledge in CDSM to develop and validate BNs based on domain specific data mainly independently and thus, diminishing the need for an additional computer scientist. In this context, we abstracted and simplified the common workflow in BN development as well as adjusted the workflow to medical experts&#x27; needs. We demonstrate our visual approach with data of endometrial cancer patients and evaluated it with six medical researchers who are domain experts in the gynecological field.</td><td>Bayesian networks, Visual Analysis, Clinical Decision Support, Causal Model Development</td><td>VIS Full Paper</td><td>Visual Analytics of Health Data</td><td></td><td><a href="https://youtu.be/nCIBQWqKRn0">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9754243.html">link</a></td></tr><tr><td>95</td><td>ChartWalk: Navigating Large Collections of Text Notes in Electronic Health Records for Clinical Chart Review</td><td>Nicole Sultanum, Farooq Naeem, Michael Brudno, Fanny Chevalier</td><td>Before seeing a patient for the first time, healthcare workers will typically conduct a comprehensive clinical chart review of the patient&#x27;s electronic health record (EHR). Within the diverse documentation pieces included there, text notes are among the most important and thoroughly perused segments for this task; and yet they are among the least supported medium in terms of content navigation and overview. In this work, we delve deeper into the task of clinical chart review from a data visualization perspective and propose a hybrid graphics+text approach via ChartWalk, an interactive tool to support the review of text notes in EHRs. We report on our iterative design process grounded in input provided by a diverse range of healthcare professionals, with steps including: (a) initial requirements distilled from interviews and the literature, (b) an interim evaluation to validate design decisions, and (c) a task-based qualitative evaluation of our final design. We contribute lessons learned to better support the design of tools not only for clinical chart reviews but also other healthcare-related tasks around medical text analysis.</td><td></td><td>VIS Full Paper</td><td>Visual Analytics of Health Data</td><td></td><td><a href="https://youtu.be/SWVZP16HIgQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1228.html">link</a></td></tr><tr><td>96</td><td>Development and Evaluation of Two Approaches of Visual Sensitivity Analysis to Support Epidemiological Modeling</td><td>Erik Rydow, Rita Borgo, Hui Fang, Thomas Torsney-Weir, Ben Swallow, Thibaud Porphyre, Cagatay Turkay, Min Chen</td><td>Computational modeling is a commonly used technology in many scientific disciplines and has played a noticeable role in combating the COVID-19 pandemic. Modeling scientists conduct sensitivity analysis frequently to observe and monitor the behavior of a model during its development and deployment. The traditional algorithmic ranking of sensitivity of different parameters usually does not provide modeling scientists with sufficient information to understand the interactions between different parameters and model outputs, while modeling scientists need to observe a large number of model runs in order to gain actionable information for parameter optimization. To address the above challenge, we developed and compared two visual analytics approaches, namely: algorithm-centric and visualization-assisted, and visualization-centric and algorithm-assisted. We evaluated the two approaches based on a structured analysis of different tasks in visual sensitivity analysis as well as the feedback of domain experts. While the work was carried out in the context of epidemiological modeling, the two approaches developed in this work are directly applicable to a variety of modeling processes featuring time series outputs, and can be extended to work with models with other types of outputs.</td><td></td><td>VIS Full Paper</td><td>Visual Analytics of Health Data</td><td></td><td><a href="https://youtu.be/yA458DmorBQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1111.html">link</a></td></tr><tr><td>97</td><td>A framework for evaluating dashboards in healthcare</td><td>Mengdie Zhuang, David Concannon, Ed Manley</td><td>In the era of ‘information overload’, effective information provision is essential for enabling rapid response and critical decision making. In making sense of diverse information sources, dashboards have become an indispensable tool, providing fast, effective, adaptable, and personalized access to information for professionals and the general public alike. However, these objectives place heavy requirements on dashboards as information systems in usability and effective design. Understanding these issues is challenging given the absence of consistent and comprehensive approaches to dashboard evaluation. In this paper we systematically review literature on dashboard implementation in healthcare, where dashboards have been employed widely, and where there is widespread interest for improving the current state of the art, and subsequently analyse approaches taken towards evaluation. We draw upon consolidated dashboard literature and our own observations to introduce a general definition of dashboards which is more relevant to current trends, together with seven evaluation scenarios - task performance, behaviour change, interaction workflow,  perceived engagement, potential utility, algorithm performance and system implementation. These scenarios distinguish different evaluation purposes which we illustrate through measurements, example studies, and common challenges in evaluation study design. We provide a breakdown of each evaluation scenario, and highlight some of the more subtle questions. We demonstrate the use of the proposed framework by a design study guided by this framework. We conclude by comparing this framework with existing literature,  outlining a number of active discussion points and a set of dashboard evaluation best practices for the academic, clinical and software development communities alike.</td><td>Visualization, Dashboard, Evaluation, Healthcare.</td><td>VIS Full Paper</td><td>Visual Analytics of Health Data</td><td></td><td><a href="https://youtu.be/cOV2kIip4Uo">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9721816.html">link</a></td></tr><tr><td>98</td><td>Extending the Nested Model for User-Centric XAI: A Design Study on GNN-based Drug Repurposing</td><td>Qianwen Wang, Kexin Huang, Payal Chandak, Marinka Zitnik, Nils Gehlenborg</td><td>Whether AI explanations can help users achieve specific tasks efficiently (i.e., usable explanations) is significantly influenced by their visual presentation. While many techniques exist to generate explanations, it remains unclear how to select and visually present AI explanations based on the characteristics of domain users. This paper aims to understand this question through a multidisciplinary design study for a specific problem: explaining graph neural network (GNN) predictions to domain experts in drug repurposing, i.e., reuse of existing drugs for new diseases. Building on the nested design model of visualization, we incorporate XAI design considerations from a literature review and from our collaborators’ feedback into the design process. Specifically, we discuss XAI-related design considerations for usable visual explanations at each design layer: target user, usage context, domain explanation, and XAI goal at the domain layer; format, granularity, and operation of explanations at the abstraction layer; encodings and interactions at the visualization layer; and XAI and rendering algorithm at the algorithm layer. We present how the extended nested model motivates and informs the design of DrugExplorer, an XAI tool for drug repurposing. Based on our domain characterization, DrugExplorer provides path-based explanations and presents them both as individual paths and meta-paths for two key XAI operations, why and what else. DrugExplorer offers a novel visualization design called MetaMatrix with a set of interactions to help domain users organize and compare explanation paths at different levels of granularity to generate domain meaningful insights. We demonstrate the effectiveness of the selected visual presentation and DrugExplorer as a whole via a usage scenario, a user study, and expert interviews. From these evaluations, we derive insightful observations and reflections that can inform the design of XAI visualizations for other scientific applications.</td><td></td><td>VIS Full Paper</td><td>Visual Analytics of Health Data</td><td></td><td><a href="https://youtu.be/0VgjOUTmtjY">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1584.html">link</a></td></tr><tr><td>99</td><td>AI4VIS: Survey on Artificial Intelligence Approaches for Data Visualization</td><td>Aoyu Wu, Yun Wang, Xinhuan Shu, Dominik Moritz, Weiwei Cui, Haidong Zhang, Dongmei Zhang, Huamin Qu</td><td>Visualizations themselves have become a data format. Akin to other data formats such as text and images, visualizations are increasingly created, stored, shared, and (re-)used with artificial intelligence (AI) techniques. In this survey, we probe the underlying vision of formalizing visualizations as an emerging data format and review the recent advance in applying AI techniques to visualization data (AI4VIS). We define visualization data as the digital representations of visualizations in computers and focus on data visualization (e.g., charts and infographics). We build our survey upon a corpus spanning ten different fields in computer science with an eye toward identifying important common interests. Our resulting taxonomy is organized around WHAT is visualization data and its representation, WHY and HOW to apply AI to visualization data. We highlight a set of common tasks that researchers apply to the visualization data and present a detailed discussion of AI approaches developed to accomplish those tasks. Drawing upon our literature review, we discuss several important research questions surrounding the management and exploitation of visualization data, as well as the role of AI in support of those processes.</td><td>Survey; Data Visualization; Artificial Intelligence; Data Format; Machine Learning</td><td>VIS Full Paper</td><td>ML for VIS</td><td></td><td><a href="https://youtu.be/OVDjLYfRwDo">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9495259.html">link</a></td></tr><tr><td>100</td><td>DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific Visualization</td><td>Chaoli Wang, Jun Han</td><td>Since 2016, we have witnessed the tremendous growth of artificial intelligence+visualization (AI+VIS) research. However, existing survey papers on AI+VIS focus on visual analytics and information visualization, not scientific visualization (SciVis). In this paper, we survey related deep learning (DL) works in SciVis, specifically in the direction of DL4SciVis: designing DL solutions for solving SciVis problems. To stay focused, we primarily consider works that handle scalar and vector field data but exclude mesh data. We classify and discuss these works along six dimensions: domain setting, research task, learning type, network architecture, loss function, and evaluation metric. The paper concludes with a discussion of the remaining gaps to fill along the discussed dimensions and the grand challenges we need to tackle as a community. This state-of-the-art survey guides SciVis researchers in gaining an overview of this emerging topic and points out future directions to grow this research.</td><td>Scientific visualization, deep learning, survey</td><td>VIS Full Paper</td><td>ML for VIS</td><td></td><td><a href="https://youtu.be/lsLoo038E9s">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9760126.html">link</a></td></tr><tr><td>101</td><td>A Survey on ML4VIS: Applying Machine Learning Advances to Data Visualization</td><td>Qianwen Wang, Zhutian Chen, Yong Wang, Huamin Qu</td><td>Inspired by the great success of machine learning (ML), researchers have applied ML techniques to visualizations to achieve a better design, development, and evaluation of visualizations. This branch of studies, known as ML4VIS, is gaining increasing research attention in recent years. To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VISis needed. In this paper, we systematically survey ML4VIS studies, aiming to answer two motivating questions: &quot;what visualization processes can be assisted by ML?&quot; and &quot;how ML techniques can be used to solve visualization problems?&quot; This survey reveals seven main processes where the employment of ML techniques can benefit visualizations: Data Processing4VIS, Data-VIS Mapping, Insight Communication, Style Imitation, VIS Interaction, VIS Reading, and User Profiling. The seven processes are related to existing visualization theoretical models in an ML4VIS pipeline, aiming to illuminate the role of ML-assisted visualization in general visualizations. Meanwhile, the seven processes are mapped into main learning tasks in ML to align the capabilities of ML with the needs in visualization. Current practices and future opportunities of ML4VIS are discussed in the context of the ML4VIS pipeline and the ML-VIS mapping. While more studies are still needed in the area of ML4VIS, we hope this paper can provide a stepping-stone for future exploration. A web-based interactive browser of this survey is available at https://ml4vis.github.io</td><td>ML4VIS, Machine Learning, Data Visualization, Survey</td><td>VIS Full Paper</td><td>ML for VIS</td><td></td><td><a href="https://youtu.be/MW-9j_aYGsI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9523770.html">link</a></td></tr><tr><td>102</td><td>Reinforcement Learning for Load-balanced Parallel Particle Tracing</td><td>Jiayi Xu, Hanqi Guo, Han-Wei Shen, Mukund Raj, Skylar W. Wurster, Tom Peterka</td><td>We explore an online reinforcement learning (RL) paradigm to dynamically optimize parallel particle tracing performance in distributed-memory systems. Our method combines three novel components: (1) a work donation algorithm, (2) a high-order workload estimation model, and (3) a communication cost model. First, we design an RL-based work donation algorithm. Our algorithm monitors workloads of processes and creates RL agents to donate data blocks and particles from high-workload processes to low-workload processes to minimize program execution time. The agents learn the donation strategy on the fly based on reward and cost functions designed to consider processes&#x27; workload changes and data transfer costs of donation actions. Second, we propose a workload estimation model, helping RL agents estimate the workload distribution of processes in future computations. Third, we design a communication cost model that considers both block and particle data exchange costs, helping RL agents make effective decisions with minimized communication costs. We demonstrate that our algorithm adapts to different flow behaviors in large-scale fluid dynamics, ocean, and weather simulation data. Our algorithm improves parallel particle tracing performance in terms of parallel efficiency, load balance, and costs of I/O and communication for evaluations with up to 16,384 processors.</td><td>Distributed and parallel particle tracing, dynamic load balancing, reinforcement learning.</td><td>VIS Full Paper</td><td>ML for VIS</td><td></td><td><a href="https://youtu.be/shSLzSrraGc">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9706326.html">link</a></td></tr><tr><td>103</td><td>IDLat: An Importance-Driven Latent Generation Method for Scientific Data</td><td>JINGYI SHEN, Haoyu Li, Jiayi Xu, Ayan Biswas, Han-Wei Shen</td><td>Deep learning based latent representations have been widely used for numerous scientific visualization applications such as isosurface similarity analysis, volume rendering, flow field synthesis, and data reduction, just to name a few. However, existing latent representations are mostly generated from raw data in an unsupervised manner, which makes it difficult to incorporate domain interest to control the size of the latent representations and the quality of the reconstructed data. In this paper, we present a novel importance-driven latent representation to facilitate domain-interest-guided scientific data visualization and analysis. We utilize spatial importance maps to represent various scientific interests and take them as the input to a feature transformation network to guide latent generation. We further reduced the latent size by a lossless entropy encoding algorithm trained together with the autoencoder, improving the storage and memory efficiency. We qualitatively and quantitatively evaluate the effectiveness and efficiency of latent representations generated by our method with data from multiple scientific visualization applications.</td><td></td><td>VIS Full Paper</td><td>ML for VIS</td><td></td><td><a href="https://youtu.be/wJkr7i_yRXg">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1018.html">link</a></td></tr><tr><td>104</td><td>DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning</td><td>Dazhen Deng, Aoyu Wu, Huamin Qu, Yingcai Wu</td><td>Analytical dashboards are popular in business intelligence to facilitate insight discovery with multiple charts. However, creating an effective dashboard is highly demanding, which requires users to have adequate data analysis background and be familiar with professional tools, such as Power BI. To create a dashboard, users have to configure charts by selecting data columns and exploring different chart combinations to optimize the communication of insights, which is trial-and-error. Recent research has started to use deep learning methods for dashboard generation to lower the burden of visualization creation. However, such efforts are greatly hindered by the lack of large-scale and high-quality datasets of dashboards. In this work, we propose using deep reinforcement learning to generate analytical dashboards that can use well-established visualization knowledge and the estimation capacity of reinforcement learning. Specifically, we use visualization knowledge to construct a training environment and rewards for agents to explore and imitate human exploration behavior with a well-designed agent network. The usefulness of the deep reinforcement learning model is demonstrated through ablation studies and user studies. In conclusion, our work opens up new opportunities to develop effective ML-based visualization recommenders without beforehand training datasets.</td><td></td><td>VIS Full Paper</td><td>ML for VIS</td><td></td><td><a href="https://youtu.be/nfJAyE7A9zA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1033.html">link</a></td></tr><tr><td>105</td><td>Survey on Visual Analysis of Event Sequence Data</td><td>Yi Guo, Shunan Guo, Zhuochen Jin, Smiti Kaul, David Gotz, Nan Cao</td><td>Event sequence data record series of discrete events in the time order of occurrence. They are commonly observed in a variety of applications ranging from electronic health records to network logs, with the characteristics of large-scale, high-dimensional and heterogeneous. This high complexity of event sequence data makes it difficult for analysts to manually explore and find patterns, resulting in ever-increasing needs for computational and perceptual aids from visual analytics techniques to extract and communicate insights from event sequence datasets. In this paper, we review the state-of-the-art visual analytics approaches, characterize them with our proposed design space, and categorize them based on analytical tasks and applications. From our review of relevant literature, we have also identified several remaining research challenges and future research opportunities.</td><td>Visual Analysis, Event Sequence Data, Visualization</td><td>VIS Full Paper</td><td>VA and ML</td><td></td><td><a href="https://youtu.be/W5fwvoRott4">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9497654.html">link</a></td></tr><tr><td>106</td><td>Towards Better Caption Supervision for Object Detection</td><td>Changjian Chen, Jing Wu, Xiaohan Wang, Shouxing Xiang, Song-Hai Zhang, Qifeng Tang, Shixia Liu</td><td>As training high-performance object detectors requires expensive bounding box annotations, recent methods resort to free available image captions. However, detectors trained on caption supervision perform poorly because captions are usually noisy and cannot provide precise location information. To tackle this issue, we present a visual analysis method, which tightly integrates caption supervision with object detection to mutually enhance each other. In particular, object labels are first extracted from captions, which are utilized to train the detectors. Then, the label information from images is fed into caption supervision for further improvement. To effectively loop users into the object detection process, a node-link-based set visualization supported by a multi-type relational co-clustering algorithm is developed to explain the relationships between the extracted labels and the images with detected objects. The co-clustering algorithm clusters labels and images simultaneously by utilizing both their representations and their relationships. Quantitative evaluations and a case study are conducted to demonstrate the efficiency and effectiveness of the developed method in improving the performance of object detectors.</td><td>Machine learning, interactive visualization, object detection, caption supervision, co-clustering.</td><td>VIS Full Paper</td><td>VA and ML</td><td></td><td><a href="https://youtu.be/_B_aBeJtg7s">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9664269.html">link</a></td></tr><tr><td>107</td><td>FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches</td><td>Angelos Chatzimparmpas, Rafael M. Martins, Kostiantyn Kucher, Andreas Kerren</td><td>The machine learning (ML) life cycle involves a series of iterative steps, from the effective gathering and preparation of the data—including complex feature engineering processes—to the presentation and improvement of results, with various algorithms to choose from in every step. Feature engineering in particular can be very beneficial for ML, leading to numerous improvements such as boosting the predictive results, decreasing computational times, reducing excessive noise, and increasing the transparency behind the decisions taken during the training. Despite that, while several visual analytics tools exist to monitor and control the different stages of the ML life cycle (especially those related to data and algorithms), feature engineering support remains inadequate. In this paper, we present FeatureEnVi, a visual analytics system specifically designed to assist with the feature engineering process. Our proposed system helps users to choose the most important feature, to transform the original features into powerful alternatives, and to experiment with different feature generation combinations. Additionally, data space slicing allows users to explore the impact of features on both local and global scales. FeatureEnVi utilizes multiple automatic feature selection techniques; furthermore, it visually guides users with statistical evidence about the influence of each feature (or subsets of features). The final outcome is the extraction of heavily engineered features, evaluated by multiple validation metrics. The usefulness and applicability of FeatureEnVi are demonstrated with two use cases and a case study. We also report feedback from interviews with two ML experts and a visualization researcher who assessed the effectiveness of our system.</td><td>Feature selection, feature extraction, feature engineering, machine learning, visual analytics, visualization</td><td>VIS Full Paper</td><td>VA and ML</td><td></td><td><a href="https://youtu.be/RUgd1cp603g">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9672706.html">link</a></td></tr><tr><td>108</td><td>A Design Space for Surfacing Content Recommendations in Visual Analytic Platforms</td><td>Zhilan Zhou, Wenyuan Wang, Mengtian Guo, Yue Wang, David Gotz</td><td>Recommendation algorithms have been leveraged in various ways within visualization systems to assist users as they perform of a range of information tasks. One common focus for these techniques has been the recommendation of content, rather than visual form, as a means to assist users in the identification of information that is relevant to their task context. A wide variety of techniques have been proposed to address this general problem, with a range of design choices in how these solutions surface relevant information to users. This paper reviews the state-of-the-art in how visualization systems surface recommended content to users during users&#x27; visual analysis; introduces a four-dimensional design space for visual content recommendation based on a characterization of prior work; and discusses key observations regarding common patterns and future research opportunities.</td><td></td><td>VIS Full Paper</td><td>VA and ML</td><td></td><td><a href="https://youtu.be/CIdzyf9xHIs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1569.html">link</a></td></tr><tr><td>109</td><td>Diverse Interaction Recommendation for Public Users Exploring Multi-view Visualization using Deep Learning</td><td>Yixuan Li, Yusheng Qi, Yang Shi, Qing Chen, Nan Cao, Siming Chen</td><td>Interaction is an important channel to offer users insights in interactive visualization systems. However, which interaction to operate and which part of data to explore are hard questions for public users facing a multi-view visualization for the first time. Making these decisions largely relies on professional experience and analytic abilities, which is a huge challenge for non-professionals. To solve the problem, we propose a method aiming to provide diverse, insightful, and real-time interaction recommendations for novice users. Building on the Long-Short Term Memory Model (LSTM) structure, our model captures users&#x27; interactions and visual states and encodes them in numerical vectors to make further recommendations. Through an illustrative example of a visualization system about Chinese poets in the museum scenario, the model is proven to be workable in systems with multi-views and multiple interaction types. A further user study demonstrates the method&#x27;s capability to help public users conduct more insightful and diverse interactive explorations and gain more accurate data insights.</td><td></td><td>VIS Full Paper</td><td>VA and ML</td><td></td><td><a href="https://youtu.be/D73x0sHuQ4Q">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1452.html">link</a></td></tr><tr><td>110</td><td>Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data</td><td>Simon Alexander Warchol, Robert Krüger, Ajit Johnson Nirmal, Giorgio Gaglia, Jared Jessup Jessup, Cecily C. Ritch, John Hoffer, Jeremy Muhlich, Megan L Burger, Tyler Jacks, Sandro Santagata Santagata, Peter Sorger, Hanspeter Pfister</td><td>New highly-multiplexed imaging technologies have enabled the study of tissues in unprecedented detail. These methods are increasingly being applied to understand how cancer cells and immune response change during tumor development, progression, and metastasis, as well as following treatment. Yet, existing analysis approaches focus on investigating small tissue samples on a per-cell basis, not taking into account the spatial proximity of cells, which indicates cell-cell interaction and specific biological processes in the larger cancer microenvironment. We present Visinity, a scalable visual analytics system to analyze cell interaction patterns across cohorts of whole-slide multiplexed tissue images. Our approach is based on a fast regional neighborhood computation, leveraging unsupervised learning to quantify, compare, and group cells by their surrounding cellular neighborhood. These neighborhoods can be visually analyzed in an exploratory and confirmatory workflow. Users can explore spatial patterns present across tissues through a scalable image viewer and coordinated views highlighting the neighborhood composition and spatial arrangements of cells. To verify or refine existing hypotheses, users can query for specific patterns to determine their presence and statistical significance. Findings can be interactively annotated, ranked, and compared in the form of small multiples. In two case studies with biomedical experts, we demonstrate that Visinity can identify common biological processes within a human tonsil and uncover novel white-blood cell networks and immune-tumor interactions.</td><td></td><td>VIS Full Paper</td><td>VA and ML</td><td></td><td><a href="https://youtu.be/J3Vd6SEQrH0">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1674.html">link</a></td></tr><tr><td>111</td><td>GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks</td><td>Zhihua Jin, Yong Wang, Qianwen Wang, Yao Ming, Tengfei Ma, Huamin Qu</td><td>Graph Neural Networks (GNNs) aim to extend deep learning techniques to graph data and have achieved significant progress in graph analysis tasks (e.g., node classification) in recent years. However, similar to other deep neural networks like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), GNNs behave like a black box with their details hidden from model developers and users. It is therefore difficult to diagnose possible errors of GNNs. Despite many visual analytics studies being done on CNNs and RNNs, little research has addressed the challenges for GNNs. This paper fills the research gap with an interactive visual analysis tool, GNNLens, to assist model developers and users in understanding and analyzing GNNs. Specifically, Parallel Sets View and Projection View enable users to quickly identify and validate error patterns in the set of wrong predictions; Graph View and Feature Matrix View offer a detailed analysis of individual nodes to assist users in forming hypotheses about the error patterns. Since GNNs jointly model the graph structure and the node features, we reveal the relative influences of the two types of information by comparing the predictions of three models: GNN, Multi-Layer Perceptron (MLP), and GNN Without Using Features (GNNWUF). Two case studies and interviews with domain experts demonstrate the effectiveness of GNNLens in facilitating the understanding of GNN models and their errors.</td><td>Graph Neural Networks, Error Diagnosis, Visualization</td><td>VIS Full Paper</td><td>VA for ML</td><td></td><td><a href="https://youtu.be/VRHrydpUPxQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9705076.html">link</a></td></tr><tr><td>112</td><td>Visual Analysis of Neural Architecture Spaces for Summarizing Design Principles</td><td>Jun Yuan, Mengchen Liu, Fengyuan Tian, Shixia Liu</td><td>Recent advances in artificial intelligence largely benefit from better neural network architectures. These architectures are a product of a costly process of trial-and-error. To ease this process, we develop ArchExplorer, a visual analysis method for understanding a neural architecture space and summarizing design principles. The key idea behind our method is to make the architecture space explainable by exploiting structural distances between architectures. We formulate the calculation of the pairwise distances as solving an all-pairs shortest path problem. To improve efficiency, we decompose this problem into a set of single-source shortest path problems. The time complexity is reduced from O(kn^2N) to O(knN). Architectures are hierarchically clustered according to the distances between them. A circle-packing-based architecture visualization has been developed to convey both the global relationships between clusters and local neighborhoods of the architectures in each cluster. Two case studies and a post-analysis are presented to demonstrate the effectiveness of ArchExplorer in summarizing design principles and selecting better-performing architectures.</td><td></td><td>VIS Full Paper</td><td>VA for ML</td><td></td><td><a href="https://youtu.be/xvRd7LIOjLA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1119.html">link</a></td></tr><tr><td>113</td><td>NAS-Navigator: Visual Steering for Explainable One-Shot Deep Neural Network Synthesis</td><td>Anjul Kumar Tyagi, Cong Xie, Klaus Mueller</td><td>Recent advancements in deep learning have shown the effectiveness of deep neural networks in several applications. The success of deep learning can be attributed to hours of parameter and architecture tuning by human experts. Neural Architecture Search (NAS) techniques aim to solve this problem by automating the search procedure for deep neural network architectures making it possible for non-experts to work with deep learning. Specifically, One-shot NAS techniques have recently gained popularity as they are known to reduce the search time for NAS techniques. One-Shot NAS works by training a large template network through parameter sharing which includes all the candidate neural networks. This is followed by applying a procedure to rank its components through evaluating the possible candidate architectures chosen randomly. However, as these search models become increasingly powerful and diverse, they become harder to understand. Consequently, even though the search results work well, it is hard to identify search biases and control the search progression, hence a need for explainability and human-in-the-loop One-Shot NAS. To alleviate these problems, we present NAS-Navigator, a visual analytics system aiming to solve three problems with One-Shot NAS; explainability, human-in-the-loop design, and performance improvements compared to existing state-of-the-art techniques. NAS-Navigator gives full control of NAS back in the hands of the users while still keeping the perks of automated search, thus assisting non-expert users. Analysts can use their domain knowledge aided by cues from the interface to guide the search. Evaluation results confirm the performance of our improved One-Shot NAS algorithm is comparable to other state-of-the-art techniques. While adding visual analytics using NAS-Navigator shows further improvements in search time and performance. We designed our interface in collaboration with several deep learning researchers and evaluated NAS-Navigator through a control experiment and expert interviews.</td><td></td><td>VIS Full Paper</td><td>VA for ML</td><td></td><td><a href="https://youtu.be/xKcNSf9gGko">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1626.html">link</a></td></tr><tr><td>114</td><td>HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning</td><td>Xumeng Wang, Wei Chen, Jiazhi Xia, Zhen Wen, Rongchen Zhu, Tobias Schreck</td><td>Horizontal federated learning (HFL) enables distributed clients to train a shared model and keep their data privacy. In training high-quality HFL models, the data heterogeneity among clients is one of the major concerns. However, due to the security issue and the complexity of deep learning models, it is challenging to investigate data heterogeneity across different clients. To address this issue, based on a requirement analysis we developed a visual analytics tool, HetVis, for participating clients to explore data heterogeneity. We identify data heterogeneity through comparing prediction behaviors of the global federated model and the stand-alone model trained with local data. Then, a context-aware clustering of the inconsistent records is done, to provide a summary of data heterogeneity. Combining with the proposed comparison techniques, we develop a novel set of visualizations to identify heterogeneity issues in HFL. We designed three case studies to introduce how HetVis can assist client analysts in understanding different types of heterogeneity issues. Expert reviews and a comparative study demonstrate the effectiveness of HetVis.</td><td></td><td>VIS Full Paper</td><td>VA for ML</td><td></td><td><a href="https://youtu.be/mN4rlnMSD7Y">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1338.html">link</a></td></tr><tr><td>115</td><td>DendroMap: Visual Exploration of Large-Scale Image Datasets for Machine Learning with Treemaps</td><td>Donald R Bertucci, Md Montaser Hamid, Yashwanthi Anand, Anita Ruangrotsakun, Delyar Tabatabai, Melissa Perez, Minsuk Kahng</td><td>In this paper, we present DendroMap, a novel approach to interactively exploring large-scale image datasets for machine learning (ML). ML practitioners often explore image datasets by generating a grid of images or projecting high-dimensional representations of images into 2-D using dimensionality reduction techniques (e.g., t-SNE). However, neither approach effectively scales to large datasets because images are ineffectively organized and interactions are insufficiently supported. To address these challenges, we develop DendroMap by adapting Treemaps, a well-known visualization technique. DendroMap effectively organizes images by extracting hierarchical cluster structures from high-dimensional representations of images. It enables users to make sense of the overall distributions of datasets and interactively zoom into specific areas of interests at multiple levels of abstraction. Our case studies with widely-used image datasets for deep learning demonstrate that users can discover insights about datasets and trained models by examining the diversity of images, identifying underperforming subgroups, and analyzing classification errors. We conducted a user study that evaluates the effectiveness of DendroMap in grouping and searching tasks by comparing it with a gridified version of t-SNE and found that participants preferred DendroMap. DendroMap is available at https://div-lab.github.io/dendromap/.</td><td></td><td>VIS Full Paper</td><td>VA for ML</td><td></td><td><a href="https://youtu.be/cZAoAEcMW6I">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1205.html">link</a></td></tr><tr><td>116</td><td>Diagnosing Ensemble Few-Shot Classifiers</td><td>Weikai Yang, Xi Ye, Xingxing Zhang, Lanxi Xiao, Jiazhi Xia, Zhongyuan Wang, Jun Zhu, Hanspeter Pfister, Shixia Liu</td><td>The base learners and labeled samples (shots) in an ensemble few-shot classifier greatly affect the model performance. When the performance is not satisfactory, it is usually difficult to understand the underlying causes and make improvements. To tackle this issue, we propose a visual analysis method, FSLDiagnotor. Given a set of base learners and a collection of samples with a few shots, we consider two problems: 1) finding a subset of base learners that well predict the sample collections; and 2) replacing the low-quality shots with more representative ones to adequately represent the sample collections. We formulate both problems as sparse subset selection and develop two selection algorithms to recommend appropriate learners and shots, respectively. A matrix visualization and a scatterplot are combined to explain the recommended learners and shots in context and facilitate users in adjusting them. Based on the adjustment, the algorithm updates the recommendation results for another round of improvement. Two case studies are conducted to demonstrate that FSLDiagnotor helps build a few-shot classifier efficiently and increases the accuracy by 12% and 21%, respectively.</td><td>Few-shot learning, ensemble model, subset selection, matrix visualization, scatterplot</td><td>VIS Full Paper</td><td>VA for ML</td><td></td><td><a href="https://youtu.be/Lf72Zn8M-6U">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9795241.html">link</a></td></tr><tr><td>117</td><td>MV2Net: Multi-Variate Multi-View Brain Network Comparison over Uncertain Data</td><td>Lei Shi, Junnan Hu, Zhihao Tan, Jun Tao, Jiayan Ding, Yan Jin, Yanjun Wu, Paul M. Thompson</td><td>Visually identifying effective bio-markers from human brain networks poses non-trivial challenges to the field of data visualization and analysis. Existing methods in the literature and neuroscience practice are generally limited to the study of individual connectivity features in the brain (e.g., the strength of neural connection among brain regions). Pairwise comparisons between contrasting subject groups (e.g., the diseased and the healthy controls) are normally performed. The underlying neuroimaging and brain network construction process is assumed to have 100% fidelity. Yet, real-world user requirements on brain network visual comparison lean against these assumptions. In this work, we present MV^2Net, a visual analytics system that tightly integrates multi-variate multi-view visualization for brain network comparison with an interactive wrangling mechanism to deal with data uncertainty. On the analysis side, the system integrates multiple extraction methods on diffusion and geometric connectivity features of brain networks, an anomaly detection algorithm for data quality assessment, single- and multi-connection feature selection methods for bio-marker detection. On the visualization side, novel designs are introduced which optimize network comparisons among contrasting subject groups and related connectivity features. Our design provides level-of-detail comparisons, from juxtaposed and explicit-coding views for subject group comparisons, to high-order composite view for correlation of network comparisons, and to fiber tract detail view for voxel-level comparisons. The proposed techniques are inspired and evaluated in expert studies, as well as through case analyses on diffusion and geometric bio-markers of certain neurology diseases. Results in these experiments demonstrate the effectiveness and superiority of MV^2Net over state-of-the-art approaches.</td><td>brain network, visual comparison, multivariate analysis</td><td>VIS Full Paper</td><td>Neuro/Brain/Medical Data</td><td></td><td><a href="https://youtu.be/yg_uqXsR6gc">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9492002.html">link</a></td></tr><tr><td>118</td><td>NeuroConstruct: 3D Reconstruction and Visualization of Neurites in Optical Microscopy Brain Images</td><td>Parmida Ghahremani, Saeed Boorboor, Pooya Mirhosseini, Chetan Gudisagar, Mala Ananth, David Talmage, Lorna W. Role, Arie E. Kaufman</td><td>We introduce NeuroConstruct, a novel end-to-end application for the segmentation, registration, and visualization of brain volumes imaged using wide-field microscopy. NeuroConstruct offers a Segmentation Toolbox with various annotation helper functions that aid experts to effectively and precisely annotate micrometer resolution neurites. It also offers an automatic neurites segmentation using convolutional neuronal networks (CNN) trained by the Toolbox annotations and somas segmentation using thresholding. To visualize neurites in a given volume, NeuroConstruct offers a hybrid rendering by combining iso-surface rendering of high-confidence classified neurites, along with real-time rendering of raw volume using a 2D transfer function for voxel classification score vs. voxel intensity value. For a complete reconstruction of the 3D neurites, we introduce a Registration Toolbox that provides automatic coarse-to-fine alignment of serially sectioned samples. The quantitative and qualitative analysis show that NeuroConstruct outperforms the state-of-the-art in all design aspects. NeuroConstruct was developed as a collaboration between computer scientists and neuroscientists, with an application to the study of cholinergic neurons, which are severely affected in Alzheimer’s disease.</td><td>Wide-field Microscopy,Neuron Morphology,Segmentation,Image Processing and Computer Vision,Computing Methodologies,Registration,Hybrid Volume Rendering,CNN</td><td>VIS Full Paper</td><td>Neuro/Brain/Medical Data</td><td></td><td><a href="https://youtu.be/a31Nhbn-H94">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9529035.html">link</a></td></tr><tr><td>119</td><td>DXplorer: A Unified Visualization Framework for Interactive Dendritic Spine Analysis using 3D morphological Features</td><td>JunYoung Choi, Sang-Eun Lee, YeIn Lee, Eunji Cho, Sunghoe Chang, Won-Ki Jeong</td><td>Dendritic spines are dynamic, submicron-scale protrusions on neuronal dendrites that receive neuronal inputs. Morphological changes in the dendritic spine often reflect alterations in physiological conditions and are indicators of various neuropsychiatric conditions. However, owing to the highly dynamic and heterogeneous nature of spines, accurate measurement and objective analysis of spine morphology are major challenges in neuroscience research. Most conventional approaches for analyzing dendritic spines are based on two-dimensional (2D) images, which barely reflect the actual three-dimensional (3D) shapes. Although some recent studies have attempted to analyze spines with various 3D-based features, it is still difficult to objectively categorize and analyze spines based on 3D morphology. Here, we propose a unified visualization framework for an interactive 3D dendritic spine analysis system, DXplorer, that displays 3D rendering of spines and plots the high-dimensional features extracted from the 3D mesh of spines. With this system, users can perform the clustering of spines interactively and explore and analyze dendritic spines based on high-dimensional features. We propose a series of high-dimensional morphological features extracted from a 3D mesh of dendritic spines. In addition, an interactive machine learning classifier with visual exploration and user feedback using an interactive 3D mesh grid view ensures a more precise classification based on the spine phenotype. A user study and two case studies were conducted to quantitatively verify the performance and usability of the DXplorer. We demonstrate that the system performs the entire analytic process effectively and provides high-quality, accurate, and objective analysis.</td><td>Biomedical and Medical Visualization, Machine Learning, Task and Requirements Analysis, User Interfaces, Intelligence Analysis</td><td>VIS Full Paper</td><td>Neuro/Brain/Medical Data</td><td></td><td><a href="https://youtu.be/ZJsqMOEzkQU">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9555234.html">link</a></td></tr><tr><td>120</td><td>A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts</td><td>Chaoqing Xu, Tyson Neuroth, Takanori Fujiwara, Ronghua Liang, Kwan-Liu Ma</td><td>Diffusion tensor imaging (DTI) has been used to study the effects of neurodegenerative diseases on neural pathways, which may lead to more reliable and early diagnosis of these diseases as well as a better understanding of how they affect the brain. We introduce a predictive visual analytics system for studying patient groups based on their labeled DTI fiber tract data and corresponding statistics. The system’s machine-learning-augmented interface guides the user through an organized and holistic analysis space, including the statistical feature space, the physical space, and the space of patients over different groups. We use a custom machine learning pipeline to help narrow down this large analysis space and then explore it pragmatically through a range of linked visualizations. We conduct several case studies using DTI and T1-weighted images from the research database of Parkinson’s Progression Markers Initiative.</td><td>Brain fiber tracts, neurodegenerative disease, machine learning, predictive visual analytics, visualization.</td><td>VIS Full Paper</td><td>Neuro/Brain/Medical Data</td><td></td><td><a href="https://youtu.be/MR6Lk9V2gpM">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9665344.html">link</a></td></tr><tr><td>121</td><td>NeuRegenerate: A Framework for Visualizing Neurodegeneration</td><td>Saeed Boorboor, Shawn Mathew, Mala Ananth, David Talmage, Lorna W. Role, Arie E. Kaufman.</td><td>Recent advances in high-resolution microscopy have allowed scientists to better understand the underlying brain connectivity. However, due to the limitation that biological specimens can only be imaged at a single timepoint, studying changes to neural projections over time is limited to observations gathered using population analysis. In this paper, we introduce NeuRegenerate, a novel end-to-end framework for the prediction and visualization of changes in neural fiber morphology within a subject across specified age-timepoints. To predict projections, we present  neuReGANerator, a deep-learning network based on cycle-consistent generative adversarial network (GAN) that translates features of neuronal structures across age-timepoints for large brain microscopy volumes. We improve the reconstruction quality of the predicted neuronal structures by implementing a density multiplier and a new loss function, called the hallucination loss. Moreover, to alleviate artifacts that occur due to tiling of large input volumes, we introduce a spatial-consistency module in the training pipeline of neuReGANerator. Finally, to visualize the change in projections, predicted using neuReGANerator, NeuRegenerate offers two modes: (i) neuroCompare to simultaneously visualize the difference in the structures of the neuronal projections, from two age domains (using structural view and bounded view), and (ii) em neuroMorph, a vesselness-based morphing technique to interactively visualize the transformation of the structures from one age-timepoint to the other. Our framework is designed specifically for volumes acquired using wide-field microscopy. We demonstrate our framework by visualizing the structural changes within the cholinergic system of the mouse brain between a young and old specimen.</td><td>Neuron visualization, volume visualization, volume transformation, neuroscience, wide-field microscopy, machine learning,</td><td>VIS Full Paper</td><td>Neuro/Brain/Medical Data</td><td></td><td><a href="https://youtu.be/cl2SgxY8KZo">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9610985.html">link</a></td></tr><tr><td>122</td><td>GUCCI - Guided Cardiac Cohort Investigation of Blood Flow Data</td><td>Monique Meuschke, Uli Niemann, Benjamin Behrendt, Matthias Gutberlet, Bernhard Preim, Kai Lawonn</td><td>We present the framework GUCCI (Guided Cardiac Cohort Investigation), which provides a guided visual analytics workflow to analyze cohort-based measured blood flow data in the aorta. In the past, many specialized techniques have been developed for the visual exploration of such data sets for a better understanding of the influence of morphological and hemodynamic conditions on cardiovascular diseases. However, there is a lack of dedicated techniques that allow visual comparison of multiple datasets and defined cohorts, which is essential to characterize pathologies. GUCCI offers visual analytics techniques and novel visualization methods to guide the user through the comparison of predefined cohorts, such as healthy volunteers and patients with a pathologically altered aorta.  The combination of overview and glyph-based depictions together with statistical cohort-specific information allows investigating differences and similarities of the time-dependent data. Our framework was evaluated in a qualitative user study with three radiologists specialized in cardiac imaging and two experts in medical blood flow visualization. They were able to discover cohort-specific characteristics, which supports the derivation of standard values as well as the assessment of pathology-related severity and the need for treatment.</td><td>Medical Visualization, Cohort Analysis, Measured Blood Flow Data, Cardiac Diseases</td><td>VIS Full Paper</td><td>Neuro/Brain/Medical Data</td><td></td><td><a href="https://youtu.be/TyYaCdT42fs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9645173.html">link</a></td></tr><tr><td>123</td><td>Visualizing the Passage of Time with Video Temporal Pyramids</td><td>Melissa E Swift, Wyatt Ayers, Sophie Pallanck, Scott Wehrwein</td><td>What can we learn about a scene by watching it for months or years? A video recorded over a long timespan will depict interesting phenomena at multiple timescales, but identifying and viewing them presents a challenge. The video is too long to watch in full, and some things are too slow to experience in real-time, such as glacial retreat or the gradual shift from summer to fall. Timelapse videography is a common approach to summarizing long videos and visualizing slow timescales. However, a timelapse is limited to a single chosen temporal frequency, and often appears flickery due to aliasing. Also, the length of the timelapse video is directly tied to its temporal resolution, which necessitates tradeoffs between those two facets. In this paper, we propose Video Temporal Pyramids, a technique that addresses these limitations and expands the possibilities for visualizing the passage of time. Inspired by spatial image pyramids from computer vision, we developed an algorithm that builds video pyramids in the temporal domain. Each level of a Video Temporal Pyramid visualizes a different timescale; for instance, videos from the monthly timescale are usually good for visualizing seasonal changes, while videos from the one-minute timescale are best for visualizing sunrise or the movement of clouds across the sky. To help explore the different pyramid levels, we also propose a Video Spectrogram to visualize the amount of activity across the entire pyramid, providing a holistic overview of the scene dynamics and the ability to explore and discover phenomena across time and timescales. To demonstrate our approach, we have built Video Temporal Pyramids from ten outdoor scenes, each containing months or years of data. We compare Video Temporal Pyramid layers to naive timelapse and find that our pyramids enable alias-free viewing of longer-term changes. We also demonstrate that the Video Spectrogram facilitates exploration and discovery of phenomena across pyramid levels, by enabling both overview and detail-focused perspectives.</td><td></td><td>VIS Full Paper</td><td>Temporal Data</td><td></td><td><a href="https://youtu.be/58dm1vK2-V4">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1380.html">link</a></td></tr><tr><td>124</td><td>Constrained Dynamic Mode Decomposition</td><td>Tim Krake, Daniel Klötzl, Bernhard Eberhardt, Daniel Weiskopf</td><td>Frequency-based decomposition of time series data is used in many visualization applications. Most of these decomposition methods (such as Fourier transform or singular spectrum analysis) only provide interaction via pre- and post-processing, but no means to influence the core algorithm. A method that also belongs to this class is Dynamic Mode Decomposition (DMD), a spectral decomposition method that extracts spatio-temporal patterns from data. In this paper, we incorporate frequency-based constraints into DMD for an adaptive decomposition that leads to user-controllable visualizations, allowing analysts to include their knowledge into the process. To accomplish this, we derive an equivalent reformulation of DMD that implicitly provides access to the eigenvalues (and therefore to the frequencies) identified by DMD. By utilizing a constrained minimization problem customized to DMD, we can guarantee the existence of desired frequencies by minimal changes to DMD. We complement this core approach by additional techniques for constrained DMD to facilitate explorative visualization and investigation of time series data. With several examples, we demonstrate the usefulness of constrained DMD and compare it to conventional frequency-based decomposition methods.</td><td></td><td>VIS Full Paper</td><td>Temporal Data</td><td></td><td><a href="https://youtu.be/zc3xLI1wt14">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1289.html">link</a></td></tr><tr><td>125</td><td>SizePairs: Achieving Stable and Balanced Temporal Treemaps using Hierarchical Size-based Pairing</td><td>Chang Han, Anyi Li, Jaemin Jo, Bongshin Lee, Oliver Deussen, Yunhai Wang</td><td>We present SizePairs, a new technique to create stable and balanced treemap layouts that visualize values changing over time in hierarchical data. To achieve an overall high-quality result across all time steps in terms of stability and aspect ratio, SizePairs employs a new hierarchical size-based pairing algorithm that recursively pairs two nodes that complement their size changes over time and have similar sizes. SizePairs maximizes the visual quality and stability by optimizing the splitting orientation of each internal node and flipping leaf nodes, if necessary. We also present a comprehensive comparison of SizePairs against the state-of-the-art treemaps developed for visualizing time-dependent data. SizePairs outperforms existing techniques in both visual quality and stability, while being faster than the local moves technique.</td><td></td><td>VIS Full Paper</td><td>Temporal Data</td><td></td><td><a href="https://youtu.be/R9ntseofJx0">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1163.html">link</a></td></tr><tr><td>126</td><td>LargeNetVis: visual exploration of large temporal networks based on community taxonomies</td><td>Claudio Linhares, Jean Roberto Ponciano, Diogenes Pedro, Luis Rocha, Agma Traina, Jorge Poco</td><td>Temporal (or time-evolving) networks are commonly used to model complex systems and the evolution of their components throughout time.
 Although these networks can be analyzed by different means, visual analytics stands out as an effective way for a pre-analysis before doing quantitative/statistical analyses to identify patterns, anomalies, and other behaviors in the data, thus leading to new insights and better decision-making.
 However, the large number of nodes, edges, and/or timestamps in many real-world networks may lead to polluted layouts that make the analysis inefficient or even infeasible.
 In this paper, we propose LargeNetVis, a web-based visual analytics system designed to assist in analyzing small and large temporal networks. It successfully achieves this goal by leveraging three taxonomies focused on network communities to guide the visual exploration process.
 The system is composed of four interactive visual components: the first (Taxonomy Matrix) presents a summary of the network characteristics, the second (Global View) gives an overview of the network evolution, the third (a node-link diagram) enables community- and node-level structural analysis, and the fourth (a Temporal Activity Map -- TAM) shows the community- and node-level activity under a temporal perspective.
 We demonstrate the usefulness and effectiveness of LargeNetVis through two usage scenarios and a user study with 14 participants.</td><td></td><td>VIS Full Paper</td><td>Temporal Data</td><td></td><td><a href="https://youtu.be/FORqRC5X8qU">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1230.html">link</a></td></tr><tr><td>127</td><td>ASTF: Visual Abstractions of Time-Varying Patterns in Radio Signals</td><td>Ying Zhao, Luhao Ge, Huixuan Xie, Genghuai Bai, Zhao Zhang, Qiang Wei, Yun Lin, Yuchao Liu, Fangfang Zhou</td><td>A time-frequency diagram is a commonly used visualization for observing the time-frequency distribution of radio signals and analyzing their time-varying patterns of communication states in radio monitoring and management. While it excels when performing short-term signal analyses, it becomes inadaptable for long-term signal analyses because it cannot adequately depict signal time-varying patterns in a large time span on a space-limited screen. This research thus presents an abstract signal time-frequency (ASTF) diagram to address this problem. In the diagram design, a visual abstraction method is proposed to visually encode signal communication state changes in time slices. A time segmentation algorithm is proposed to divide a large time span into time slices. Three new quantified metrics and a loss function are defined to ensure the preservation of important time-varying information in the time segmentation. An algorithm performance experiment and a user study are conducted to evaluate the effectiveness of the diagram for long-term signal analyses.</td><td></td><td>VIS Full Paper</td><td>Temporal Data</td><td></td><td><a href="https://youtu.be/Id5k5AXg7is">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1069.html">link</a></td></tr><tr><td>128</td><td>Geometry-Aware Merge Tree Comparisons for Time-Varying Data with Interleaving Distances</td><td>Lin Yan, Talha Bin Masood, Farhan Rasheed, Ingrid Hotz, Bei Wang</td><td>Merge trees, a type of topological descriptor, serve to identify and summarize the topological characteristics associated with scalar fields. They have great potential for analyzing and visualizing time-varying data. First, they give compressed and topology-preserving representations of data instances. Second, their comparisons provide a basis for studying the relations among data instances, such as their distributions, clusters, outliers, and periodicities. A number of comparative measures have been developed for merge trees. However, these measures are often computationally expensive since they implicitly consider all possible correspondences between critical points of the merge trees. In this paper, we perform geometry-aware comparisons of merge trees using labeled interleaving distances. The main idea is to decouple the computation of a comparative measure into two steps: a labeling step that generates a correspondence between the critical points of two merge trees, and a comparison step that computes distances between a pair of labeled merge trees by encoding them as matrices. We show that our approach is general, computationally efficient, and practically useful. Our framework makes it possible to integrate geometric information of the data domain in the labeling process. At the same time, the framework reduces the computational complexity since not all possible correspondences have to be considered. We demonstrate via experiments that such geometry-aware merge tree comparisons help to detect transitions, clusters, and periodicities of time-varying datasets, as well as to diagnose and highlight the topological changes between adjacent data instances.</td><td>Merge trees, merge tree metrics, topological data analysis, topology in visualization</td><td>VIS Full Paper</td><td>Comparisons</td><td></td><td><a href="https://youtu.be/zm16C45xjcQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9744472.html">link</a></td></tr><tr><td>129</td><td>Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison</td><td>Shunan Guo, Zhuochen Jin, Qing Chen, David Gotz, Hongyuan Zha, Nan Cao</td><td>Anomaly detection is a common analytical task that aims to identify rare cases that differ from the typical cases that make up the majority of a dataset. When analyzing event sequence data, the task of anomaly detection can be complex because the sequential and temporal nature of such data results in diverse definitions and flexible forms of anomalies. This, in turn, increases the difficulty in interpreting detected anomalies. In this paper, we propose a visual analytic approach for detecting anomalous sequences in an event sequence dataset via an unsupervised anomaly detection algorithm based on Variational AutoEncoders. We further compare the anomalous sequences with their reconstructions and with the normal sequences through a sequence matching algorithm to identify event anomalies. A visual analytics system is developed to support interactive exploration and interpretations of anomalies through novel visualization designs that facilitate the comparison between anomalous sequences and normal sequences. Finally, we quantitatively evaluate the performance of our anomaly detection algorithm, demonstrate the effectiveness of our system through case studies, and report feedback collected from study participants.</td><td>Anomaly detection, Data models, Data visualizations, Task analysis, Sequences, Heart, Diabetes</td><td>VIS Full Paper</td><td>Comparisons</td><td></td><td><a href="https://youtu.be/GZwW1w41Tro">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9468958.html">link</a></td></tr><tr><td>130</td><td>Comparative Analysis of Merge Trees using Local Tree Edit Distance</td><td>Raghavendra Sridharamurthy, Vijay Natarajan</td><td>Comparative analysis of scalar fields is an important problem with various applications including feature-directed visualization and feature tracking in time-varying data. Comparing topological structures that are abstract and succinct representations of the scalar fields lead to faster and meaningful comparison. While there are many distance or similarity measures to compare topological structures in a global context, there are no known measures for comparing topological structures locally. While the global measures have many applications, they do not directly lend themselves to fine-grained analysis across multiple scales. We define a local variant of the tree edit distance and apply it towards local comparative analysis of merge trees with support for finer analysis. We also present experimental results on time-varying scalar fields, 3D cryo-electron microscopy data, and other synthetic data sets to show the utility of this approach in applications like symmetry detection and feature tracking.</td><td>Merge tree, scalar field, local distance measure, persistence, edit distance, symmetry detection, feature tracking.</td><td>VIS Full Paper</td><td>Comparisons</td><td></td><td><a href="https://youtu.be/FV4ZI8S_AoI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9585392.html">link</a></td></tr><tr><td>131</td><td>Visual Exploration of Relationships and Structure in Low-Dimensional Embeddings</td><td>Klaus Eckelt, Andreas Hinterreiter, Patrick Adelberger, Conny Walchshofer, Vaishali Dhanoa, Christina Humer, Moritz Heckmann, Christian A. Steinparz, Marc Streit</td><td>In this work, we propose an interactive visual approach for the exploration and formation of structural relationships in embeddings of high-dimensional data. These structural relationships, such as item sequences, associations of items with groups, and hierarchies between groups of items, are defining properties of many real-world datasets. Nevertheless, most existing methods for the visual exploration of embeddings treat these structures as second-class citizens or do not take them into account at all. In our proposed analysis workflow, users explore enriched scatterplots of the embedding, in which relationships between items and\slash or groups are visually highlighted. The original high-dimensional data for single items, groups of items, or differences between connected items and groups are accessible through additional summary visualizations. We carefully tailored these summary and difference visualizations to the various data types and semantic contexts. During their exploratory analysis, users can externalize their insights by setting up additional groups and relationships between items and\slash or groups. We demonstrate the utility and potential impact of our approach by means of two use cases and multiple examples from various domains.</td><td>Dimensionality reduction, projection, visual analytics, layout enrichment, aggregation, comparison</td><td>VIS Full Paper</td><td>Comparisons</td><td></td><td><a href="https://youtu.be/vf-ogTRIvvY">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9729550.html">link</a></td></tr><tr><td>132</td><td>View Composition Algebra for Ad Hoc Comparison</td><td>Eugene Wu</td><td>Comparison is a core task in visual analysis.  Although there are numerous guidelines to help users design effective visualizations to aid known comparison tasks, there are few techniques available when users want to make ad hoc comparisons between marks, trends, or charts during data exploration and visual analysis.  For instance, to compare voting count maps from different years, two stock trends in a line chart, or a scatterplot of country GDPs with a textual summary of the average GDP.  Ideally, users can directly select the comparison targets and compare them, however what elements of a visualization should be candidate targets, which combinations of targets are safe to compare, and what comparison operations make sense?  This paper proposes a conceptual model that lets users compose combinations of values, marks, legend elements, and charts using a set of composition operators that summarize, compute differences, merge, and model their operands.  We further define a View Composition Algebra (VCA) that is compatible with datacube-based visualizations, derive an interaction design based on this algebra that supports ad hoc visual comparisons, and illustrate its utility through several use cases.</td><td>Visualization, Algebra, Comparison, Databases</td><td>VIS Full Paper</td><td>Comparisons</td><td></td><td><a href="https://youtu.be/TVKF__0hfpo">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9716867.html">link</a></td></tr><tr><td>133</td><td>Visual Comparison of Language Model Adaptation</td><td>Rita Sevastjanova, Eren Cakmak, Shauli Ravfogel, Ryan Cotterell, Mennatallah El-Assady</td><td>Neural language models are widely used; however, their model parameters often need to be adapted to the specific domains and tasks of an application, which is time- and resource-consuming. Thus, adapters have recently been introduced as a lightweight alternative for model adaptation. They consist of a small set of task-specific parameters with a reduced training time and simple parameter composition. The simplicity of adapter training and composition comes along with new challenges, such as maintaining an overview of adapter properties and effectively comparing their produced embedding spaces. To help developers overcome these challenges, we provide a twofold contribution. First, in close collaboration with NLP researchers, we conducted a requirement analysis for an approach supporting adapter evaluation and detected, among others, the need for both intrinsic (i.e., embedding similarity-based) and extrinsic (i.e., prediction-based) explanation methods. Second, motivated by the gathered requirements, we designed a flexible visual analytics workspace that enables the comparison of adapter properties. In this paper, we discuss several design iterations and alternatives for interactive, comparative visual explanation methods. Our comparative visualizations show the differences in the adapted embedding vectors and prediction outcomes for diverse human-interpretable concepts (e.g., person names, human qualities). We evaluate our workspace through case studies and show that, for instance, an adapter trained on the language debiasing task according to context-0 (decontextualized) embeddings introduces a new type of bias where words (even gender-independent words such as countries) become more similar to female- than male pronouns. We demonstrate that these are artifacts of context-0 embeddings, and the adapter effectively eliminates the gender information from the contextualized word representations.</td><td></td><td>VIS Full Paper</td><td>Comparisons</td><td></td><td><a href="https://youtu.be/iYDyaWRhKmU">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1607.html">link</a></td></tr><tr><td>134</td><td>Temporal Merge Tree Maps: A Topology-Based Static Visualization for Temporal Scalar Data</td><td>Wiebke Köpp, Tino Weinkauf</td><td>Creating a static visualization for a time-dependent scalar field is a non-trivial task, yet very insightful as it shows the dynamics in one picture. Existing approaches are based on a linearization of the domain or on feature tracking. Domain linearizations use space-filling curves to place all sample points into a 1D domain, thereby breaking up individual features. Feature tracking methods explicitly respect feature continuity in space and time, but generally neglect the data context in which those features live. We present a feature-based linearization of the spatial domain that keeps features together and preserves their context by involving all data samples. We use augmented merge trees to linearize the domain and show that our linearized function has the same merge tree as the original data. A greedy optimization scheme aligns the trees over time providing temporal continuity. This leads to a static 2D visualization with one temporal dimension, and all spatial dimensions compressed into one. We compare our method against other domain linearizations as well as feature-tracking approaches, and apply it to several real-world data sets.</td><td></td><td>VIS Full Paper</td><td>Topology</td><td></td><td><a href="https://youtu.be/B8q5o0s_TaY">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1051.html">link</a></td></tr><tr><td>135</td><td>TopoCluster: A Localized Data Structure for Topology-based Visualization</td><td>Guoxi Liu, Federico Iuricich, Riccardo Fellegara, Leila De Floriani</td><td>Unstructured data are collections of points with irregular topology, often represented through simplicial meshes, such as triangle and tetrahedral meshes. Whenever possible such representations are avoided in visualization since they are computationally demanding if compared with regular grids. In this work, we aim at simplifying the encoding and processing of simplicial meshes. The paper proposes TopoCluster, a new localized data structure for tetrahedral meshes. TopoCluster provides efficient computation of the connectivity of the mesh elements with a low memory footprint. The key idea of TopoCluster is to subdivide the simplicial mesh into clusters. Then, the connectivity information is computed locally for each cluster and discarded when it is no longer needed. We define two instances of TopoCluster. The first instance prioritizes time efficiency and provides only modest savings in memory, while the second instance drastically reduces memory consumption up to an order of magnitude with respect to comparable data structures. Thanks to the simple interface provided by TopoCluster, we have been able to integrate both data structures into the existing Topological Toolkit (TTK) framework. As a result, users can run any plugin of TTK using TopoCluster without changing a single line of code.</td><td>Data visualization, data structures, topological data analysis, simplicial meshes, tetrahedral meshes</td><td>VIS Full Paper</td><td>Topology</td><td></td><td><a href="https://youtu.be/irZJCvultIQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9583888.html">link</a></td></tr><tr><td>136</td><td>Computing a Stable Distance on Merge Trees</td><td>Brian C Bollen, Joshua A Levine, Pasindu P. Tennakoon</td><td>Distances on merge trees facilitate visual comparison of collections of scalar fields. Two desirable properties for these distances to exhibit are 1) the ability to discern between scalar fields which other, less complex topological summaries cannot and 2) to still be robust to perturbations in the dataset. The combination of these two properties, known respectively as stability and discriminativity, has led to theoretical distances which are either thought to be or shown to be computationally complex and thus their implementations have been scarce. In order to design similarity measures on merge trees which are computationally feasible for more complex merge trees, many researchers have elected to loosen the restrictions on at least one of these two properties. The question still remains, however, if there are practical situations where trading these desirable properties is necessary. Here we construct a distance between merge trees which is designed to retain both discriminativity and stability. While our approach can be expensive for large merge trees, we illustrate its use in a setting where the number of nodes is small. This setting can be made more practical since we also provide a proof that persistence simplification increases the outputted distance by at most half of the simplified value. We demonstrate our distance measure on applications in shape comparison and on detection of periodicity in the von Kármán vortex street.</td><td></td><td>VIS Full Paper</td><td>Topology</td><td></td><td><a href="https://youtu.be/pRQ1Q_yCHNc">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1233.html">link</a></td></tr><tr><td>137</td><td>Topological Simplifications of Hypergraphs</td><td>Youjia Zhou, Archit Rathore, Emilie Purvine, Bei Wang</td><td>We study hypergraph visualization via its topological simplification. We explore both vertex simplification and hyperedge simplification of hypergraphs using tools from topological data analysis. In particular, we transform a hypergraph into its graph representations known as the line graph and clique expansion. A topological simplification of such a graph representation induces a simplification of the hypergraph. In simplifying a hypergraph, we allow vertices to be combined if they belong to almost the same set of hyperedges, and hyperedges to be merged if they share almost the same set of vertices. Our proposed approaches are general, mathematically justifiable, and put vertex simplification and hyperedge simplification in a unifying framework.</td><td>Hypergraph simplification, hypergraph visualization, graph simplification, topological data analysis</td><td>VIS Full Paper</td><td>Topology</td><td></td><td><a href="https://youtu.be/4fNV3RcrunU">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9721603.html">link</a></td></tr><tr><td>138</td><td>Persistence cycles for visual exploration of persistent homology</td><td>Federico Iuricich</td><td>Persistent homology is a fundamental tool in topological data analysis used for the most diverse applications. Information captured by persistent homology is commonly visualized using scatter plots representations. Despite being widely adopted, such a visualization technique limits user understanding and is prone to misinterpretation. This paper proposes a new approach for the efficient computation of persistence cycles, a geometric representation of the features captured by persistent homology. We illustrate the importance of rendering persistence cycles when analyzing scalar fields, and we discuss the advantages that our approach provides compared to other techniques in topology-based visualization. We provide an efficient implementation of our approach based on discrete Morse theory, as a new module for the Topology Toolkit. We show that our implementation has comparable performance with respect to state-of-the-art toolboxes while providing a better framework for visually analyzing persistent homology information.</td><td>Persistent homology, Topological Data Analysis, Scalar fields</td><td>VIS Full Paper</td><td>Topology</td><td></td><td><a href="https://youtu.be/t-pFTYFdzB0">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9531544.html">link</a></td></tr><tr><td>139</td><td>Geometry-Aware Planar Embedding of Treelike Structures</td><td>Ping Hu, Saeed Boorboor, Joseph Marino, Arie E. Kaufman</td><td>The growing complexity of spatial and structural information in 3D data makes data inspection and visualization a challenging task. We describe a method to create a planar embedding of 3D treelike structures using their skeleton representations. Our method maintains the original geometry, without overlaps, to the best extent possible, allowing exploration of the topology within a single view. We present a novel camera view generation method which maximizes the visible geometric attributes (segment shape and relative placement between segments). Camera views are created for individual segments and are used to determine local bending angles at each node by projecting them to 2D. The final embedding is generated by minimizing an energy function (the weights of which are user adjustable) based on branch length and the 2D angles, while avoiding intersections. The user can also interactively modify segment placement within the 2D embedding, and the overall embedding will update accordingly. A global to local interactive exploration is provided using hierarchical camera views that are created for subtrees within the structure. We evaluate our method both qualitatively and quantitatively and demonstrate our results by constructing planar visualizations of line data (traced neurons) and volume data (CT vascular and bronchial data).</td><td>I.3.5 Computational Geometry and Object Modeling &lt; I.3 Computer Graphics &lt; I Computing Methodologies, I.6 Simulation, Modeling, and Visualization &lt; I Computing Methodologies</td><td>VIS Full Paper</td><td>Topology</td><td></td><td><a href="https://youtu.be/ZZAb1IJ9ycE">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9721643.html">link</a></td></tr><tr><td>140</td><td>MosaicSets: Embedding Set Systems into Grid Graphs</td><td>Peter Rottmann, Markus Wallinger, Annika Bonerath, Sven Gedicke, Martin Nöllenburg, Jan-Henrik Haunert</td><td>Visualizing sets of elements and their relations is an important research area in information visualization. In this paper, we present MosaicSets: a novel approach to create Euler-like diagrams from non-spatial set systems such that each element occupies one cell of a regular hexagonal or square grid. The main challenge is to find an assignment of the elements to the grid cells such that each set constitutes a contiguous region. As use case, we consider the research groups of a university faculty as elements, and the departments and joint research projects as sets. We aim at finding a suitable mapping between the research groups and the grid cells such that the department structure forms a base map layout. Our objectives are to optimize both the compactness of the entirety of all cells and of each set by itself. We show that computing the mapping is NP-hard. However, using integer linear programming we can solve real-world instances optimally within a few seconds. Moreover, we propose a relaxation of the contiguity requirement to visualize otherwise non-embeddable set systems. 
 We present and discuss different rendering styles for the set overlays. Based on a case study with real-world data, our evaluation comprises quantitative measures as well as expert interviews.</td><td></td><td>VIS Full Paper</td><td>Graphs and Networks</td><td></td><td><a href="https://youtu.be/hB8RRJeHuCA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1551.html">link</a></td></tr><tr><td>141</td><td>Taurus: Towards A Unified Force Representation and Universal Solver for Graph Layout</td><td>Mingliang Xue, Zhi Wang, Fahai Zhong, Yong Wang, Mingliang Xu, Oliver Deussen, Yunhai Wang</td><td>Over the past few decades, a large number of graph layout techniques have been proposed for visualizing graphs from various domains. In this paper, we present a general framework, Taurus, for unifying popular techniques such as the spring-electrical model, stress model, and maxent-stress model. It is based on a unified force representation, which formulates most existing techniques as a combination of quotient-based forces that combine power functions of graph-theoretical and Euclidean distances. This representation enables us to compare the strengths and weaknesses of existing techniques, while facilitating the development of new methods. Based on this, we propose a new balanced stress model (BSM) that is able to layout graphs in superior quality. In addition, we introduce a universal augmented stochastic gradient descent (SGD) optimizer that efficiently finds proper solutions for all layout techniques. To demonstrate the power of our framework, we conduct a comprehensive evaluation of existing techniques on a large number of synthetic and real graphs. We release an open-source package, which facilitates easy comparison of different graph layout methods for any graph input as well as effectively creating customized graph layout techniques.</td><td></td><td>VIS Full Paper</td><td>Graphs and Networks</td><td></td><td><a href="https://youtu.be/w1yKkdkoVi8">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1165.html">link</a></td></tr><tr><td>142</td><td>Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to Its Embedding</td><td>Zipeng Liu, Yang Wang, Jürgen Bernard, Tamara Munzner</td><td>Graph neural networks (GNNs) are a class of powerful machine learning tools that model node relations for making predictions of nodes or links. GNN developers rely on quantitative metrics of the predictions to evaluate a GNN, but similar to many other neural networks, it is difficult for them to understand if the GNN truly learns characteristics of a graph as expected. We propose an approach to corresponding an input graph to its node embedding (aka latent space), a common component of GNNs that is later used for prediction. We abstract the data and tasks, and develop an interactive multi-view interface called CorGIE to instantiate the abstraction. As the key function in CorGIE, we propose the K-hop graph layout to show topological neighbors in hops and their clustering structure. To evaluate the functionality and usability of CorGIE, we present how to use CorGIE in two usage scenarios, and conduct a case study with five GNN experts.</td><td>Visualization for machine learning, graph neural network, graph layout</td><td>VIS Full Paper</td><td>Graphs and Networks</td><td></td><td><a href="https://youtu.be/smJUJPxcfQk">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9705082.html">link</a></td></tr><tr><td>143</td><td>Comparative Evaluation of Bipartite, Node-Link, and Matrix-Based Network Representations</td><td>Moataz Abdelaal, Nathan D Schiele, Katrin Angerbauer, Kuno Kurzhals, Michael Sedlmair, Daniel Weiskopf</td><td>This work investigates and compares the performance of node-link diagrams, adjacency matrices, and bipartite layouts for visualizing networks. In a crowd-sourced user study (n = 150), we measure the task accuracy and completion time of the three representations for different network classes and properties. In contrast to the literature, which covers mostly topology-based tasks (e.g., path finding) in small datasets, we mainly focus on overview tasks for large and directed networks. We consider three overview tasks on networks with 500 nodes: (T1) network class identification, (T2) cluster detection, and (T3) network density estimation, and two detailed tasks: (T4) node in-degree vs. out-degree and (T5) representation mapping, on networks with 50 and 20 nodes, respectively. Our results show that bipartite layouts are beneficial for revealing the overall network structure, while adjacency matrices are most reliable across the different tasks.</td><td></td><td>VIS Full Paper</td><td>Graphs and Networks</td><td></td><td><a href="https://youtu.be/oNKd4xQCG64">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1541.html">link</a></td></tr><tr><td>144</td><td>Understanding Barriers to Network Exploration with Visualization: A Report from the Trenches</td><td>Mashael AlKadi, Vanessa Serrano, James Scott-Brown, Uta Hinrichs, Catherine Plaisant, Jean-Daniel Fekete, Benjamin Bach</td><td>This article reports on an in-depth study that investigates barriers to network exploration with visualizations. Network visualization tools are becoming increasingly popular, but little is known about how analysts plan and engage in the visual exploration of network data—which exploration strategies they employ, and how they prepare their data, define questions, and decide on visual mappings. Our study involved a series of workshops, interaction logging, and observations from a 6-week network exploration course. Our findings shed light on the stages that define analysts’ approaches to network visualization and barriers experienced by some analysts during their network visualization processes. These barriers mainly appear before using a specific tool and include defining exploration goals, identifying relevant network structures and abstractions, or creating appropriate visual mappings for their network data. Our findings inform future work in visualization education and analyst-centered network visualization tool design.</td><td></td><td>VIS Full Paper</td><td>Graphs and Networks</td><td></td><td><a href="https://youtu.be/m5OSeJOOOzo">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1154.html">link</a></td></tr><tr><td>145</td><td>VividGraph: Learning to Extract and Redesign Network Graphs from Visualization Images</td><td>Sicheng Song, Chenhui Li, Yujing Sun, Changbo Wang</td><td>Network graphs are common visualization charts. They often appear in the form of bitmaps in papers, web pages, magazine prints, and designer sketches. People often want to modify network graphs because of their poor design, but it is difficult to obtain their underlying data. In this paper, we present VividGraph, a pipeline for automatically extracting and redesigning network graphs from static images. We propose using convolutional neural networks to solve the problem of network graph data extraction. Our method is robust to hand-drawn graphs, blurred graph images, and large graph images. We also present a network graph classification module to make it effective for directed graphs. We propose two evaluation methods to demonstrate the effectiveness of our approach. It can be used to quickly transform designer sketches, extract underlying data from existing network graphs, and interactively redesign poorly designed network graphs.</td><td>Information visualization , Network graph , Data extraction , Chart recognition , Semantic segmentation , Redesign</td><td>VIS Full Paper</td><td>Graphs and Networks</td><td></td><td><a href="https://youtu.be/7gEvJi628Zs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9720180.html">link</a></td></tr><tr><td>146</td><td>Towards Systematic Design Considerations for Visualizing Cross-View Data Relationships</td><td>Maoyuan Sun, Akhil Namburi, David Koop, Jian Zhao, Tianyi Li, Haeyong Chung</td><td>Due to the scale of data and the complexity of analysis tasks, insight discovery often requires coordinating multiple visualizations (views), with each view displaying different parts of data or the same data from different perspectives. For example, to analyze car sales records, a marketing analyst uses a line chart to visualize the trend of car sales, a scatterplot to inspect the price and horsepower of different cars, and a matrix to compare the transaction amounts in types of deals. To explore related information across multiple views, current visual analysis tools heavily rely on brushing and linking techniques, which may require a significant amount of user effort (e.g., many trial-and-error attempts). There may be other efficient and effective ways of displaying cross-view data relationships to support data analysis with multiple views, but currently there are no guidelines to address this design challenge. In this paper, we present systematic design considerations for visualizing cross-view data relationships, which leverages descriptive aspects of relationships and usable visual context of multi-view visualizations. We discuss pros and cons of different designs for showing cross-view data relationships, and provide a set of recommendations for helping practitioners make design decisions.</td><td>Visualization, Data visualization, Task analysis, Organizations, Systematics, Periodic structures, Computer science</td><td>VIS Full Paper</td><td>Visualization Design</td><td></td><td><a href="https://youtu.be/NSk2io4FpeA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9508898.html">link</a></td></tr><tr><td>147</td><td>Designing with Pictographs: Envision Topics without Sacrificing Understanding</td><td>Alyxander Burns, Cindy Xiong, Steven Franconeri, Alberto Cairo, Narges Mahyar</td><td>Past studies have shown that when a visualization uses pictographs to encode data, they have a positive effect on memory, engagement, and assessment of risk. However, little is known about how pictographs affect one&#x27;s ability to understand a visualization, beyond memory for values and trends. We conducted two crowdsourced experiments to compare the effectiveness of using pictographs when showing part-to-whole relationships. In Experiment 1, we compared pictograph arrays to more traditional bar and pie charts. We tested participants&#x27; ability to generate high-level insights following Bloom&#x27;s taxonomy of educational objectives via 6 free-response questions. We found that accuracy for extracting information and generating insights did not differ overall between the two versions. To explore the motivating differences between the designs, we conducted a second experiment where participants compared charts containing pictograph arrays to more traditional charts on 5 metrics and explained their reasoning. We found that some participants preferred the way that pictographs allowed them to envision the topic more easily, while others preferred traditional bar and pie charts because they seem less cluttered and faster to read. These results suggest that, at least in simple visualizations depicting part-to-whole relationships, the choice of using pictographs has little influence on sensemaking and insight extraction. When deciding whether to use pictograph arrays, designers should consider visual appeal, perceived comprehension time, ease of envisioning the topic, and clutteredness.</td><td>Infographics, pictographs, design, graph comprehension, understanding, casual sensemaking</td><td>VIS Full Paper</td><td>Visualization Design</td><td></td><td><a href="https://youtu.be/icVqNvcD74I">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9465643.html">link</a></td></tr><tr><td>148</td><td>Visualizing Higher-Order 3D Tensors by Multipole Lines</td><td>Chiara Hergl, Thomas Nagel, Gerik Scheuermann</td><td>Physics, medicine, earth sciences, mechanical engineering, geo-engineering, bio-engineering and many more application areas use tensorial data. For example, tensors are used in formulating the balance equations of charge, mass, momentum, or energy as well as the constitutive relations that complement them. Some of these tensors (i.e. stiffness tensor, strain gradient, photo-elastic tensor) are of order higher than two. Currently, there are nearly no visualization techniques for such data beyond glyphs. An important reason for this is the limit of currently used tensor decomposition techniques. In this article, we propose to use the deviatoric decomposition to draw lines describing tensors of arbitrary order in three dimensions. The deviatoric decomposition splits a three-dimensional tensor of any order with any type of index symmetry into totally symmetric, traceless tensors. These tensors, called deviators, can be described by a unique set of directions (called multipoles by J.~C. Maxwell) and scalars. These multipoles allow the definition of multipole lines which can be computed in a similar fashion to tensor lines and allow a line-based visualization of three-dimensional tensors of any order. We give examples for the visualization of symmetric, second-order tensor fields as well as fourth-order tensor fields. To allow an interpretation of the multipole lines, we analyze the connection between the multipoles and the eigenvectors/eigenvalues in the second-order case. For the fourth-order stiffness tensor, we prove relations between multipoles and important physical quantities such as shear moduli as well as the eigenvectors of the second-order right Cauchy-Green tensor.</td><td>tensor algebra, higher-order tensor, line-based, deviatoric decomposition, anisotropy</td><td>VIS Full Paper</td><td>Visualization Design</td><td></td><td><a href="https://youtu.be/Z0Y7BeNyM0E">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9737134.html">link</a></td></tr><tr><td>149</td><td>MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization</td><td>Lu Ying, Yuchen Yang, Xinhuan Shu, Dazhen Deng, Tan Tang, Lingyun Yu, Yingcai Wu</td><td>Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.</td><td></td><td>VIS Full Paper</td><td>Visualization Design</td><td></td><td><a href="https://youtu.be/3Xq2cN06nos">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1335.html">link</a></td></tr><tr><td>150</td><td>Dashboard Design Patterns</td><td>Benjamin Bach, Euan Freeman, Alfie Abdul-Rahman, Cagatay Turkay, Saiful Khan, Yulei Fan, Min Chen</td><td>This paper introduces design patterns for dashboards to inform dashboard design processes. Despite a growing number of public examples, case studies, and general guidelines there is surprisingly little design guidance for dashboards. Such guidance is necessary to inspire designs and discuss tradeoffs in, e.g., screenspace, interaction, or information shown. Based on a systematic review of 144 dashboards, we report on eight groups of design patterns that provide common solutions in dashboard design. We discuss combinations of these patterns in “dashboard genres” such as narrative, analytical, or embedded dashboard. We ran a 2-week dashboard design workshop with 23 participants of varying expertise working on their own data and dashboards. We discuss the application of patterns for the dashboard design processes, as well as general design tradeoffs and common challenges. Our work complements previous surveys and aims to support dashboard designers and researchers in co-creation, structured design decisions, as well as future user evaluations about dashboard design guidelines. Detailed pattern descriptions and workshop material can be found online: https://dashboarddesignpatterns.github.io.</td><td></td><td>VIS Full Paper</td><td>Visualization Design</td><td></td><td><a href="https://youtu.be/P70kXAco3Qo">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1554.html">link</a></td></tr><tr><td>151</td><td>A Framework for Multiclass Contour Visualization</td><td>Sihang Li, Jiacheng Yu, Mingxuan Li, Le Liu, Xiaolong (Luke) Zhang, Xiaoru Yuan</td><td>Multiclass contour visualization is often used to interpret complex data attributes in such fields as weather forecasting, computational fluid dynamics, and artificial intelligence. However, effective and accurate representations of underlying data patterns and correlations can be challenging in multiclass contour visualization, primarily due to the inevitable visual cluttering and occlusions when the number of classes is significant. To address this issue, visualization design must carefully choose design parameters to make visualization more comprehensible. With this goal in mind, we proposed a framework for multiclass contour visualization. The framework has two components: a set of four visualization design parameters, which are developed based on an extensive review of literature on contour visualization, and a declarative domain-specific language (DSL) for creating multiclass contour rendering, which enables a fast exploration of those design parameters. A task-oriented user study was conducted to assess how those design parameters affect users&#x27; interpretations of real-world data. The study results offered some suggestions on the value choices of design parameters in multiclass contour visualization.</td><td></td><td>VIS Full Paper</td><td>Visualization Design</td><td></td><td><a href="https://youtu.be/ZTwky8yd_yY">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1206.html">link</a></td></tr><tr><td>152</td><td>Exploring Interactions with Printed Data Visualizations in Augmented Reality</td><td>Wai Tong, Zhutian Chen, Meng Xia, Leo Yu-Ho Lo, Linping Yuan, Benjamin Bach, Huamin Qu</td><td>This paper presents a design space of interaction techniques to engage with visualizations that are printed on paper and augmented through Augmented Reality. Paper sheets are widely used to deploy visualizations and provide a rich set of tangible affordances for interactions, such as touch, folding, tilting, or stacking. At the same time, augmented reality can dynamically update visualization content to provide commands such as pan, zoom, filter, or detail on demand. This paper is the first to provide a structured approach to mapping possible actions with the paper to interaction commands. This design space and the findings of a controlled user study have implications for future designs of augmented reality systems involving paper sheets and visualizations. Through workshops (N=20) and ideation, we identified 81 interactions that we classify in three dimensions: 1) commands that can be supported by an interaction, 2) the specific parameters provided by an (inter)action with paper, and 3) the number of paper sheets involved in an interaction. We tested user preference and viability of 11 of these interactions with a prototype implementation in a controlled study (N=12, HoloLens 2) and found that most of the interactions are intuitive and engaging to use. We summarized interactions (e.g., tilt to pan) that have strong affordance to complement “point” for data exploration, physical limitations and properties of paper as a medium, cases requiring redundancy and shortcuts, and other implications for design.</td><td></td><td>VIS Full Paper</td><td>Immersive Analytics and Situated Visualization</td><td></td><td><a href="https://youtu.be/edZMvbOatfA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1101.html">link</a></td></tr><tr><td>153</td><td>RagRug: A Toolkit for Situated Analytics</td><td>Philipp Fleck, Aimee Sousa Calepso, Sebastian Hubenschmid,Michael Sedlmair, Dieter Schmalstieg</td><td>We present RagRug, an open-source toolkit for situated analytics. The  , abilities of RagRug go beyond previous immersiveanalytics toolkits by  , focusing on specific requirements emerging when using augmented  , reality (AR) rather than virtual reality. RagRugcombines state of the  , art visual encoding capabilities with a comprehensive physical-virtual  , model, which lets application developerssystematically describe the  , physical objects in the real world and their role in AR. We connect AR  , visualizations with data streams fromthe Internet of Things using  , distributed dataflow. To this end, we use reactive programming  , patterns so that visualizations becomecontext-aware, i.e., they adapt  , to events coming in from the environment. The resulting authoring  , system is low-code; it emphasisesdescribing the physical and the  , virtual world and the dataflow between the elements contained therein.  , We describe the technicaldesign and implementation of RagRug, and  , report on five example applications illustrating the toolkit’s  , abilities.</td><td>Augmented Reality, Visualization, Visual Analytics, Immersive Analytics, Situated Analytics</td><td>VIS Full Paper</td><td>Immersive Analytics and Situated Visualization</td><td></td><td><a href="https://youtu.be/4xh-4LGWkCE">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9729627.html">link</a></td></tr><tr><td>154</td><td>PuzzleFixer: A Visual Reassembly System for Immersive Fragments Restoration</td><td>Shuainan Ye, Zhutian Chen, Xiangtong Chu, Kang Li, Juntong Luo, Yi Li, Guohua Geng, Yingcai Wu</td><td>We present PuzzleFixer, an immersive interactive system for experts to rectify defective reassembled 3D objects. Reassembling the fragments of a broken object to restore its original state is the prerequisite of many analytical tasks such as cultural relics analysis and forensics reasoning. While existing computer-aided methods can automatically reassemble fragments, they often derive incorrect objects due to the complex and ambiguous fragment shapes. Thus, experts usually need to refine the object manually. Prior advances in immersive technologies provide benefits for realistic perception and direct interactions to visualize and interact with 3D fragments. However, few studies have investigated the reassembled object refinement. The specific challenges include: 1) the fragment combination set is too large to determine the correct matches, and 2) the geometry of the fragments is too complex to align them properly. To tackle the first challenge, PuzzleFixer leverages dimensionality reduction and clustering techniques, allowing users to review possible match categories, select the matches with reasonable shapes, and drill down to shapes to correct the corresponding faces. For the second challenge, PuzzleFixer embeds the object with node-link networks to augment the perception of match relations. Specifically, it instantly visualizes matches with graph edges and provides force feedback to facilitate the efficiency of alignment interactions. To demonstrate the effectiveness of PuzzleFixer, we conducted an expert evaluation based on two cases on real-world artifacts and collected feedback through post-study interviews. The results suggest that our system is suitable and efficient for experts to refine incorrect reassembled objects.</td><td></td><td>VIS Full Paper</td><td>Immersive Analytics and Situated Visualization</td><td></td><td><a href="https://youtu.be/q1ZT8jv7EUE">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1272.html">link</a></td></tr><tr><td>155</td><td>Effects of View Layout On Situated Analytics for Multiple-View Representations in Immersive Visualization</td><td>Zhen Wen, Wei Zeng, Luoxuan Weng, Yihan Liu, Mingliang Xu, Wei Chen</td><td>Multiple-view (MV) representations enabling multi-perspective exploration of large and complex data are often employed on 2D displays. The technique also shows great potential in addressing complex analytic tasks in immersive visualization. However, although useful, the design space of MV representations in immersive visualization lacks in deep exploration. In this paper, we propose a new perspective to this line of research, by examining the effects of view layout for MV representations on situated analytics. Specifically, we disentangle situated analytics in perspectives of situatedness regarding spatial relationship between visual representations and physical referents, and analytics regarding cross-view data analysis including filtering, refocusing, and connecting tasks. Through an in-depth analysis of existing layout paradigms, we summarize design trade-offs for achieving high situatedness and effective analytics simultaneously. We then distill a list of design requirements for a desired layout that balances situatedness and analytics, and develop a prototype system with an automatic layout adaptation method to fulfill the requirements. The method mainly includes a cylindrical paradigm for egocentric reference frame, and a force-directed method for proper view-view, view-user, and view-referent proximities and high view visibility. We conducted a formal user study that compares layouts by our method with linked and embedded layouts. Quantitative results show that participants finished filtering- and connecting-centered tasks significantly faster with our layouts, and user feedback confirms high usability of the prototype system.</td><td></td><td>VIS Full Paper</td><td>Immersive Analytics and Situated Visualization</td><td></td><td><a href="https://youtu.be/dUdu9GOSxKs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1382.html">link</a></td></tr><tr><td>156</td><td>RoboHapalytics: A Robot Assisted Haptic Controller for Immersive Analytics</td><td>Shaozhang Dai, Tim Dwyer, Barrett Ens, Jim Smiley, Lonni Besançon</td><td>Immersive environments offer new possibilities for exploring three-dimensional volumetric or abstract data. However, typical mid-air interaction offers little guidance to the user in interacting with the resulting visuals. Previous work has explored the use of haptic controls to give users tangible affordances for interacting with the data, but these controls have either: been limited in their range and resolution; were spatially fixed; or required users to manually align them with the data space. We explore the use of a robot arm with hand tracking to align tangible controls under the user’s fingers as they reach out to interact with data affordances. We begin with a study evaluating the effectiveness of a robot-extended slider control compared to a large fixed physical slider and a purely virtual mid-air slider. We find that the robot slider has similar accuracy to the physical slider but is significantly more accurate than mid-air interaction. Further, the robot slider can be arbitrarily reoriented, opening up many new possibilities for tangible haptic interaction with immersive visualisations. We demonstrate these possibilities through three use-cases: selection in a time-series chart; interactive slicing of CT scans; and finally exploration of a scatter plot depicting time-varying socio-economic data.</td><td></td><td>VIS Full Paper</td><td>Immersive Analytics and Situated Visualization</td><td></td><td><a href="https://youtu.be/XBApjWgHDJo">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1523.html">link</a></td></tr><tr><td>157</td><td>Labeling Out-of-View Objects in Immersive Analytics to Support Situated Visual Searching</td><td>Tica Lin, Yalong Yang, Johanna Beyer, Hanspeter Pfister</td><td>Augmented Reality (AR) embeds digital information into objects of the physical world. Data can be shown in-situ, thereby enabling real-time visual comparisons and object search in real-life user tasks, such as comparing products and looking up scores in a sports game. While there have been studies on designing AR interfaces for situated information retrieval, there has only been limited research on AR object labeling for visual search tasks in the spatial environment. In this paper, we identify and categorize different design aspects in AR label design and report on a formal user study on labels for out-of-view objects to support visual search tasks in AR. We design three visualization techniques for out-of-view object labeling in AR, which respectively encode the relative physical position (height-encoded), the rotational direction (angle-encoded), and the label values (value-encoded) of the objects. We further implement two traditional in-view object labeling techniques, where labels are placed either next to the respective objects (situated) or at the edge of the AR FoV (boundary). We evaluate these five different label conditions in three visual search tasks for static objects. Our study shows that out-of-view object labels are beneficial when searching for objects outside the FoV, spatial orientation, and when comparing multiple spatially sparse objects. Angle-encoded labels with directional cues of the surrounding objects have the overall best performance with the highest user satisfaction. We discuss the implications of our findings for future immersive AR interface design.</td><td>Object Labeling, Mixed / Augmented Reality, Immersive Analytics, Situated Analytics, Data Visualization</td><td>VIS Full Paper</td><td>Immersive Analytics and Situated Visualization</td><td></td><td><a href="https://youtu.be/eMIjoMIpXf0">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9645242.html">link</a></td></tr><tr><td>158</td><td>VACSEN: A Visualization Approach for Noise Awareness in Quantum Computing</td><td>Shaolun Ruan, Yong Wang, Weiwen Jiang, Ying Mao, Qiang Guan</td><td>Quantum computing has attracted considerable public attention due to its exponential speedup over classical computing. Despite its advantages, today’s quantum computers intrinsically suffer from noise and are error-prone. To guarantee the high fidelity of the execution result of a quantum algorithm, it is crucial to inform users of the noises of the used quantum computer and the compiled physical circuits. However, an intuitive and systematic way to make users aware of the quantum computing noise is still missing. In this paper, we fill the gap by proposing a novel visualization approach to achieve noise-aware quantum computing. It provides a holistic picture of the noise of quantum computing through multiple interactively coordinated views: a Computer Evolution View with a circuit-like design overviews the temporal evolution of the noises of different quantum computers, a Circuit Filtering View facilitates quick filtering of multiple compiled physical circuits for the same quantum algorithm, and a Circuit Comparison View with a coupled bar chart enables detailed comparison of the filtered compiled circuits. We extensively evaluate the performance of VACSEN through two case studies on quantum algorithms of different scales and in-depth interviews with 12 quantum computing users. The results demonstrate the effectiveness and usability of VACSEN in achieving noise-aware quantum computing.</td><td></td><td>VIS Full Paper</td><td>Questioning Data and Data Bias</td><td></td><td><a href="https://youtu.be/QWEnGpJAThs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1351.html">link</a></td></tr><tr><td>159</td><td>Do you believe your (social media) data? A personal story on location data biases, errors, and plausibility as well as their visualization</td><td>Tobias Isenberg, Zujany Salazar, Rafael Blanco, Catherine Plaisant</td><td>We present a case study on a journey about a personal data collection of carnivorous plant species habitats, and the resulting scientific exploration of location data biases, data errors, location hiding, and data plausibility. While initially driven by personal interest, our work led to the analysis and development of various means for visualizing threats to insight from geo-tagged social media data. In the course of this endeavor we analyzed local and global geographic distributions and their inaccuracies. We also contribute Motion Plausibility Profiles---a new means for visualizing how believable a specific contributor’s location data is or if it was likely manipulated. We then compared our own repurposed social media dataset with data from a dedicated citizen science project. Compared to biases and errors in the literature on traditional citizen science data, with our visualizations we could also identify some new types or show new aspects for known ones. Moreover, we demonstrate several types of errors and biases for repurposed social media data.</td><td>Social media data; Flickr; Panoramio; iNaturalist; data bias; data error; data plausibility; data obfuscation; citizen science</td><td>VIS Full Paper</td><td>Questioning Data and Data Bias</td><td></td><td><a href="https://youtu.be/TWLFSRxhFfw">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9676662.html">link</a></td></tr><tr><td>160</td><td>D-BIAS: A Causality-Based Human-in-the-Loop System for Tackling Algorithmic Bias</td><td>Bhavya Ghai, Klaus Mueller</td><td>With the rise of AI, algorithms have become better at learning underlying patterns from the training data including ingrained social biases based on gender, race, etc. Deployment of such algorithms to domains such as hiring, healthcare, law enforcement, etc. has raised serious concerns about fairness, accountability, trust and interpretability in machine learning algorithms. To alleviate this problem, we propose D-BIAS, a visual interactive tool that embodies human-in-the-loop AI approach for auditing and mitigating social biases from tabular datasets. It uses a graphical causal model to represent causal relationships among different features in the dataset and as a medium to inject domain knowledge. A user can detect the presence of bias against a group, say females, or a subgroup, say black females, by identifying unfair causal relationships in the causal network and using an array of fairness metrics. Thereafter, the user can mitigate bias by refining the causal model and acting on the unfair causal edges. For each interaction, say weakening/deleting a biased causal edge, the system uses a novel method to simulate a new (debiased) dataset based on the current causal model while ensuring a minimal change from the original dataset. Users can visually assess the impact of their interactions on different fairness metrics, utility metrics, data distortion, and the underlying data distribution. Once satisfied, they can download the debiased dataset and use it for any downstream application for fairer predictions. We evaluate D-BIAS by conducting experiments on 3 datasets and also a formal user study. We found that D-BIAS helps reduce bias significantly compared to the baseline debiasing approach across different fairness metrics while incurring little data distortion and a small loss in utility. Moreover, our human-in-the-loop based approach significantly outperforms an automated approach on trust, interpretability and accountability.</td><td></td><td>VIS Full Paper</td><td>Questioning Data and Data Bias</td><td></td><td><a href="https://youtu.be/iN5kabYi_g4">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1050.html">link</a></td></tr><tr><td>161</td><td>A Unified Comparison of User Modeling Techniques for Predicting Data Interaction and Detecting Exploration Bias</td><td>Sunwoo Ha, Shayan Monadjemi, Alvitta Ottley, Roman Garnett</td><td>The visual analytics community has proposed several user modeling algorithms to capture and analyze users&#x27; interaction behavior in order to assist users in data exploration and insight generation. For example, some can detect exploration biases while others can predict data points that the user will interact with before that interaction occurs. Researchers believe this collection of algorithms can help create more intelligent visual analytics tools. However, the community lacks a rigorous evaluation and comparison of these existing techniques. As a result, there is limited guidance on which method to use and when. Our paper seeks to fill in this missing gap by comparing and ranking eight user modeling algorithms based on their performance on a diverse set of four user study datasets. We analyze exploration bias detection, data interaction prediction, and algorithmic complexity, among other measures. Based on our findings, we highlight open challenges and new directions for analyzing user interactions and visualization provenance.</td><td></td><td>VIS Full Paper</td><td>Questioning Data and Data Bias</td><td></td><td><a href="https://youtu.be/3c3wp4MtdFQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1618.html">link</a></td></tr><tr><td>162</td><td>Seeing What You Believe or Believing What You See? Belief Biases Correlation Estimation</td><td>Cindy Xiong, Chase Stokes, Yea-Seul Kim, Steven Franconeri</td><td>When an analyst or scientist has a belief about how the world works, their thinking can be biased in favor of that belief. Therefore, one bedrock principle of science is to minimize that bias by testing the predictions of one’s belief against objective data. But interpreting visualized data is a complex perceptual and cognitive process. Through two crowdsourced experiments, we demonstrate that supposedly objective assessments of the strength of a correlational relationship can be influenced by how strongly a viewer believes in the existence of that relationship. Participants viewed scatterplots depicting a relationship between meaningful variable pairs (e.g., number of environmental regulations and air quality) and estimated their correlations. They also estimated the correlation of the same scatterplots labeled instead with generic &#x27;X&#x27; and &#x27;Y&#x27; axes. In a separate section, they also reported how strongly they believed there to be a correlation between the meaningful variable pairs. Participants estimated correlations more accurately when they viewed scatterplots labeled with generic axes compared to scatterplots labeled with meaningful variable pairs. Furthermore, when viewers believed that two variables should have a strong relationship, they overestimated correlations between those variables by an r-value of about 0.1. When they believed that the variables should be unrelated, they underestimated the correlations by an r-value of about 0.1. While data visualizations are typically thought to present objective truths to the viewer, these results suggest that existing personal beliefs can bias even objective statistical values people extract from data.</td><td></td><td>VIS Full Paper</td><td>Questioning Data and Data Bias</td><td></td><td><a href="https://youtu.be/xBSZC2jj_wc">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1151.html">link</a></td></tr><tr><td>163</td><td>Data Hunches: Incorporating Personal Knowledge into Visualizations</td><td>Haihan Lin, Derya Akbaba, Miriah Meyer, Alexander Lex</td><td>The trouble with data is that it frequently provides only an imperfect representation of a phenomenon of interest. Experts who are familiar with their datasets will often make implicit, mental corrections when analyzing a dataset, or will be cautious not to be overly confident about their findings if caveats are present. However, personal knowledge about the caveats of a dataset is typically not incorporated in a structured way, which is problematic if others who lack that knowledge interpret the data. In this work, we define such analysts&#x27; knowledge about datasets as data hunches. We differentiate data hunches from uncertainty and discuss types of hunches. We then explore ways of recording data hunches, and, based on a prototypical design, develop recommendations for designing visualizations that support data hunches. We conclude by discussing various challenges associated with data hunches, including the potential for harm and challenges for trust and privacy. We envision that data hunches will empower analysts to externalize their knowledge, facilitate collaboration and communication, and support the ability to learn from others&#x27; data hunches.</td><td></td><td>VIS Full Paper</td><td>Questioning Data and Data Bias</td><td></td><td><a href="https://youtu.be/tZ4HaUAoSNw">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1155.html">link</a></td></tr><tr><td>164</td><td>Multi-View Design Patterns and Responsive Visualization for Genomics Data</td><td>Sehi L&#x27;Yi, Nils Gehlenborg</td><td>A series of recent studies has focused on designing cross-resolution and cross-device visualizations, i.e., responsive visualization, a concept adopted from responsive web design. However, these studies mainly focused on visualizations with a single view to a small number of views, and there are still unresolved questions about how to design responsive multi-view visualizations. In this paper, we present a reusable and generalizable framework for designing responsive multi-view visualizations focused on genomics data. To gain a better understanding of existing design challenges, we review web-based genomics visualization tools in the wild. By characterizing tools based on a taxonomy of responsive designs, we find that responsiveness is rarely supported in existing tools. To distill insights from the survey results in a systematic way, we classify typical view composition patterns, such as “vertically long,” “horizontally wide,” “circular,” and “cross-shaped” compositions. We then identify their usability issues in different resolutions that stem from the composition patterns, as well as discussing approaches to address the issues and to make genomics visualizations responsive. By extending the Gosling visualization grammar to support responsive constructs, we show how these approaches can be supported. A valuable follow-up study would be taking different input modalities into account, such as mouse and touch interactions, which was not considered in our study.</td><td></td><td>VIS Full Paper</td><td>DNA/Genome and Molecular Data/Vis</td><td></td><td><a href="https://youtu.be/dJVPjcxK_-Q">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1193.html">link</a></td></tr><tr><td>165</td><td>Vivern – A Virtual Environment for Multiscale Visualization and Modeling of DNA Nanostructures</td><td>David Kuťák, Matias Nicolás Selzer, Jan Byška, María Luján Ganuza, Ivan Barišić, Barbora Kozlíková, Haichao Miao</td><td>DNA nanostructures offer promising applications, particularly in the biomedical domain, as they can be used for targeted drug delivery, construction of nanorobots, or as a basis for molecular motors. One of the most prominent techniques for assembling these structures is DNA origami. Nowadays, desktop applications are used for the in silico design of such structures. However, as such structures are often spatially complex, their assembly and analysis are complicated. Since virtual reality was proven to be advantageous for such spatial-related tasks and there are no existing VR solutions focused on this domain, we propose Vivern, a VR application that allows domain experts to design and visually examine DNA origami nanostructures. Our approach presents different abstracted visual representations of the nanostructures, various color schemes, and an ability to place several DNA nanostructures and proteins in one environment, thus allowing for the detailed analysis of complex assemblies. We also present two novel examination tools, the Magic Scale Lens and the DNA Untwister, that allow the experts to visually embed different representations into local regions to preserve the context and support detailed investigation. To showcase the capabilities of our solution, prototypes of novel nanodevices conceptualized by our collaborating experts, such as DNA-protein hybrid structures and DNA origami superstructures, are presented. Finally, the results of two rounds of evaluations are summarized. They demonstrate the advantages of our solution, especially for scenarios where current desktop tools are very limited, while also presenting possible future research directions.</td><td>Virtual reality, abstraction, DNA origami, nanostructures, visualization, focus+context, interaction, in silico modeling, nanotechnology, multiscale, magic scale lens</td><td>VIS Full Paper</td><td>DNA/Genome and Molecular Data/Vis</td><td></td><td><a href="https://youtu.be/VNf4UaJls9Y">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9523759.html">link</a></td></tr><tr><td>166</td><td>GenoREC: A Recommendation System for Interactive Genomics Data Visualization</td><td>Aditeya Pandey, Sehi L&#x27;Yi, Qianwen Wang, Michelle A. Borkin, Nils Gehlenborg</td><td>Interpretation of genomics data is critically reliant on the application of a wide range of visualization tools. A large number of visualization techniques for genomics data and different analysis tasks pose a significant challenge for analysts: which visualization technique is most likely to help them generate insights into their data? Since genomics analysts typically have limited training in data visualization, their choices are often based on trial and error or guided by technical details, such as data formats that a specific tool can load. This approach prevents them from making effective visualization choices for the many combinations of data types and analysis questions they encounter in their work. Visualization recommendation systems assist non-experts in creating data visualization by recommending appropriate visualizations based on the data and task characteristics. However, existing visualization recommendation systems are not designed to handle domain-specific problems. To address these challenges, we designed GenoREC, a novel visualization recommendation system for genomics. GenoREC enables genomics analysts to select effective visualizations based on a description of their data and analysis tasks. Here, we present the recommendation model that uses a knowledge-based method for choosing appropriate visualizations and a web application that enables analysts to input their requirements, explore recommended visualizations, and export them for their usage. Furthermore, we present the results of two user studies demonstrating that GenoREC recommends visualizations that are both accepted by domain experts and suited to address the given genomics analysis problem. All supplemental materials are available at https://osf.io/y73pt/.</td><td></td><td>VIS Full Paper</td><td>DNA/Genome and Molecular Data/Vis</td><td></td><td><a href="https://youtu.be/UmYxcrR1PmY">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1212.html">link</a></td></tr><tr><td>167</td><td>sMolBoxes: Dataflow Model for Molecular Dynamics Exploration</td><td>Pavol Ulbrich, Manuela Waldner, Katarína Furmanová, Sérgio M. Marques, David Bednář, Barbora Kozlikova, Jan Byška</td><td>We present sMolBoxes, a dataflow representation for the exploration and analysis of long molecular dynamics (MD) simulations. When MD simulations reach millions of snapshots, a frame-by-frame observation is not feasible anymore. Thus, biochemists rely to a large extent only on quantitative analysis of geometric and physico-chemical properties. However, the usage of abstract methods to study inherently spatial data hinders the exploration and poses a considerable workload. sMolBoxes link quantitative analysis of a user-defined set of properties with interactive 3D visualizations. They enable visual explanations of molecular behaviors, which lead to an efficient discovery of biochemically significant parts of the MD simulation. sMolBoxes follow a node-based model for flexible definition, combination, and immediate evaluation of properties to be investigated. Progressive analytics enable fluid switching between multiple properties, which facilitates hypothesis generation. Each sMolBox provides quick insight to an observed property or function, available in more detail in the bigBox View. The case studies illustrate that even with relatively few sMolBoxes, it is possible to express complex analytical tasks, and their use in exploratory analysis is perceived as more efficient than traditional scripting-based methods.</td><td></td><td>VIS Full Paper</td><td>DNA/Genome and Molecular Data/Vis</td><td></td><td><a href="https://youtu.be/Bu2_1uwUdUY">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1175.html">link</a></td></tr><tr><td>168</td><td>Molecumentary: Adaptable Narrated Documentaries Using Molecular Visualization</td><td>David Kouřil, Ondřej Strnad, Peter Mindek, Sarkis Halladjian, Tobias Isenberg, M. Eduard Gröller, Ivan Viola</td><td>We present a method for producing documentary-style content using real-time scientific visualization. We introduce molecumentaries, i.e., molecular documentaries featuring structural models from molecular biology, created through adaptable methods instead of the rigid traditional production pipeline. Our work is motivated by the rapid evolution of scientific visualization and it potential in science dissemination. Without some form of explanation or guidance, however, novices and lay-persons often find it difficult to gain insights from the visualization itself. We integrate such knowledge using the verbal channel and provide it along an engaging visual presentation. To realize the synthesis of a molecumentary, we provide technical solutions along two major production steps: (1) preparing a story structure and (2) turning the story into a concrete narrative. In the first step, we compile information about the model from heterogeneous sources into a story graph. We combine local knowledge with external sources to complete the story graph and enrich the final result. In the second step, we synthesize a narrative, i.e., story elements presented in sequence, using the story graph. We then traverse the story graph and generate a virtual tour, using automated camera and visualization transitions. We turn texts written by domain experts into verbal representations using text-to-speech functionality and provide them as a commentary. Using the described framework, we synthesize fly-throughs with descriptions: automatic ones that mimic a manually authored documentary or semi-automatic ones which guide the documentary narrative solely through curated textual input.</td><td>Virtual tour, audio, biological data, storytelling, illustrative visualization.</td><td>VIS Full Paper</td><td>DNA/Genome and Molecular Data/Vis</td><td></td><td><a href="https://youtu.be/y1N9285aVIo">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9627526.html">link</a></td></tr><tr><td>169</td><td>Polyphony: an Interactive Transfer Learning Framework for Single-Cell Data Analysis</td><td>Furui Cheng, Mark S Keller, Huamin Qu, Nils Gehlenborg, Qianwen Wang</td><td>Reference-based cell-type annotation can significantly reduce time and effort in single-cell analysis by transferring labels from a previously-annotated dataset to a new dataset. However, label transfer by end-to-end computational methods is challenging due to the entanglement of technical (e.g., from different sequencing batches or techniques) and biological (e.g., from different cellular microenvironments) variations, only the first of which must be removed. To address this issue, we propose Polyphony, an interactive transfer learning (ITL) framework, to complement biologists&#x27; knowledge with advanced computational methods. Polyphony is motivated and guided by domain experts&#x27; needs for a controllable, interactive, and algorithm-assisted annotation process, identified through interviews with seven biologists. We introduce anchors, i.e., analogous cell populations across datasets, as a paradigm to explain the computational process and collect user feedback for model improvement. We further design a set of visualizations and interactions to empower users to add, delete, or modify anchors, resulting in refined cell type annotations. The effectiveness of this approach is demonstrated through quantitative experiments, two hypothetical use cases, and interviews with two biologists. The results show that our anchor-based ITL method takes advantage of both human and machine intelligence in annotating massive single-cell datasets.</td><td></td><td>VIS Full Paper</td><td>DNA/Genome and Molecular Data/Vis</td><td></td><td><a href="https://youtu.be/FkbFjJO8QZY">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1578.html">link</a></td></tr><tr><td>170</td><td>CohortVA: A Visual Analytic System for Interactive Exploration of Cohorts based on Historical Data</td><td>Wei Zhang, Jason Kamkwai Wong, Xumeng Wang, Youcheng Gong, Rongchen Zhu, Kai Liu, Zihan Yan, Siwei Tan, Huamin Qu, Siming Chen, Wei Chen</td><td>In history research, cohort analysis seeks to identify social structures and figure mobilities by studying the group-based behavior of historical figures. Prior works mainly employ automatic data mining approaches, lacking effective visual explanation. In this paper, we present CohortVA, an interactive visual analytic approach that enables historians to incorporate expertise and insight into the iterative exploration process. The kernel of CohortVA is a novel identification model that generates candidate cohorts and constructs cohort features by means of pre-built knowledge graphs constructed from large-scale history databases. We propose a set of coordinated views to illustrate identified cohorts and features coupled with historical events and figure profiles. Two case studies and interviews with historians demonstrate that CohortVA can greatly enhance the capabilities of cohort identifications, figure authentications, and hypothesis generation.</td><td></td><td>VIS Full Paper</td><td>Digital Humanities, e-Commerce, and Engineering</td><td></td><td><a href="https://youtu.be/Pyn6-kD13Ho">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1426.html">link</a></td></tr><tr><td>171</td><td>PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics</td><td>Chenyang Zhang, Xiyuan Wang, Chuyi Zhao, Yijing Ren, Tianyu Zhang, Zhenhui Peng, Xiaomeng Fan, Xiaojuan Ma, Quan Li</td><td>Promotions are commonly used by e-commerce merchants to boost sales. The efficacy of different promotion strategies can help sellers adapt their offering to customer demand in order to survive and thrive. Current approaches to designing promotion strategies are either based on econometrics, which may not scale to large amounts of sales data, or are spontaneous and provide little explanation of sales volume. Moreover, accurately measuring the effects of promotion designs and making bootstrappable adjustments accordingly remains a challenge due to the incompleteness and complexity of the information describing promotion strategies and their market environments. We present PromotionLens, a visual analytics system for exploring, comparing, and modeling the impact of various promotion strategies. Our approach combines representative multivariant time-series forecasting models and well-designed visualizations to demonstrate and explain the impact of sales and promotional factors, and to support &quot;what-if&quot; analysis of promotions. Two case studies, expert feedback, and a qualitative user study demonstrate the efficacy of PromotionLens.</td><td></td><td>VIS Full Paper</td><td>Digital Humanities, e-Commerce, and Engineering</td><td></td><td><a href="https://youtu.be/pbwjVySy8O8">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1421.html">link</a></td></tr><tr><td>172</td><td>Interactive Visual Analysis of Structure-borne Noise Data</td><td>Rainer Splechtna, Denis Gracanin, Goran Todorovic, Stanislav Goja, Boris Bedic, Helwig Hauser, Kresimir Matkovic</td><td>Numerical simulation has become omnipresent in the automotive domain, posing new challenges such as high-dimensional parameter spaces and large as well as incomplete and multi-faceted data. In this design study, we show how interactive visual exploration and analysis of high-dimensional, spectral data from noise simulation can facilitate design improvements in the context of conflicting criteria. Here, we focus on structure-borne noise, i.e., noise from vibrating mechanical parts. Detecting problematic noise
 sources early in the design and production process is essential for reducing a product’s development costs and its time to market. In a close collaboration of visualization and automotive engineering, we designed a new, interactive approach to quickly identify and
 analyze critical noise sources, also contributing to an improved understanding of the analyzed system. Several carefully designed, interactive linked views enable the exploration of noises, vibrations, and harshness at multiple levels of detail, both in the frequency and spatial domain. This enables swift and smooth changes of perspective; selections in the frequency domain are immediately reflected in the spatial domain, and vice versa. Noise sources are quickly identified and shown in the context of their neighborhood, both in the frequency and spatial domain. We propose a novel drill-down view, especially tailored to noise data analysis. Split boxplots and synchronized 3D geometry views support comparison tasks. With this solution, engineers iterate over design optimizations much faster, while maintaining a good overview at each iteration. We evaluated the new approach in the automotive industry, studying noise
 simulation data for an internal combustion engine.</td><td></td><td>VIS Full Paper</td><td>Digital Humanities, e-Commerce, and Engineering</td><td></td><td><a href="https://youtu.be/k6SuFkvq4FY">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1547.html">link</a></td></tr><tr><td>173</td><td>Traveler: Navigating Task Parallel Traces for Performance Analysis</td><td>Sayef Azad Sakin, Alex Bigelow, Mohammad Tohid, Connor Scully-Allison, Carlos Scheidegger, Steven Robert Brandt, Christopher P. Taylor, Kevin A. Huck, Hartmut Kaiser, Katherine E. Isaacs</td><td>Understanding the behavior of software in execution is a key step in identifying and fixing performance issues. This is especially important in high performance computing contexts where even minor performance tweaks can translate into large savings in terms of computational resource use. To aid performance analysis, developers may collect an execution trace—a chronological log of program activity during execution. As traces represent the full history, developers can discover a wide array of possibly previously unknown performance issues, making them an important artifact for exploratory performance analysis. However, interactive trace visualization is difficult due to issues of data size and complexity of meaning. Traces represent nanosecond-level events across many parallel processes, meaning the collected data is often large and difficult to explore. The rise of asynchronous task parallel programming paradigms complicates the relation between events and their probable cause. To address these challenges, we conduct a continuing design study in collaboration with high performance computing researchers. We develop diverse and hierarchical ways to navigate and represent execution trace data in support of their trace analysis tasks. Through an iterative design process, we developed Traveler, an integrated visualization platform for task parallel traces. Traveler provides multiple linked interfaces to help navigate trace data from multiple contexts. We evaluate the utility of Traveler through feedback from users and a case study, finding that integrating multiple modes of navigation in our design supported performance analysis tasks and led to the discovery of previously unknown behavior in a distributed array library.</td><td></td><td>VIS Full Paper</td><td>Digital Humanities, e-Commerce, and Engineering</td><td></td><td><a href="https://youtu.be/z4pGQdqGEJ4">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1312.html">link</a></td></tr><tr><td>174</td><td>Visual Analysis and Detection of Contrails in Aircraft Engine Simulations</td><td>Md Nafiul Alam Nipu, Carla Gabriela Floricel, Negar Naghash Zadeh, Roberto Paoli, G. Elisabeta Marai</td><td>Contrails are condensation trails generated from emitted particles by aircraft engines, which perturb Earth’s radiation budget. Simulation modeling is used to interpret the formation and development of contrails. These simulations are computationally intensive and rely on high-performance computing solutions, and the contrail structures are not well defined. We propose a visual computing system to assist in defining contrails and their characteristics, as well as in the analysis of parameters for computer-generated aircraft engine simulations. The back-end of our system leverages a contrail-formation criterion and clustering methods to detect contrails’ shape and evolution and identify similar simulation runs. The front-end system helps analyze contrails and their parameters across multiple simulation runs. The evaluation with domain experts shows this approach successfully aids in contrail data investigation.</td><td></td><td>VIS Full Paper</td><td>Digital Humanities, e-Commerce, and Engineering</td><td></td><td><a href="https://youtu.be/MzQT1pGOAyw">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1617.html">link</a></td></tr><tr><td>175</td><td>DPVisCreator: Incorporating Pattern Constraints to Privacy-preserving Visualizations via Differential Privacy</td><td>Jiehui Zhou, Xumeng Wang, Jason Kamkwai Wong, Huanliang Wang, Zhongwei Wang, Xiaoyu Yang, Xiaoran Yan, Haozhe Feng, Huamin Qu, Haochao Ying, Wei Chen</td><td>Data privacy is an essential issue in publishing data visualizations. However, it is challenging to represent multiple data patterns in privacy-preserving visualizations. The prior approaches target specific chart types or perform an anonymization model uniformly without considering the importance of data patterns in visualizations. In this paper, we propose a visual analytics approach that facilitates data custodians to generate multiple private charts while maintaining user-preferred patterns. To this end, we introduce pattern constraints to model users&#x27; preferences over data patterns in the dataset and incorporate them into the proposed Bayesian network-based Differential Privacy (DP) model PriVis. A prototype system, DPVisCreator, is developed to assist data custodians in implementing our approach. The effectiveness of our approach is demonstrated with quantitative evaluation of pattern utility under the different levels of privacy protection, case studies, and semi-structured expert interviews.</td><td></td><td>VIS Full Paper</td><td>Digital Humanities, e-Commerce, and Engineering</td><td></td><td><a href="https://youtu.be/LYdxLA3hD3c">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1478.html">link</a></td></tr><tr><td>176</td><td>ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections</td><td>Manlio Miguel Massiris Fernandez, Sanjin Rados, Eduard Gröller, Claudio Delrieux, Kresimir Matkovic</td><td>Ergonomic risk assessment is now, due to an increased awareness, carried out more often than in the past. The conventional risk assessment evaluation, based on expert-assisted observation of the workplaces and manually filling in score tables, is still predominant. Data analysis is usually done with a focus on critical moments, although without the support of contextual information and changes over time. In this paper we introduce ErgoExplorer, a system for the interactive visual analysis of risk assessment data. In contrast to the current practice, we focus on data that span across multiple actions and multiple workers while keeping all contextual information. Data is automatically extracted from video streams. Based on carefully investigated analysis tasks, we introduce new views and their corresponding interactions. These views also incorporate domain-specific score tables to guarantee an easy adoption by domain experts. All views are integrated into ErgoExplorer, which relies on coordinated multiple views to facilitate analysis through interaction.
 ErgoExplorer makes it possible for the first time to examine complex relationships between risk assessments of individual body parts over long sessions that span multiple operations. The newly introduced approach supports analysis and exploration at several levels of detail, ranging from a general overview, down to inspecting individual frames in the video stream, if necessary. We illustrate the usefulness of the newly proposed approach applying it to several datasets.</td><td></td><td>VIS Full Paper</td><td>Decision Making and Reasoning</td><td></td><td><a href="https://youtu.be/MJFVNNI5Pqs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1568.html">link</a></td></tr><tr><td>177</td><td>TRAFFICVIS: Visualizing Organized Activity and Spatio-Temporal Patterns for Detecting and Labeling Human Trafficking</td><td>Catalina Vajiac, Duen Horng Chau, Andreas Olligschlaeger, Rebecca Mackenzie, Pratheeksha Nair, Meng-Chieh Lee, Yifei Li, Namyong Park, Reihaneh Rabbany, Christos Faloutsos</td><td>Law enforcement and domain experts can detect human trafficking (HT) in online escort websites by analyzing suspicious clusters of connected ads. How can we explain clustering results intuitively and interactively, visualizing potential evidence for experts to analyze?
 We present TrafficVis, the first interface for cluster-level HT detection and labeling. Developed through months of participatory design with domain experts, TrafficVis provides coordinated views in conjunction with carefully chosen backend algorithms to effectively show spatio-temporal and text patterns to a wide variety of anti-HT stakeholders. We build upon state-of-the-art text clustering algorithms by incorporating shared metadata as a signal of connected and possibly suspicious activity, then visualize the results. Domain experts can use TrafficVis to label clusters as HT, or other, suspicious, but non-HT activity such as spam and scam, quickly creating labeled datasets to enable further HT research.
 Through domain expert feedback and a usage scenario, we demonstrate TrafficVis&#x27;s efficacy. The feedback was overwhelmingly positive, with repeated high praises for the usability and explainability of our tool, the latter being vital for indicting possible criminals.</td><td></td><td>VIS Full Paper</td><td>Decision Making and Reasoning</td><td></td><td><a href="https://youtu.be/VTTTk0pfc8M">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1614.html">link</a></td></tr><tr><td>178</td><td>Outcome-Explorer: A Causality Guided Interactive Visual Interface for Interpretable Algorithmic Decision Making</td><td>Hoque, Md Naimul，Mueller, Klaus</td><td>The widespread adoption of algorithmic decision-making systems has brought about the necessity to interpret the reasoning behind these decisions. The majority of these systems are complex black box models, and auxiliary models are often used to approximate and then explain their behavior. However, recent research suggests that such explanations are not overly accessible to lay users with no specific expertise in machine learning and this can lead to an incorrect interpretation of the underlying model. In this paper, we show that a predictive and interactive model based on causality is inherently interpretable, does not require any auxiliary model, and allows both expert and non-expert users to understand the model comprehensively. To demonstrate our method we developed Outcome Explorer, a causality guided interactive interface, and evaluated it by conducting think-aloud sessions with three expert users and a user study with 18 non-expert users. All three expert users found our tool to be comprehensive in supporting their explanation needs while the non-expert users were able to understand the inner workings of a model easily.</td><td>Explainable AI, Causality, Visual Analytics, Human-Computer Interaction.</td><td>VIS Full Paper</td><td>Decision Making and Reasoning</td><td></td><td><a href="https://youtu.be/ZO39xdgUFOA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9507307.html">link</a></td></tr><tr><td>179</td><td>MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry</td><td>Chuhan Shi, Fei Nie, Yicheng Hu, Yige Xu, Lei Chen, Xiaojuan Ma, Qiong Luo</td><td>Interdisciplinary experimental science (e.g., medicinal chemistry) refers to the disciplines that integrate knowledge from different scientific backgrounds and involve experiments in the research process. Deciding &quot;in what direction to proceed&quot; is critical for the success of the research in such disciplines, since the time, money, and resource costs of the subsequent research steps depend largely on this decision. However, such a direction identification task is challenging in that researchers need to integrate information from large-scale, heterogeneous materials from all associated disciplines and summarize the related publications of which the core contributions are often showcased in diverse formats. The task also requires researchers to estimate the feasibility and potential in future experiments in the selected directions. In this work, we selected medicinal chemistry as a case and presented an interactive visual tool, MedChemLens, to assist medicinal chemists in choosing their intended directions of research. This task is also known as drug target (i.e., disease-linked proteins) selection. Given a candidate target name, MedChemLens automatically extracts the molecular features of drug compounds from chemical papers and clinical trial records, organizes them based on the drug structures, and interactively visualizes factors concerning subsequent experiments. We evaluated MedChemLens through a within-subjects study (N=16). Compared with the control condition (i.e., unrestricted online search without using our tool), participants who only used MedChemLens reported faster search, better-informed selections, higher confidence in their selections, and lower cognitive load.</td><td></td><td>VIS Full Paper</td><td>Decision Making and Reasoning</td><td></td><td><a href="https://youtu.be/_n2PNb6qzuM">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1400.html">link</a></td></tr><tr><td>180</td><td>GestureLens: Visual Analysis of Gestures in Presentation Videos</td><td>Haipeng Zeng, Xingbo Wang, Yong Wang, Aoyu Wu, Ting Chuen Pong, Huamin Qu</td><td>Appropriate gestures can enhance message delivery and audience engagement in both daily communication and public presentations. In this paper, we contribute a visual analytic approach that assists professional public speaking coaches in improving their practice of gesture training through analyzing presentation videos. Manually checking and exploring gesture usage in the presentation videos is often tedious and time-consuming. There lacks an efficient method to help users conduct gesture exploration, which is challenging due to the intrinsically temporal evolution of gestures and their complex correlation to speech content. In this paper, we propose GestureLens, a visual analytics system to facilitate gesture-based and content-based exploration of gesture usage in presentation videos. Specifically, the exploration view enables users to obtain a quick overview of the spatial and temporal distributions of gestures. The dynamic hand movements are firstly aggregated through a heatmap in the gesture space for uncovering spatial patterns, and then decomposed into two mutually perpendicular timelines for revealing temporal patterns. The relation view allows users to explicitly explore the correlation between speech content and gestures by enabling linked analysis and intuitive glyph designs. The video view and dynamic view show the context and overall dynamic movement of the selected gestures, respectively. Two usage scenarios and expert interviews with professional presentation coaches demonstrate the effectiveness and usefulness of GestureLens in facilitating gesture exploration and analysis of presentation videos.</td><td>Gesture, hand movements, presentation video analysis, visual analysis.</td><td>VIS Full Paper</td><td>Decision Making and Reasoning</td><td></td><td><a href="https://youtu.be/HiXjBx7rlhM">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9761750.html">link</a></td></tr><tr><td>181</td><td>Visual Concept Programming: A Visual Analytics Approach to Injecting Human Intelligence at Scale</td><td>Md Naimul Hoque, Wenbin He, Shekar Arvind Kumar, Liang Gou, Liu Ren</td><td>Data-centric AI has emerged as a new research area to systematically engineer the data to land AI models for real-world applications. As a core method for data-centric AI, data programming helps experts inject domain knowledge into data and label data at scale using carefully designed labeling functions (e.g., heuristic rules, logistics). Though data programming has shown great success in the NLP domain, it is challenging to program image data because of a) the challenge to describe images using visual vocabulary without human annotations and b) lacking efficient tools for data programming of images. We present Visual Concept Programming, a first-of-its-kind visual analytics approach of using visual concepts to program image data at scale while requiring a few human efforts. Our approach is built upon three unique components. It first uses a self-supervised learning approach to learn visual representation at the pixel level and extract a dictionary of visual concepts from images without using any human annotations. The visual concepts serve as building blocks of labeling functions for experts to inject their domain knowledge. We then design interactive visualizations to explore and understand visual concepts and compose labeling functions with concepts without writing code. Finally, with the composed labeling functions, users can label the image data at scale and use the labeled data to refine the pixel-wise visual representation and concept quality. We evaluate the learned pixel-wise visual representation for the downstream task of semantic segmentation to show the effectiveness and usefulness of our approach. In addition, we demonstrate how our approach tackles real-world problems of image retrieval for autonomous driving.</td><td></td><td>VIS Full Paper</td><td>Decision Making and Reasoning</td><td></td><td><a href="https://youtu.be/Rq9T84ssE0o">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1667.html">link</a></td></tr><tr><td>182</td><td>The Influence of Visual Provenance Representations on Strategies in a Collaborative Hand-off Data Analysis Scenario</td><td>Jeremy E Block, Shaghayegh Esmaeili, Eric Ragan, John Goodall, Gregory David Richardson</td><td>Conducting data analysis tasks rarely occur in isolation. Especially in intelligence analysis scenarios where different experts contribute knowledge to a shared understanding, members must communicate how insights develop to establish common ground among collaborators. The use of provenance to communicate analytic sensemaking carries promise by describing the interactions and summarizing the steps taken to reach insights. Yet, no universal guidelines exist for communicating provenance in different settings. Our work focuses on the presentation of provenance information and the resulting conclusions reached and strategies used by new analysts. In an open-ended, 30-minute, textual exploration scenario, we qualitatively compare how adding different types of provenance information (speciﬁcally data coverage and interaction history) affects analysts’ conﬁdence in conclusions developed, propensity to repeat work, ﬁltering of data, identiﬁcation of relevant information, and typical investigation strategies. We see that data coverage (i.e. what was interacted with) provides provenance information without limiting individual investigation freedom. On the other hand, while interaction history (i.e. when something was interacted with) does not signiﬁcantly encourage more mimicry, it does take more time to comfortably understand, as represented by less conﬁdent conclusions and less relevant information gathering behaviors. Our results contribute empirical data towards understanding how provenance summarizations can inﬂuence analysis behaviors.</td><td></td><td>VIS Full Paper</td><td>Provenance and Guidance</td><td></td><td><a href="https://youtu.be/-Bf7XWXbvLk">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1089.html">link</a></td></tr><tr><td>183</td><td>Understanding How In-Visualization Provenance Can Support Trade-off Analysis</td><td>Mehdi Chakhchoukh, Nadia Boukhelifa, Anastasia Bezerianos</td><td>In domains such as agronomy or manufacturing, experts need to consider trade-offs when making decisions that involve several, often competing, objectives. Such analysis is complex and may be conducted over long periods of time, making it hard to revisit.In this paper, we consider the use of analytic provenance mechanisms to aid experts recall and keep track of trade-off analysis. Weimplemented VisProm, a web-based trade-off analysis system, that incorporates in-visualization provenance views, designed to helpexperts keep track of trade-offs and their objectives. We used VisProm as a technology probe to understand user needs and explore thepotential role of provenance in this context. Through observation sessions with three groups of experts analyzing their own data, we makethe following contributions. We first, identify eight high-level tasks that experts engaged in during trade-off analysis, such as locating andcharacterizing interest zones in the trade-off space, and show how these tasks can be supported by provenance visualization. Second,we refine findings from previous work on provenance purposes such as recall and reproduce, by identifying specific objects of thesepurposes related to trade-off analysis, such as interest zones, and exploration structure (e.g., exploration of alternatives and branches).Third, we discuss insights on how the identified provenance objects and our designs support these trade-off analysis tasks, both whenrevisiting past analysis and while actively exploring. And finally, we identify new opportunities for provenance-driven trade-off analysis, forexample related to monitoring the coverage of the trade-off space, and tracking alternative trade-off scenario.</td><td>Provenance, visualization, trade-offs, multi-criteria, decision making, qualitative study</td><td>VIS Full Paper</td><td>Provenance and Guidance</td><td></td><td><a href="https://youtu.be/0ucOrp2mEiY">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9768153.html">link</a></td></tr><tr><td>184</td><td>Provectories: Embedding-based Analysis of Interaction Provenance Data</td><td>Conny Walchshofer, Andreas Hinterreiter, Kai Xu, Holger Stitz, Marc Streit</td><td>Understanding user behavior patterns and visual analysis strategies is a long-standing challenge. Existing approaches rely largely on time-consuming manual processes such as interviews and the analysis of observational data. While it is technically possible to capture a history of user interactions and application states, it remains difficult to extract and describe analysis strategies based on interaction provenance. In this paper, we propose a novel visual approach to the meta-analysis of interaction provenance. We capture single and multiple user sessions as graphs of high-dimensional application states. Our meta-analysis is based on two different types of two-dimensional embeddings of these high-dimensional states: layouts based on (i) topology and (ii) attribute similarity. We applied these visualization approaches to synthetic and real user provenance data captured in two user studies. From our visualizations, we were able to extract patterns for data types and analytical reasoning strategies.</td><td>Visualization techniques, Information visualization, Visual analytics, Interaction Provenance, Sensemaking</td><td>VIS Full Paper</td><td>Provenance and Guidance</td><td></td><td><a href="https://youtu.be/od45CtTCsLw">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9652041.html">link</a></td></tr><tr><td>185</td><td>Lotse: A Practical Framework for Guidance in Visual Analytics</td><td>Fabian Sperrle, Davide Ceneda, Mennatallah El-Assady</td><td>Co-adaptive guidance aims to enable efficient human-machine collaboration in visual analytics, as proposed by multiple theoretical frameworks. This paper bridges the gap between such conceptual frameworks and practical implementation by introducing an accessible model of guidance and an accompanying guidance library, mapping theory into practice. We contribute a model of system-provided guidance based on design templates and derived strategies. We instantiate the model in a library called Lotse that allows specifying guidance strategies in definition files and generates running code from them. Lotse is the first guidance library using such an approach. It supports the creation of reusable guidance strategies to retrofit existing applications with guidance and fosters the creation of general guidance strategy patterns. We demonstrate its effectiveness through first-use case studies with VA researchers of varying guidance design expertise and find that they are able to effectively and quickly implement guidance with Lotse. Further, we analyze our framework&#x27;s cognitive dimensions to evaluate its expressiveness and outline a summary of open research questions for aligning guidance practice with its intricate theory.</td><td></td><td>VIS Full Paper</td><td>Provenance and Guidance</td><td></td><td><a href="https://youtu.be/cYallUixgps">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1003.html">link</a></td></tr><tr><td>186</td><td>Medley: Intent-based Recommendations to Support Dashboard Composition</td><td>Aditeya Pandey, Arjun Srinivasan, Vidya Setlur</td><td>Despite the ever-growing popularity of dashboards across a wide range of domains, their authoring still remains a tedious and complex process. Current tools offer considerable support for creating individual visualizations but provide limited support for discovering groups of visualizations that can be collectively useful for composing analytic dashboards. To address this problem, we present MEDLEY, a mixed-initiative interface that assists in dashboard composition by recommending dashboard collections (i.e., a logically grouped set of views and filtering widgets) that map to specific analytical intents. Users can specify dashboard intents (namely, measure analysis, change analysis, category analysis, or distribution analysis) explicitly through an input panel in the interface or implicitly by selecting data attributes and views of interest. The system recommends collections based on these analytic intents, and views and widgets can be selected to compose a variety of dashboards. MEDLEY also provides a lightweight direct manipulation interface to configure interactions between views in a dashboard. Based on a study with 13 participants performing both targeted and open-ended tasks, we discuss how MEDLEY&#x27;s recommendations guide dashboard composition and facilitate different user workflows. Observations from the study identify potential directions for future work, including combining manual view specification with dashboard recommendations and designing natural language interfaces for dashboard authoring.</td><td></td><td>VIS Full Paper</td><td>Provenance and Guidance</td><td></td><td><a href="https://youtu.be/iGMQYKgfPnI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1142.html">link</a></td></tr><tr><td>187</td><td>GEViTRec: Data Reconnaissance Through Recommendation Using a Domain-Specific Visualization Prevalence Design Space</td><td>Anamaria Crisan, Shannah Fisher, Jennifer L. Gardy, Tamara Munzner</td><td>Genomic Epidemiology (genEpi) is a branch of public health that uses many different data types including tabular, network, genomic, and geographic, to identify and contain outbreaks of deadly diseases. Due to the volume and variety of data, it is challenging for genEpi domain experts to conduct data reconnaissance; that is, have an overview of the data they have and make assessments toward its quality, completeness, and suitability. We present an algorithm for data reconnaissance through automatic visualization recommendation, GEViTRec. Our approach handles a broad variety of dataset types and automatically generates visually coherent combinations of charts, in contrast to existing systems that primarily focus on singleton visual encodings of tabular datasets. We automatically detect linkages across multiple input datasets by analyzing non-numeric attribute fields, creating a data source graph within which we analyze and rank paths. For each high-ranking path, we specify chart combinations with positional and color alignments between shared fields, using a gradual binding approach to transform initial partial specifications of singleton charts to complete specifications that are aligned and oriented consistently. A novel aspect of our approach is its combination of domain-agnostic elements with domain-specific information that is captured through a domain-specific visualization prevalence design space. Our implementation is applied to both synthetic data and real Ebola outbreak data. We compare GEViTRec’s output to what previous visualization recommendation systems would generate, and to manually crafted visualizations used by practitioners. We conducted formative evaluations with ten genEpi experts to assess the relevance and interpretability of our results.</td><td>Heterogeneous Data, Multiple Coordinated Views, Data Reconnaissance, Bioinformatics.</td><td>VIS Full Paper</td><td>Provenance and Guidance</td><td></td><td><a href="https://youtu.be/z9yeTw8hbfI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9524484.html">link</a></td></tr><tr><td>188</td><td>FoVolNet: Fast Volume Rendering using Foveated Deep Neural Networks</td><td>David Bauer, Qi Wu, Kwan-Liu Ma</td><td>Volume data is found in many important scientific and engineering applications. Rendering this data for visualization at high quality and interactive rates for demanding applications such as virtual reality is still not easily achievable even using professional-grade hardware. We introduce FoVolNet --- a method to significantly increase the performance of volume data visualization. We develop a cost-effective foveated rendering pipeline that sparsely samples a volume around a focal point and reconstructs the full-frame using a deep neural network. Foveated rendering is a technique that prioritizes rendering computations around the user&#x27;s focal point. This approach leverages properties of the human visual system, thereby saving computational resources when rendering data in the periphery of the user&#x27;s field of vision. Our reconstruction network combines direct and kernel prediction methods to produce fast, stable, and perceptually convincing output. With a slim design and the use of quantization, our method outperforms state-of-the-art neural reconstruction techniques in both end-to-end frame times and visual quality. We conduct extensive evaluations of the system&#x27;s rendering performance, inference speed, and perceptual properties, and we provide comparisons to competing neural image reconstruction techniques. Our test results show that FoVolNet consistently achieves significant time saving over conventional rendering while preserving perceptual quality.</td><td></td><td>VIS Full Paper</td><td>(Volume) Rendering</td><td></td><td><a href="https://youtu.be/TCQiw2DTc_0">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1010.html">link</a></td></tr><tr><td>189</td><td>GRay: Ray Casting for Visualization and Interactive Data Exploration of Gaussian Mixture Models</td><td>Kai Lawonn, Monique Meuschke, Pepe Eulzer, Matthias Mitterreiter, Joachim Giesen, Tobias Günther</td><td>The Gaussian mixture model (GMM) describes the distribution of random variables from several different populations. GMMs have widespread applications in probability theory, statistics, machine learning for unsupervised cluster analysis and topic modeling, as well as in deep learning pipelines. So far, few efforts have been made to explore the underlying point distribution in combination with the GMMs, in particular when the data becomes high-dimensional and when the GMMs are composed of many Gaussians. We present an analysis tool comprising various GPU-based visualization techniques to explore such complex GMMs. To facilitate the exploration of high-dimensional data, we provide a novel navigation system to analyze the underlying data. Instead of projecting the data to 2D, we utilize interactive 3D views to better support users in understanding the spatial arrangements of the Gaussian distributions. The interactive system is composed of two parts: (1) raycasting-based views that visualize cluster memberships, spatial arrangements, and support the discovery of new modes. (2) overview visualizations that enable the comparison of Gaussians with each other, as well as small multiples of different choices of basis vectors. Users are supported in their exploration with customization tools and smooth camera navigations. Our tool was developed and assessed by five domain experts, and its usefulness was evaluated with 23 participants. To demonstrate the effectiveness, we identify interesting features in several data sets.</td><td></td><td>VIS Full Paper</td><td>(Volume) Rendering</td><td></td><td><a href="https://youtu.be/4KbR6BNn-ws">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1418.html">link</a></td></tr><tr><td>190</td><td>Quick Clusters: A GPU-Parallel Partitioning for Efficient Path Tracing of Unstructured Volumetric Grids</td><td>Nate Morrical, Alper Sahistan, Ugur Gudukbay, Ingo Wald, Valerio Pascucci</td><td>We propose a simple yet effective method for clustering finite elements to improve preprocessing times and rendering performance of unstructured volumetric grids without requiring auxiliary connectivity data. Rather than building bounding volume hierarchies (BVHs) over individual elements, we sort elements along with a Hilbert curve and aggregate neighboring elements together, improving BVH memory consumption by over an order of magnitude. Then to further reduce memory consumption, we cluster the mesh on the fly into sub-meshes with smaller indices using a series of efficient parallel mesh re-indexing operations. 
 These clusters are then passed to a highly optimized ray tracing API for point containment queries and ray-cluster intersection testing. Each cluster is assigned a maximum extinction value for adaptive sampling, which we rasterize into non-overlapping view-aligned bins allocated along the ray. These maximum extinction bins are then used to guide the placement of samples along the ray during visualization, reducing the number of samples required by multiple orders of magnitude (depending on the dataset), thereby improving overall visualization interactivity. Using our approach, we improve rendering performance over a competitive baseline on the NASA Mars Lander dataset from 6X (1 frame per second (fps) and 1.0M rays per second (rps) up to now 6fps and 12.4~M rps, now including volumetric shadows) while simultaneously reducing memory consumption by 3X (33GB down to 11GB) and avoiding any offline preprocessing steps, enabling high-quality interactive visualization on consumer graphics cards. Then by utilizing the full 48GB of an RTX 8000, we improve the performance of Lander by 17X (1fps up to 17fps, 1.0M rps up to 35.6M rps).</td><td></td><td>VIS Full Paper</td><td>(Volume) Rendering</td><td></td><td><a href="https://youtu.be/Msu4ZX01FHY">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1339.html">link</a></td></tr><tr><td>191</td><td>Finding Nano-Ötzi: Cryo-Electron Tomography Visualization Guided by Learned Segmentation</td><td>Ngan Nguyen, Ciril Bohak, Dominik Engel, Peter Mindek, Ondřej Strnad, Peter Wonka, Sai Li, Timo Ropinski, Ivan Viola</td><td>Cryo-electron tomography (cryo-ET) is a new 3D imaging technique with unprecedented potential for resolving submicron structural details. Existing volume visualization methods, however, are not able to reveal details of interest due to low signal-to-noise ratio. In order to design more powerful transfer functions, we propose leveraging soft segmentation as an explicit component of visualization for noisy volumes. Our technical realization is based on semi-supervised learning, where we combine the advantages of two segmentation algorithms. First, the weak segmentation algorithm provides good results for propagating sparse user-provided labels to other voxels in the same volume and is used to generate dense pseudo-labels. Second, the powerful deep-learning-based segmentation algorithm learns from these pseudo-labels to generalize the segmentation to other unseen volumes, a task that the weak segmentation algorithm fails at completely. The proposed volume visualization uses deep-learning-based segmentation as a component for segmentation-aware transfer function design. Appropriate ramp parameters can be suggested automatically through frequency distribution analysis. Furthermore, our visualization uses gradient-free ambient occlusion shading to further suppress the visual presence of noise, and to give structural detail the desired prominence. The cryo-ET data studied in our technical experiments are based on the highest-quality tilted series of intact SARS-CoV-2 virions. Our technique shows the high impact in target sciences for visual data analysis of very noisy volumes that cannot be visualized with existing techniques.</td><td>Volume Rendering; Computer Graphics Techniques; Machine Learning Techniques; Scalar Field Data; Life Sciences</td><td>VIS Full Paper</td><td>(Volume) Rendering</td><td></td><td><a href="https://youtu.be/wJlMWafiNHU">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9806341.html">link</a></td></tr><tr><td>192</td><td>Level Set Restricted Voronoi Tessellation for Large scale Spatial Statistical Analysis</td><td>Tyson Neuroth, Martin Rieth, Myoungkyu Lee, Konduri Aditya, Jacqueline Chen, Kwan-Liu Ma</td><td>Spatial statistical analysis of multivariate volumetric data can be challenging due to scale, complexity, and occlusion. Advances in topological segmentation, feature extraction, and statistical summarization have helped overcome the challenges. This work introduces a new spatial statistical decomposition method based on level sets, connected components, and a novel variation of the restricted centroidal Voronoi tessellation that is better suited for spatial statistical decomposition and parallel efficiency. The resulting data structures organize features into a coherent nested hierarchy to support flexible and efficient out-of-core region-of-interest extraction. Next, we provide an efficient parallel implementation. Finally, an interactive visualization system based on this approach is designed and then applied to turbulent combustion data. The combined approach enables an interactive spatial statistical analysis workflow for large-scale data with a top-down approach through multiple-levels-of-detail that links phase space statistics with spatial features.</td><td></td><td>VIS Full Paper</td><td>(Volume) Rendering</td><td></td><td><a href="https://youtu.be/-HMcxq7x_eQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-full-1496.html">link</a></td></tr><tr><td>193</td><td>Watertight Incremental Heightfield Tessellation</td><td>Daniel Cornel; Silvana Zechmeister; Eduard Gröller; Jürgen Waser</td><td>In this paper, we propose a method for the interactive visualization of medium-scale dynamic heightfields without visual artifacts. Our data fall into a category too large to be rendered directly at full resolution, but small enough to fit into GPU memory without pre-filtering and data streaming. We present the real-world use case of unfiltered flood simulation data of such medium scale that need to be visualized in real time for scientific purposes. Our solution facilitates compute shaders to maintain a guaranteed watertight triangulation in GPU memory that approximates the interpolated heightfields with view-dependent, continuous levels of detail. In each frame, the triangulation is updated incrementally by iteratively refining the cached result of the previous frame to minimize the computational effort. In particular, we minimize the number of heightfield sampling operations to make adaptive and higher-order interpolations viable options. We impose no restriction on the number of subdivisions and the achievable level of detail to allow for extreme zoom ranges required in geospatial visualization. Our method provides a stable runtime performance and can be executed with a limited time budget. We present a comparison of our method to three state-of-the-art methods, in which our method is competitive to previous non-watertight methods in terms of runtime, while outperforming them in terms of accuracy.</td><td>Visualization techniques and methodologies, heightfield rendering, terrain rendering, level of detail, tessellation</td><td>VIS Full Paper</td><td>(Volume) Rendering</td><td></td><td><a href="https://youtu.be/gNEmkFpyswg">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-tvcg-9770381.html">link</a></td></tr><tr><td>194</td><td>Facilitating Conversational Interaction in Natural Language Interfaces for Visualization</td><td>Rishab Mitra, Arpit Narechania, Alex Endert, John Stasko</td><td>Natural language (NL) toolkits enable visualization developers, who may not have a background in natural language processing (NLP), to create natural language interfaces (NLIs) for end-users to flexibly specify and interact with visualizations. However, these toolkits currently only support one-off utterances, with minimal capability to facilitate a multi-turn dialog between the user and the system. Developing NLIs with such conversational interaction capabilities remains a challenging task, requiring implementations of low-level NLP techniques to process a new query as an intent to follow-up on an older query. We extend an existing Python-based toolkit, NL4DV, that processes an NL query about a tabular dataset and returns an analytic specification containing data attributes, analytic tasks, and relevant visualizations, modeled as a JSON object. Specifically, NL4DV now enables developers to facilitate multiple simultaneous conversations about a dataset and resolve associated ambiguities, augmenting new conversational information into the output JSON object. We demonstrate these capabilities through three examples: (1) an NLI to learn aspects of the Vega-Lite grammar, (2) a mind mapping application to create free-flowing conversations, and (3) a chatbot to answer questions and resolve ambiguities.</td><td>Natural Language Interfaces; Visualization Toolkits; Conversational Interaction</td><td>VIS Short Paper</td><td>Visualization Systems and Graph Visualization</td><td></td><td><a href="https://youtu.be/oZGeYwf5h7E">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1012.html">link</a></td></tr><tr><td>195</td><td>VegaFusion: Automatic Server-Side Scaling for Interactive Vega Visualizations</td><td>Nicolas Kruchten, Jon Mease, Dominik Moritz</td><td>The Vega grammar has been broadly adopted by a growing ecosystem of browser-based visualization tools. However, the reference Vega renderer does not scale well to large datasets (e.g., millions of rows or hundreds of megabytes) because it requires the entire dataset to be loaded into browser memory. We introduce VegaFusion, which brings automatic server-side scaling to the Vega ecosystem. VegaFusion accepts generic Vega specifications and partitions the required computation between the client and an out-of-browser, natively-compiled server-side process. Large datasets can be processed server-side to avoid loading them into the browser and to take advantage of multi-threading, more powerful server hardware and caching. We demonstrate how VegaFusion can be integrated into the existing Vega ecosystem, and show that VegaFusion greatly outperforms the reference implementation. We demonstrate these benefits with VegaFusion running on the same machine as the client as well as on a remote machine.</td><td>Human-centered computing—Visualization—Visualization systems and tools—Visualization toolkits; Human-centered computing—Visualization—Visualization application domains—Information visualization;</td><td>VIS Short Paper</td><td>Visualization Systems and Graph Visualization</td><td></td><td><a href="https://youtu.be/hPP8z94Js2w">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1044.html">link</a></td></tr><tr><td>196</td><td>Streamlining Visualization Authoring in D3 Through User-Driven Templates</td><td>Hannah K. Bako, Alisha Varma, Anuoluwapo Faboro, Mahreen Haider, Favour Nerrise, Bissaka Kenah, Leilani Battle</td><td>D3 is arguably the most popular tool for implementing web-based visualizations. Yet D3 has a steep learning curve that may hinder its adoption and continued use. To simplify the process of programming D3 visualizations, we must first understand the space of implementation practices that D3 users engage in. We present a qualitative analysis of 2500 D3 visualizations and their corresponding implementations. We find that 5 visualization types (Bar Charts, Geomaps, Line Charts, Scatterplots, and Force Directed Graphs) account for 80% of D3 visualizations found in our corpus. While implementation styles vary slightly across designs, the underlying code structure for all visualization types remains the same; presenting an opportunity for code reuse. Using our corpus of D3 examples, we synthesize reusable code templates for eight popular D3 visualization types and share them in our open source repository. Based on our results, we discuss design considerations for leveraging users’ implementation patterns to reduce visualization design effort through design templates and auto-generated code recommendations.</td><td>Human-centered computing—Visualization—Visualization systems and tools—Visualization toolkits;</td><td>VIS Short Paper</td><td>Visualization Systems and Graph Visualization</td><td></td><td><a href="https://youtu.be/EqILmHF4MjM">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1098.html">link</a></td></tr><tr><td>197</td><td>Plotly-Resampler: Effective Visual Analytics for Large Time Series</td><td>Jonas Van Der Donckt, Jeroen Van Der Donckt, Emiel Deprost, Sofie Van Hoecke</td><td>Visual analytics is arguably the most important step in getting acquainted with your data. This is especially the case for time series, as this data type is hard to describe and cannot be fully understood when using for example summary statistics. To realize effective time series visualization, four requirements have to be met; a tool should be (1) interactive, (2) scalable to millions of data points, (3) integrable in conventional data science environments, and (4) highly configurable. We observe that open source Python visualization toolkits empower data scientists in most visual analytics tasks, but lack the combination of scalability and interactivity to realize effective time series visualization. As a means to facilitate these requirements, we created Plotly-Resampler, an open source Python library. Plotly-Resampler is an add-on for Plotly&#x27;s Python bindings, enhancing line chart scalability on top of an interactive toolkit by aggregating the underlying data depending on the current graph view. Plotly-Resampler is built to be snappy, as the reactivity of a tool qualitatively affects how analysts visually explore and analyze data. A benchmark task highlights how our toolkit scales better than alternatives in terms of number of samples and time series. Additionally, Plotly-Resampler&#x27;s flexible data aggregation functionality paves the path towards researching novel aggregation techniques. Plotly-Resampler&#x27;s integrability, together with its configurability, convenience, and high scalability, allows to effectively analyze high-frequency data in your day-to-day Python environment.</td><td>Time series, Visual analytics, Python, Dash, Plotly, Open source</td><td>VIS Short Paper</td><td>Visualization Systems and Graph Visualization</td><td></td><td><a href="https://youtu.be/ENr8_jrW6KM">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1021.html">link</a></td></tr><tr><td>198</td><td>Explaining Website Reliability by Visualizing Hyperlink Connectivity</td><td>Seongmin Lee, Sadia Afroz, Haekyu Park, Zijie J. Wang, Omar Shaikh, Vibhor Sehgal, Ankit Peshin, Duen Horng Chau</td><td>As the information on the Internet continues growing exponentially, understanding and assessing the reliability of a website is becoming increasingly important. Misinformation has far-ranging repercussions, from sowing mistrust in media to undermining democratic elections. While some research investigates how to alert people to misinformation on the web, much less research has been conducted on explaining how websites engage in spreading false information. To fill the research gap, we present MisVis, a web-based interactive visualization tool that helps users assess a website’s reliability by understanding how it engages in spreading false information on the World Wide Web. MisVis visualizes the hyperlink connectivity of the website and summarizes key characteristics of the Twitter accounts that mention the site. A large-scale user study with 139 participants demonstrates that MisVis facilitates users to assess and understand false information on the web and node-link diagrams can be used to communicate with non-experts. MisVis is available at the public demo link: https://poloclub.github.io/MisVis.</td><td>Human-centered computing—Visualization—Visualization systems and tools—Visualization toolkits</td><td>VIS Short Paper</td><td>Visualization Systems and Graph Visualization</td><td></td><td><a href="https://youtu.be/EgANajNVbpM">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1011.html">link</a></td></tr><tr><td>199</td><td>Paths through Spatial Networks</td><td>Alex Godwin</td><td>Spatial networks present unique challenges to understanding topological structure. Each node occupies a location in physical space (e.g., longitude and latitude) and each link may either indicate a fixed and well-described path (e.g., along streets) or a logical connection only. Placing these elements in a map maintains these physical relationships while making it more difficult to identify topological features of interest such as clusters, cliques, and paths. While some systems provide coordinated representations of a spatial network, it is almost universally assumed that the network itself is the sole mechanism for movement across space. In this paper, I present a novel approach for exploring spatial networks by orienting them along a point or path in physical space that provides the guide for parameters in a force-directed layout. By specifying a path across topography, networks can be spatially filtered independently of the topology of the network. Initial case studies indicate promising results for exploring spatial networks in transportation and energy distribution.</td><td>Human-centered computing—Visualization—Visualization techniques—Graph Drawing; Human-centered computing—Visualization—Visualization application domains—Geographic visualization</td><td>VIS Short Paper</td><td>Visualization Systems and Graph Visualization</td><td></td><td><a href="https://youtu.be/r_eGbqnCgJ8">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1137.html">link</a></td></tr><tr><td>200</td><td>LineCap: Line Charts for Data Visualization Captioning Models</td><td>Anita Mahinpei, Zona Kostic, Chris Tanner</td><td>Data visualization captions help readers understand the purpose of a visualization and are crucial for individuals with visual impairments. The prevalence of poor figure captions and the successful application of deep learning approaches to image captioning motivate the use of similar techniques for automated figure captioning. However, research in this field has been stunted by the lack of suitable datasets. We introduce LineCap, a novel figure captioning dataset of 3,528 figures, and we provide insights from curating this dataset and using end-to-end deep learning models for automated figure captioning.</td><td>figure captioning, line charts, deep learning dataset</td><td>VIS Short Paper</td><td>Visualization Systems and Graph Visualization</td><td></td><td><a href="https://youtu.be/dGCXHUBa7rs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1070.html">link</a></td></tr><tr><td>201</td><td>Intentable: A Mixed-Initiative System for Intent-Based Chart Captioning</td><td>Jiwon Choi, Jaemin Jo</td><td>We present Intentable, a mixed-initiative caption authoring system that allows the author to steer an automatic caption generation process to reflect their intent, e.g., the finding that the author gained from visualization and thus wants to write a caption for. We first derive a grammar for specifying the intent, i.e., a caption recipe, and build a neural network that generates caption sentences given a recipe. Our quantitative evaluation revealed that our intent-based generation system not only allows the author to engage in the generation process but also produces more fluent captions than the previous end-to-end approaches without user intervention. Finally, we demonstrate the versatility of our system, such as context adaptation, unit conversion, and sentence reordering.</td><td>Human-centered computing—Visualization—Visualization systems and tools; Human-centered computing—Human-computer interaction (HCI)—Interactive systems and tools</td><td>VIS Short Paper</td><td>Visualization Systems and Graph Visualization</td><td></td><td><a href="https://youtu.be/fXhXB6vXFUQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1048.html">link</a></td></tr><tr><td>202</td><td>Visual Auditor: Interactive Visualization for Detection and Summarization of Model Biases</td><td>David Munechika, Zijie J. Wang, Jack Reidy, Josh Rubin, Krishna Gade, Krishnaram Kenthapadi, Duen Horng Chau</td><td>As machine learning (ML) systems become increasingly widespread, it is necessary to audit these systems for biases prior to their deployment. Recent research has developed algorithms for effectively identifying intersectional bias in the form of interpretable, underperforming subsets (or slices) of the data. However, these solutions and their insights are limited without a tool for visually understanding and interacting with the results of these algorithms. We propose Visual Auditor, an interactive visualization tool for auditing and summarizing model biases. Visual Auditor assists model validation by providing an interpretable overview of intersectional bias (bias that is present when examining populations defined by multiple features), details about relationships between problematic data slices, and a comparison between underperforming and overperforming data slices in a model. Our open-source tool runs directly in both computational notebooks and web browsers, making model auditing accessible and easily integrated into current ML development workflows. An observational user study in collaboration with domain experts at Fiddler AI highlights that our tool can help ML practitioners identify and understand model biases.</td><td>Machine Learning, Statistics, Modelling, and Simulation Applications</td><td>VIS Short Paper</td><td>Visual Analytics, Decision Support, and Machine Learning</td><td></td><td><a href="https://youtu.be/8u-6v6Ja1jo">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1004.html">link</a></td></tr><tr><td>203</td><td>RMExplorer: A Visual Analytics Approach to Explore the Performance and the Fairness of Disease Risk Models on Population Subgroups</td><td>Bum Chul Kwon, Uri Kartoun, Shaan Khurshid, Mikhail Yurochkin, Subha Maity, Deanna G Brockman, Amit V Khera, Patrick T Ellinor, Steven A Lubitz, Kenney Ng</td><td>Disease risk models can identify high-risk patients and help clinicians provide more personalized care. However, risk models developed on one dataset may not generalize across diverse subpopulations of patients in different datasets and may have unexpected performance. It is challenging for clinical researchers to inspect risk models across different subgroups without any tools. Therefore, we developed an interactive visualization system called RMExplorer (Risk Model Explorer) to enable interactive risk model assessment. Specifically, the system allows users to define subgroups of patients by selecting clinical, demographic, or other characteristics, to explore the performance and fairness of risk models on the subgroups, and to understand the feature contributions to risk scores. To demonstrate the usefulness of the tool, we conduct a case study, where we use RMExplorer to explore three atrial fibrillation risk models by applying them to the UK Biobank dataset of 445,329 individuals. RMExplorer can help researchers to evaluate the performance and biases of risk models on subpopulations of interest in their data.</td><td>visual analytics, health informatics, fairness, subgroup analysis, explainability, interpretability, electronic health records</td><td>VIS Short Paper</td><td>Visual Analytics, Decision Support, and Machine Learning</td><td></td><td><a href="https://youtu.be/L-AXNZxIzIM">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1076.html">link</a></td></tr><tr><td>204</td><td>Visualizing Rule-based Classifiers for Clinical Risk Prognosis</td><td>Dario Antweiler, Georg Fuchs</td><td>Deteriorating conditions in hospital patients are a major factor in clinical patient mortality. Currently, timely detection is based on clinical experience, expertise, and attention. However, healthcare trends towards larger patient cohorts, more data, and the desire for better and more personalized care are pushing the existing, simple scoring systems to their limits. Data-driven approaches can extract decision rules from available medical coding data, which offer good interpretability and thus are key for successful adoption in practice. Before deployment, models need to be scrutinized by domain experts to identify errors and check them against existing medical knowledge. We propose a visual analytics system to support healthcare professionals in inspecting and enhancing rule-based classifier through identification of similarities and contradictions, as well as modification of rules. This work was developed iteratively in close collaboration with medical professionals. We discuss how our tool supports the inspection and assessment of rule-based classifiers in the clinical coding domain and propose possible extensions.</td><td>Information systems applications, Decision support systems, Data analytics, Human computer interaction (HCI), HCI design and evaluation methods, User studies, Applied computing, Life and medical sciences, Health care information systems</td><td>VIS Short Paper</td><td>Visual Analytics, Decision Support, and Machine Learning</td><td></td><td><a href="https://youtu.be/IM_3lzcFmW0">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1105.html">link</a></td></tr><tr><td>205</td><td>TimberTrek: Exploring and Curating Sparse Decision Trees with Interactive Visualization</td><td>Zijie J. Wang, Chudi Zhong, Rui Xin, Takuya Takagi, Zhi Chen, Duen Horng Chau, Cynthia Rudin, Margo Seltzer</td><td>Given thousands of equally accurate machine learning (ML) models, how can users choose among them? A recent ML technique enables domain experts and data scientists to generate a complete Rashomon set for sparse decision trees—a huge set of almost-optimal interpretable ML models. To help ML practitioners identify models with desirable properties from this Rashomon set, we develop TimberTrek, the first interactive visualization system that summarizes thousands of sparse decision trees at scale. Two usage scenarios highlight how TimberTrek can empower users to easily explore, compare, and curate models that align with their domain knowledge and values. Our open-source tool runs directly in users&#x27; computational notebooks and web browsers, lowering the barrier to creating more responsible ML models. TimberTrek is available at the following public demo link: https://poloclub.github.io/timbertrek.</td><td>Machine Learning, Interpretability, Rashomon Set, Decision Trees</td><td>VIS Short Paper</td><td>Visual Analytics, Decision Support, and Machine Learning</td><td></td><td><a href="https://youtu.be/q9L2MtV8em0">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1006.html">link</a></td></tr><tr><td>206</td><td>FairFuse: Interactive Visual Support for Fair Consensus Ranking</td><td>Hilson Shrestha, Kathleen Cachel, Mallak Alkhathlan, Elke A Rundensteiner, Lane Harrison</td><td>Fair consensus building combines the preferences of multiple rankers into a single consensus ranking, while ensuring any ranked candidate group defined by a protected attribute (such as race or gender) is not disadvantaged compared to other groups. Manually generating a fair consensus ranking is time-consuming and impractical— even for a fairly small number of candidates. While algorithmic approaches for auditing and generating fair consensus rankings have been developed recently, these have not been operationalized in interactive systems. To bridge this gap, we introduce FairFuse, a visualization-enabled tool for generating, analyzing, and auditing fair consensus rankings. In developing FairFuse, we construct a data model which includes several base rankings entered by rankers, augmented with measures of group fairness, algorithms for generating consensus rankings with varying degrees of fairness, and other fairness and rank-related capabilities. We design novel visualizations that encode these measures in a parallel-coordinates style rank visualization, with interactive capabilities for generating and exploring fair consensus rankings. We provide case studies in which FairFuse supports a decision-maker in ranking scenarios in which fairness is important. Finally, we discuss emerging challenges for future efforts supporting fairness-oriented rank analysis; including handling intersectionality, defined by multiple protected attributes, and the need for user studies targeting peoples’ perceptions and use of fairness oriented visualization systems. Code and demo videos available at https://osf.io/hd639/.</td><td>Fairness, consensus, rank aggregation, visualization</td><td>VIS Short Paper</td><td>Visual Analytics, Decision Support, and Machine Learning</td><td></td><td><a href="https://youtu.be/65QRCOTz-x0">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1083.html">link</a></td></tr><tr><td>207</td><td>Guided Data Discovery in Interactive Visualizations via Active Search</td><td>Shayan Monadjemi, Sunwoo Ha, Quan Nguyen, Henry Chai, Roman Garnett, Alvitta Ottley</td><td>Recent advances in visual analytics have enabled us to learn from user interactions and uncover analytic goals. These innovations set the foundation for actively guiding users during data exploration. Providing such guidance will become more critical as datasets grow in size and complexity, precluding exhaustive investigation. Meanwhile, the machine learning community also struggles with datasets growing in size and complexity, precluding exhaustive labeling. Active learning is a broad family of algorithms developed for actively guiding models during training. We will consider the intersection of these analogous research thrusts. First, we discuss the nuances of matching the choice of an active learning algorithm to the task at hand. This is critical for performance, a fact we demonstrate in a simulation study. We then present results of a user study for the particular task of data discovery guided by an active learning algorithm specifically designed for this task.</td><td>visual analytics, empirical studies in visualization, active learning settings</td><td>VIS Short Paper</td><td>Visual Analytics, Decision Support, and Machine Learning</td><td></td><td><a href="https://youtu.be/6Q92qgwJ9o4">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1041.html">link</a></td></tr><tr><td>208</td><td>Parametric Dimension Reduction by Preserving Local Structure</td><td>Chien-Hsun Lai, Ming-Feng Kuo, Yun-Hsuan Lien, Kuan-An Su, Yu-Shuen Wang</td><td>We extend a well-known dimension reduction method, t-distributed stochastic neighbor embedding (t-SNE), from non-parametric to parametric by training neural networks. The main advantage of a parametric technique is the generalization of handling new data, which is beneficial for streaming data visualization. While previous parametric methods either require a network pre-training by the restricted Boltzmann machine or intermediate results obtained from the traditional non-parametric t-SNE, we found that recent network training skills can enable a direct optimization for the t-SNE objective function. Accordingly, our method achieves high embedding quality while enjoying generalization. Due to mini-batch network training, our parametric dimension reduction method is highly efficient. For evaluation, we compared our method to several baselines on a variety of datasets. Experiment results demonstrate the feasibility of our method. The source code is available at https://github.com/a07458666/parametric_dr.</td><td>Computing methodologies—Dimensionality reduction and manifold learning—; Human-centered computing—Visualization toolkit</td><td>VIS Short Paper</td><td>Visual Analytics, Decision Support, and Machine Learning</td><td></td><td><a href="https://youtu.be/_LnMwXdeP_M">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1028.html">link</a></td></tr><tr><td>209</td><td>Uniform Manifold Approximation with Two-phase Optimization</td><td>Hyeon Jeon, Hyung-Kwon Ko, Soohyun Lee, Jaemin Jo, Jinwook Seo</td><td>We introduce Uniform Manifold Approximation with Two-phase Optimization (UMATO), a dimensionality reduction (DR) technique that improves UMAP to capture the global structure of high-dimensional data more accurately. In UMATO, optimization is divided into two phases so that the resulting embeddings can depict the global structure reliably while preserving the local structure with sufficient accuracy. In the first phase, hub points are identified and projected to construct a skeletal layout for the global structure. In the second phase, the remaining points are added to the embedding preserving the regional characteristics of local areas. Through quantitative experiments, we found that UMATO (1) outperformed widely used DR techniques in preserving the global structure while (2) producing competitive accuracy in representing the local structure. We also verified that UMATO is preferable in terms of robustness over diverse initialization methods, numbers of epochs, and subsampling techniques.</td><td>Human-centered computing—Visualization—Visualization techniques; Computing methodologies—Machine learning—Machine learning algorithms</td><td>VIS Short Paper</td><td>Visual Analytics, Decision Support, and Machine Learning</td><td></td><td><a href="https://youtu.be/xtGVKlQxW98">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1047.html">link</a></td></tr><tr><td>210</td><td>Color Coding of Large Value Ranges Applied to Meteorological Data</td><td>Daniel Braun, Kerstin Ebell, Vera Schemann, Laura Pelchmann, Susanne Crewell, Rita Borgo, Tatiana von Landesberger</td><td>This paper presents a novel color scheme designed to address the challenge of visualizing data series with large value ranges, where scale transformation provides limited support. We focus on meteorological data, where the presence of large value ranges is common. We apply our approach to meteorological scatterplots, as one of the most common plots used in this domain area. Our approach leverages the numerical representation of mantissa and exponent of the values to guide the design of novel ``nested&#x27;&#x27; color schemes, able to emphasize differences between magnitudes. Our user study evaluates the new designs, the state of the art color scales and representative color schemes used in the analysis of meteorological data: ColorCrafter, Viridis, and Rainbow. We assess accuracy, time and confidence in the context of discrimination (comparison) and interpretation (reading) tasks. Our proposed color scheme significantly outperforms the others in interpretation tasks, while showing comparable performances in discrimination tasks.</td><td>Color Coding—Perception—Large Value Ranges—User study</td><td>VIS Short Paper</td><td>Scientific Visualization, Ensembles, and Accessibility</td><td></td><td><a href="https://youtu.be/IGButD8sOLI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1072.html">link</a></td></tr><tr><td>211</td><td>Volume Puzzle: visual analysis of segmented volume data with multivariate attributes</td><td>Marco Agus, Amal Aboulhassan, Khaled Ahmed Lutf Al-Thelaya, Giovanni Pintore, Enrico Gobbetti, Corrado Cali&#x27;, Jens Schneider</td><td>A variety of application domains, including material science, neuroscience, and connectomics, commonly use segmented volume data for explorative visual analysis. In many cases, segmented objects are characterized by multivariate attributes expressing specific geometric or physical features. Objects with similar characteristics, determined by selected attribute configurations, can create peculiar spatial patterns, whose detection and study is of fundamental importance. This task is notoriously difficult, especially when the number of attributes per segment is large. In this work, we propose an interactive framework that combines a state-of-the-art direct volume renderer for categorical volumes with techniques for the analysis of the attribute space and for the automatic creation of 2D transfer function. We show, in particular, how dimensionality reduction, kernel-density estimation, and topological techniques such as Morse analysis combined with scatter and density plots allow the efficient design of two-dimensional color maps that highlight spatial patterns. The capabilities of our framework are demonstrated on synthetic and real-world data from several domains.</td><td>Segmented Volumes, Multivariate data, Color mapping, Dimensionality reduction</td><td>VIS Short Paper</td><td>Scientific Visualization, Ensembles, and Accessibility</td><td></td><td><a href="https://youtu.be/Tk_lohpqIcI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1085.html">link</a></td></tr><tr><td>212</td><td>Droplet-Local Line Integration for Multiphase Flow</td><td>Alexander Straub, Sebastian Boblest, Grzegorz Karch, Filip Sadlo, Thomas Ertl</td><td>Line integration of stream-, streak-, and pathlines is widely used and popular for visualizing single-phase flow. In multiphase flow, i.e., where the fluid consists, e.g., of a liquid and a gaseous phase, these techniques could also provide valuable insights into the internal flow of droplets and ligaments, and thus into their dynamics. However, since such structures tend to act as entities, high translational and rotational velocities often obfuscate their detail. As a remedy, we present a method for deriving a droplet-local velocity field, using a decomposition of the original velocity field removing translational and rotational velocity parts, and adapt path- and streaklines. Generally, the resulting integral lines are thus shorter and less tangled, which simplifies their analysis. We demonstrate and discuss the utility of our approach on droplets in two-phase flow data, and visualize the removed velocity parts employing glyphs for context.</td><td>Human-centered computing—Visualization—Visualization application domains—Scientific visualization;</td><td>VIS Short Paper</td><td>Scientific Visualization, Ensembles, and Accessibility</td><td></td><td><a href="https://youtu.be/DtoIuR-rrgs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1075.html">link</a></td></tr><tr><td>213</td><td>Efficient Interpolation-based Pathline Tracing with B-spline Curves in Particle Dataset</td><td>Haoyu Li, Tianyu Xiong, Han-Wei Shen</td><td>Particle tracing through numerical integration is a well-known approach to generating pathlines for visualization. However, for particle simulations, the computation of pathlines is expensive, since the interpolation method is complicated due to the lack of connectivity information. Previous studies utilize the $k$-d tree to reduce the time for neighborhood search. However, the efficiency is still limited by the number of tracing time steps. Therefore, we propose a novel interpolation-based particle tracing method that first represents particle data as B-spline curves and interpolates B-spline control points to reduce the number of interpolation time steps. We demonstrate our approach achieves good tracing accuracy with much less computation time.</td><td>Particle Tracing, Pathlines, flow visualization, B-spline</td><td>VIS Short Paper</td><td>Scientific Visualization, Ensembles, and Accessibility</td><td></td><td><a href="https://youtu.be/10zxkm0MO1I">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1037.html">link</a></td></tr><tr><td>214</td><td>Visualizing Confidence Intervals for Critical Point Probabilities in 2D Scalar Field Ensembles</td><td>Dominik Vietinghoff, Michael Böttinger, Gerik Scheuermann, Christian Heine</td><td>An important task in visualization is the extraction and highlighting of dominant features in data to support users in their analysis process. Topological methods are a well-known means of identifying such features in deterministic fields. However, many real-world phenomena studied today are the result of a chaotic system that cannot be fully described by a single simulation. Instead, the variability of such systems is usually captured with ensemble simulations that produce a variety of possible outcomes of the simulated process. The topological analysis of such ensemble data sets and uncertain data, in general, is less well studied. In this work, we present an approach for the computation and visual representation of confidence intervals for the occurrence probabilities of critical points in ensemble data sets. We demonstrate the added value of our approach over existing methods for critical point prediction in uncertain data on a synthetic data set and show its applicability to a data set from climate research.</td><td>Uncertainty visualization, scalar topology, critical points, ensemble data, climate data, inferential statistics, glyphs.</td><td>VIS Short Paper</td><td>Scientific Visualization, Ensembles, and Accessibility</td><td></td><td><a href="https://youtu.be/aw5Ol5LHTcQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1081.html">link</a></td></tr><tr><td>215</td><td>ASEVis: Visual Exploration of Active System Ensembles to Define Characteristic Measures</td><td>Marina Evers, Raphael Wittkowski, Lars Linsen</td><td>Simulation ensembles are a common tool in physics for understanding how a model outcome depends on input parameters. We analyze an active particle system, where each particle can use energy from its surroundings to propel itself. A multi-dimensional feature vector containing all particles&#x27; motion information can describe the whole system at each time step. The system&#x27;s behavior strongly depends on input parameters like the propulsion mechanism of the particles. To understand how the time-varying behavior depends on the input parameters, it is necessary to introduce new measures to quantify the difference of the dynamics of the ensemble members. We propose a tool that supports the interactive visual analysis of time-varying feature-vector ensembles. A core component of our tool allows for the interactive definition and refinement of new measures that can then be used to understand the system&#x27;s behavior and compare the ensemble members. Different visualizations support the user in finding a characteristic measure for the system. By visualizing the user-defined measure, the user can then investigate the parameter dependencies and gain insights into the relationship between input parameters and simulation output.</td><td>Physical &amp; Environmental Sciences, Engineering, Mathematics ; Comparison and Similarity ; Coordinated and Multiple Views ; Application Motivated Visualization ; Task Abstractions &amp; Application Domains ; Temporal Data</td><td>VIS Short Paper</td><td>Scientific Visualization, Ensembles, and Accessibility</td><td></td><td><a href="https://youtu.be/RzCd25jDfNk">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1103.html">link</a></td></tr><tr><td>216</td><td>Accelerated Probabilistic Marching Cubes by Deep Learning for Time-Varying Scalar Ensembles</td><td>Mengjiao Han, Tushar M. Athawale, David Pugmire, Chris R. Johnson</td><td>Visualizing the uncertainty of ensemble simulations is challenging due to the large size and multivariate and temporal features of ensemble data sets. One popular approach to studying the uncertainty of ensembles is analyzing the positional uncertainty of the level sets. Probabilistic marching cubes is a technique that performs Monte Carlo sampling of multivariate Gaussian noise distributions for positional uncertainty visualization of level sets. However, the technique suffers from high computational time, making interactive visualization and analysis impossible to achieve. This paper introduces a deep-learning-based approach to learning the level-set uncertainty for two-dimensional ensemble data with a multivariate Gaussian noise assumption. We train the model using the first few time steps from time-varying ensemble data in our workflow. We demonstrate that our trained model accurately infers uncertainty in level sets for new time steps and is up to 170X faster than that of the original probabilistic model with serial computation and 10X faster than that of the original parallel computation.</td><td>Human-centered computing—Visualization—Visualization application domains—Scientific visualization; Computing methodologies—Machine learning—Machine learning approaches—Neural networks</td><td>VIS Short Paper</td><td>Scientific Visualization, Ensembles, and Accessibility</td><td></td><td><a href="https://youtu.be/NnARcpAmeXg">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1110.html">link</a></td></tr><tr><td>217</td><td>Beyond Visuals : Examining the Experiences of Geoscience Professionals With Vision Disabilities in Accessing Data Visualizations</td><td>Nihanth W Cherukuru, David Bailey, Tiffany Fourment, Becca Hatheway, Marika Holland, Matt Rehme</td><td>Data visualizations are ubiquitous in all disciplines and have become the primary means of analysing data and communicating insights. However, the predominant reliance on visual encoding of data continues to create accessibility barriers for people who are blind/vision impaired resulting in their under representation in Science, Technology, Engineering and Mathematics (STEM) disciplines. This research study seeks to understand the experiences of professionals who are blind/vision impaired in one such STEM discipline (geosciences) in accessing data visualizations. In-depth, semi-structured interviews with seven professionals were conducted to examine the accessibility barriers and areas for improvement to inform accessibility research pertaining to data visualizations through a socio-technical lens. A reflexive thematic analysis revealed the negative impact of visualizations in influencing their career path, lack of data exploration tools for research, barriers in accessing works of peers and mismatched pace of visualization and accessibility research. The article also includes recommendations from the participants to address some of these accessibility barriers.</td><td>Human-centered computing—Visualization—Visualization Design and evaluation methods; Human-centered computing—Accessibility—Accessibility technologies</td><td>VIS Short Paper</td><td>Scientific Visualization, Ensembles, and Accessibility</td><td></td><td><a href="https://youtu.be/o-Af9sCAT44">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1055.html">link</a></td></tr><tr><td>218</td><td>Let&#x27;s Get Personal: Exploring the Design of Personalized Visualizations</td><td>Beleicia Bullock, Shunan Guo, Eunyee Koh, Ryan Rossi, Fan Du, Jane Hoffswell</td><td>Media outlets often publish visualizations that can be personalized based on users’ demographics, such as location, race, and age. However, the design of such personalized visualizations remains underexplored. In this work, we contribute a design space analysis of 47 public-facing articles with personalized visualizations to understand how designers structure content, encourage exploration, and present insights. We find that articles often lack explicit exploration suggestions or instructions, data notices, and personalized visual insights. We then outline three trajectories for future research: (1) explore how users choose to personalize visualizations, (2) examine how exploration suggestions and examples impact user interaction, and (3) investigate how personalization influences user insights.</td><td>Human-centered computing—Visualization—Visualization application domains</td><td>VIS Short Paper</td><td>Personal Visualization, Theory, Evaluation, and eXtended Reality</td><td></td><td><a href="https://youtu.be/-khEBdPbJPQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1064.html">link</a></td></tr><tr><td>219</td><td>Who benefits from Visualization Adaptations? Towards a better Understanding of the Influence of Visualization Literacy</td><td>Marc Satkowski, Franziska Kessler, Susanne Narciss, Raimund Dachselt</td><td>The ability to read, understand, and comprehend visual information representations is subsumed under the term visualization literacy (VL). One possibility to improve the use of information visualizations is to introduce adaptations. However, it is yet unclear whether people with different VL benefit from adaptations to the same degree. We conducted an online experiment (n = 42) to investigate whether the effect of an adaptation (here: De-Emphasis) of visualizations (bar charts, scatter plots) on performance (accuracy, time) and user experiences depends on users’ VL level. Using linear mixed models for the analyses, we found a positive impact of the De-Emphasis adaptation across all conditions, as well as an interaction effect of adaptation and VL on the task completion time for bar charts. This work contributes to a better understanding of the intertwined relationship of VL and visual adaptations and motivates future research.</td><td>User Study; Visualization Adaptation; Visualization Literacy; Visualization Competence; Information Visualization; Online Survey; User Experience</td><td>VIS Short Paper</td><td>Personal Visualization, Theory, Evaluation, and eXtended Reality</td><td></td><td><a href="https://youtu.be/HqPjeNSXtkw">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1016.html">link</a></td></tr><tr><td>220</td><td>VisQuiz: Exploring Feedback Mechanisms to Improve Graphical Perception</td><td>Ryan Birchfield, Maddison Caten, Errica Cheng, Madyson Kelly, Truman Larson, Hoang Phan Pham, Yiren Ding, Noëlle Rakotondravony, Lane Harrison</td><td>Graphical perception studies are a key element of visualization research, forming the basis of design recommendations and contributing to our understanding of how people make sense of visualizations. However, graphical perception studies typically include only brief training sessions, and the impact of longer and more in-depth feedback remains unclear. In this paper, we explore the design and evaluation of feedback for graphical perception tasks, called VisQuiz. Using a quiz-like metaphor, we design feedback for a typical visualization comparison experiment, showing participants their answer alongside the correct answer in an animated sequence in each trial. We extend this quiz metaphor to include summary feedback after each stage of the experiment, providing additional moments for participants to reflect on their performance. To evaluate VisQuiz, we conduct a between-subjects experiment, including three stages of 40 trials each with a control condition that included only summary feedback. Results from n = 80 participants show that once participants started receiving trial feedback (Stage 2) they performed significantly better with bubble charts than those in the control condition. This effect carried over when feedback was removed (Stage 3). Results also suggest an overall trend of improved performance due to feedback. We discuss these findings in the context of other visualization literacy efforts, and possible future work at the intersection of visualization, feedback, and learning. Experiment data and analysis scripts are available at the following repository https://osf.io/jys5d/</td><td>Visualization, Graphical Perception, Feedback</td><td>VIS Short Paper</td><td>Personal Visualization, Theory, Evaluation, and eXtended Reality</td><td></td><td><a href="https://youtu.be/WUneq9yAfUI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1141.html">link</a></td></tr><tr><td>221</td><td>OSCAR: A Semantic-based Data Binning Approach</td><td>Vidya Setlur, Michael Correll, Sarah Battersby</td><td>Binning is applied to categorize data values or to see distributions of data. Existing binning algorithms often rely on statistical properties of data. However, there are semantic considerations for selecting appropriate binning schemes. Surveys, for instance, gather respondent data for demographic-related questions such as age, salary, number of employees, etc., that are bucketed into defined semantic categories. In this paper, we leverage common semantic categories from survey data and Tableau Public visualizations to identify a set of semantic binning categories. We employ these semantic binning categories in OSCAR: a method for automatically selecting bins based on the inferred semantic type of the field. We conducted a crowdsourced study with 120 participants to better understand user preferences for bins generated by OSCAR vs. binning provided in Tableau. We find that maps and histograms using binned values generated by OSCAR are preferred by users as compared to binning schemes based purely on the statistical properties of the data.</td><td>Data-driven semantics, binning, constraints, geospatial</td><td>VIS Short Paper</td><td>Personal Visualization, Theory, Evaluation, and eXtended Reality</td><td></td><td><a href="https://youtu.be/Ja5I6weKww8">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1100.html">link</a></td></tr><tr><td>222</td><td>Towards Systematic Design Considerations of Organizing Multiple Views</td><td>Abdul Rahman Shaikh, David Koop, Hamed Alhoori, Maoyuan Sun</td><td>Multiple-view visualization (MV) has been used for visual analytics in various fields (e.g., bioinformatics, cybersecurity, and intelligence analysis). Because each view encodes data from a particular perspective, analysts often use a set of views laid out in 2D space to link and synthesize information. The difficulty of this process is impacted by the spatial organization of these views. For instance, connecting information from views far from each other can be more challenging than neighboring ones. However, most visual analysis tools currently either fix the positions of the views or completely delegate this organization of views to users (who must manually drag and move views). This either limits user involvement in managing the layout of MV or is overly flexible without much guidance. Then, a key design challenge in MV layout is determining the factors in a spatial organization that impact understanding. To address this, we review a set of MV-based systems and identify considerations for MV layout rooted in two key concerns: perception, which considers how users perceive view relationships, and content, which considers the relationships in the data. We show how these allow us to study and analyze the design of MV layout systematically.</td><td>Multiple views, visual analytics, spatial layout</td><td>VIS Short Paper</td><td>Personal Visualization, Theory, Evaluation, and eXtended Reality</td><td></td><td><a href="https://youtu.be/rWfIVfre_hE">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1143.html">link</a></td></tr><tr><td>223</td><td>Toward Systematic Considerations of Missingness in Visual Analytics</td><td>Maoyuan Sun, Yue Ma, Yuanxin Wang, Tianyi Li, Jian Zhao, Yujun Liu, Ping-Shou Zhong</td><td>Data-driven decision making has been a common task in today’s big data era, from simple choices such as finding a fast way to drive home, to complex decisions on medical treatment. It is often supported by visual analytics. For various reasons (e.g., system failure, interrupted network, intentional information hiding, or bias), visual analytics for sensemaking of data involves missingness (e.g., data loss and incomplete analysis), which impacts human decisions. For example, missing data can cost a business millions of dollars, and failing to recognize key evidence can put an innocent person in jail. Being aware of missingness is critical to avoid such catastrophes. To fulfill this, as an initial step, we consider missingness in visual analytics from two aspects: data-centric and human-centric. The former emphasizes missingness in three data-related categories: data composition, data relationship, and data usage. The latter focuses on the human-perceived missingness at three levels: observed-level, inferred-level, and ignored-level. Based on them, we discuss possible roles of visualizations for handling missingness, and conclude our discussion with future research opportunities.</td><td>Missingness, missing data visualization, sensemaking, visual analytics</td><td>VIS Short Paper</td><td>Personal Visualization, Theory, Evaluation, and eXtended Reality</td><td></td><td><a href="https://youtu.be/MYLZ9B_qMDw">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1133.html">link</a></td></tr><tr><td>224</td><td>The role of extended reality for planning coronary artery bypass graft surgery</td><td>Madhurima Vardhan, Harvey Shi, David Urick, Manesh Patel, Jane A. Leopold, Amanda Randles</td><td>Immersive visual displays are becoming more common in the diagnostic imaging and pre-procedural planning of complex cardiology revascularization surgeries. One such procedure is coronary artery bypass grafting (CABG) surgery, which is a gold standard treatment for patients with advanced coronary heart disease. Treatment planning of the CABG surgery can be aided by extended reality (XR) displays as they are known for offering advantageous visualization of spatially heterogeneous and complex tasks. Despite the benefits of XR, it remains unknown whether clinicians will benefit from higher visual immersion offered by XR. In order to assess the impact of increased immersion as well as the latent factor of geometrical complexity, a quantitative user evaluation (n=14) was performed with clinicians of advanced cardiology training simulating CABG placement on sixteen 3D arterial tree models derived from 6 patients two levels of anatomic complexity. These arterial models were rendered on 3D/XR and 2D display modes with the same tactile interaction input device. The findings of this study reveal that compared to a monoscopic 2D display, the greater visual immersion of 3D/XR does not significantly alter clinician accuracy in the task of bypass graft placement. Latent factors such as arterial complexity and clinical experience both influence the accuracy of graft placement. In addition, an anatomically less complex model and greater clinical experience improve accuracy on both 3D/XR and 2D display modes. The findings of this study can help inform design guidelines of efficient and robust XR tools for pre-procedural planning of complex coronary revascularization surgeries such as CABG.</td><td>extended reality, coronary artery bypass graft surgery, anatomic complexity, treatment planning, stereoscopic and monoscopic displays</td><td>VIS Short Paper</td><td>Personal Visualization, Theory, Evaluation, and eXtended Reality</td><td></td><td><a href="https://youtu.be/WieW-gUTpKI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1061.html">link</a></td></tr><tr><td>225</td><td>ARShopping: In-Store Shopping Decision Support Through Augmented Reality and Immersive Visualization</td><td>Bingjie Xu, Shunan Guo, Eunyee Koh, Jane Hoffswell, Ryan Rossi, Fan Du</td><td>Online shopping gives customers boundless options to choose from, backed by extensive product details and customer reviews, all from the comfort of home; yet, no amount of detailed, online information can outweigh the instant gratification and hands-on understanding of a product that is provided by physical stores. However, making purchasing decisions in physical stores can be challenging due to a large number of similar alternatives and limited accessibility of the relevant product information (e.g., features, ratings, and reviews). In this work, we present ARShopping: a web-based prototype to visually communicate detailed product information from an online setting on portable smart devices (e.g., phones, tablets, glasses), within the physical space at the point of purchase. This prototype uses augmented reality (AR) to identify products and display detailed information to help consumers make purchasing decisions that fulfill their needs while decreasing the decision-making time. In particular, we use a data fusion algorithm to improve the precision of the product detection; we then integrate AR visualizations into the scene to facilitate comparisons across multiple products and features. We designed our prototype based on interviews with 14 participants to better understand the utility and ease of use of the prototype.</td><td>Human-centered computing—Visualization—Visualization systems and tools; Human-centered computing—Human computer interaction (HCI)—Interaction paradigms—Mixed / augmented reality</td><td>VIS Short Paper</td><td>Personal Visualization, Theory, Evaluation, and eXtended Reality</td><td></td><td><a href="https://youtu.be/gFBn3mh1iuo">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-short-1092.html">link</a></td></tr><tr><td>226</td><td>What Students Learn With Personal Data Physicalization</td><td>Charles Perin</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization Teaching and Literacy (cont.) and Machine Learning for Vis.</td><td></td><td><a href="https://youtu.be/LEgNGii68Yo">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9547792.html">link</a></td></tr><tr><td>227</td><td>DeepGD: A Deep Learning Framework for Graph Drawing Using GNN</td><td>Xiaoqi Wang, Kevin Yen, Yifan Hu, Han-Wei Shen</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization Teaching and Literacy (cont.) and Machine Learning for Vis.</td><td></td><td><a href="https://youtu.be/OMvOLnMPBAg">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9476996.html">link</a></td></tr><tr><td>228</td><td>Interactive Visualization of Hyperspectral Images based on Neural Networks</td><td>Feiyu Zhu, Yu Pan, Tian Gao, Harkamal Walia, Hongfeng Yu</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization Teaching and Literacy (cont.) and Machine Learning for Vis.</td><td></td><td><a href="https://youtu.be/8EZjd-LP1ZI">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9490338.html">link</a></td></tr><tr><td>229</td><td>STSRNet: Deep Joint Space–Time Super-Resolution for Vector Field Visualization</td><td>Yifei An, Han-Wei Shen, Guihua Shan, Guan Li, Jun Liu</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization Teaching and Literacy (cont.) and Machine Learning for Vis.</td><td></td><td><a href="https://youtu.be/Wt03PjX99r8">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9488227.html">link</a></td></tr><tr><td>230</td><td>Visual Clustering Factors in Scatterplots</td><td>Jiazhi Xia, Weixing Lin, Guang Jiang, Yunhai Wang, Wei Chen, Tobias Schreck</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization Teaching and Literacy (cont.) and Machine Learning for Vis.</td><td></td><td><a href="https://youtu.be/5ml5uzTfc5s">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9495208.html">link</a></td></tr><tr><td>231</td><td>Cartolabe: A Web-Based Scalable Visualization of Large Document Collections</td><td>Philippe Caillou, Jonas Renault, Jean-Daniel Fekete, Anne-Catherine Letournel, Michèle Sebag</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization Teaching and Literacy (cont.) and Machine Learning for Vis.</td><td></td><td><a href="https://youtu.be/8eYEhPS1epA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9238399.html">link</a></td></tr><tr><td>232</td><td>A Taxonomy-Driven Model for Designing Educational Games in  Visualization</td><td>Lorenzo Amabili, Kuhu Gupta, Renata Georgia Raidou</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization Teaching and Literacy</td><td></td><td><a href="https://youtu.be/67FxzNCWa98">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9556564.html">link</a></td></tr><tr><td>233</td><td>Remote Instruction for Data Visualization Design-A Report From the Trenches</td><td>Jan Aerts, Jannes Peeters, Jelmer Bot, Danai Kafetzaki, Houda Lamqaddam</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization Teaching and Literacy</td><td></td><td><a href="https://youtu.be/i_xK5TujtMk">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9551781.html">link</a></td></tr><tr><td>234</td><td>A Didactic Framework for Analyzing Learning Activities to Design InfoVis Courses</td><td>Mandy Keck, Elena Stoll, Dietrich Kammer</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization Teaching and Literacy</td><td></td><td><a href="https://youtu.be/eykEvQot4is">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9556143.html">link</a></td></tr><tr><td>235</td><td>Through the Looking Glass: Insights Into Visualization Pedagogy Through Sentiment Analysis of Peer Review Text</td><td>Zachariah J. Beasley, Alon Friedman, Paul Rosen</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization Teaching and Literacy</td><td></td><td><a href="https://youtu.be/nMGk12jNKlQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9547773.html">link</a></td></tr><tr><td>236</td><td>Visualization Design Sprints for Online and On-Campus Courses</td><td>Johanna Beyer, Yalong Yang, Hanspeter Pfister</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization Teaching and Literacy</td><td></td><td><a href="https://youtu.be/ifegiBg5Dxk">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9547834.html">link</a></td></tr><tr><td>237</td><td>Activity Worksheets for Teaching and Learning Data Visualization</td><td>Vetria L. Byrd, Nicole Dwenger</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization Teaching and Literacy</td><td></td><td><a href="https://youtu.be/pME0jGFAPMw">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9547790.html">link</a></td></tr><tr><td>238</td><td>Surgical Navigation System for Low-Dose-Rate Brachytherapy Based on Mixed Reality</td><td>Zeyang Zhou, Zhiyong Yang, Shan Jiang, Xiaodong Ma, Fujun Zhang, Huzheng Yan</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization in Industry</td><td></td><td><a href="https://youtu.be/hIehF6wnUoE">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-8948290.html">link</a></td></tr><tr><td>239</td><td>Visualization for Architecture, Engineering, and Construction: Shaping the Future of Our Built World</td><td>Moataz Abdelaal, Felix Amtsberg, Michael Becher, Rebeca Duque Estrada, Fabian Kannenberg, Aimee Sousa Calepso, Hans Jakob Wagner, Guido Reina, Michael Sedlmair, Achim Menges, Daniel Weiskopf</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization in Industry</td><td></td><td><a href="https://youtu.be/IREcB2buRgs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9709159.html">link</a></td></tr><tr><td>240</td><td>Visual Parameter Space Analysis for Optimizing the Quality of  Industrial Nonwovens</td><td>Viny Saajan Victor, Andre Schmeiser, Heike Leitte, Simone Gramsch</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization in Industry</td><td></td><td><a href="https://youtu.be/TiNQItU6VjQ">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9726809.html">link</a></td></tr><tr><td>241</td><td>Reflections on Visualization Research Projects in the Manufacturing Industry</td><td>Lena Cibulski, Johanna Schmidt, Wolfgang Aigner</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization in Industry</td><td></td><td><a href="https://youtu.be/lOool3Nc7do">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9729397.html">link</a></td></tr><tr><td>242</td><td>Situated Visual Analysis and Live Monitoring for Manufacturing</td><td>Michael Becher, Dominik Herr, Christoph Muller, Kuno Kurzhals, Guido Reina, Lena Wagner, Thomas Ertl, Daniel Weiskopf</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization in Industry</td><td></td><td><a href="https://youtu.be/ZnAOpZk02MA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9732172.html">link</a></td></tr><tr><td>243</td><td>Stress Visualization for Interface Optimization of a Hybrid Component Using Surface Tensor Spines</td><td>Vanessa Kretzschmar, Allan Rocha, Fabian Gunther, Markus Stommel, Gerik Scheuermann</td><td></td><td></td><td>VIS Full Paper</td><td>Visualization in Industry</td><td></td><td><a href="https://youtu.be/9tpEiv3Oesg">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-cga-9709109.html">link</a></td></tr><tr><td>244</td><td>Image Features Influence Reaction Time: A Learned Probabilistic Perceptual Model for Saccade Latency</td><td>Budmonde Duinkharjav, Praneeth Chakravarthula, Rachel Brown, Anjul Patney, Qi Sun</td><td>We aim to ask and answer an essential question &quot;how quickly do we react after observing a displayed visual target?&quot; To this end, we present psychophysical studies that characterize the remarkable disconnect between human saccadic behaviors and spatial visual acuity. Building on the results of our studies, we develop a perceptual model to predict temporal gaze behavior, particularly saccadic latency, as a function of the statistics of a displayed image. Specifically, we implement a neurologically-inspired probabilistic model that mimics the accumulation of confidence that leads to a perceptual decision. We validate our model with a series of objective measurements and user studies using an eye-tracked VR display. The results demonstrate that our model prediction is in statistical alignment with real-world human behavior. Further, we establish that many sub-threshold image modifications commonly introduced in graphics pipelines may significantly alter human reaction timing, even if the differences are visually undetectable. Finally, we show that our model can serve as a metric to predict and alter reaction latency of users in interactive computer graphics applications, thus may improve gaze-contingent rendering, design of virtual experiences, and player performance in e-sports. We illustrate this with two examples: estimating competition fairness in a video game with two different team colors, and tuning display viewing distance to minimize player reaction time.</td><td>Virtual Reality, Augmented Reality, Visual Perception, Human Performance, Esports, Gaze-Contingent Rendering</td><td>VIS Full Paper</td><td>SIGGRAPH Invited Talks</td><td></td><td><a href="https://youtu.be/L52MGqO8x7I">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-siggraph-1.html">link</a></td></tr><tr><td>245</td><td>CLIPasso: Semantically Aware Object Sketching</td><td>Yael Vinker, Ehsan Pajouheshgar, Jessica Y. Bo, Roman Christian Bachmann, Amit Bermano, Daniel Cohen-Or, Amir Zamir, Ariel Shamir</td><td>Abstraction is at the heart of sketching due to the simple and minimal nature of line drawings. Abstraction entails identifying the essential visual properties of an object or scene, which requires semantic understanding and prior knowledge of high-level concepts. Abstract depictions are therefore challenging for artists, and even more so for machines. We present CLIPasso, an object sketching method that can achieve different levels of abstraction, guided by geometric and semantic simplifications. While sketch generation methods often rely on explicit sketch datasets for training, we utilize the remarkable ability of CLIP (Contrastive-Language-Image-Pretraining) to distill semantic concepts from sketches and images alike. We define a sketch as a set of Bézier curves and use a differentiable rasterizer to optimize the parameters of the curves directly with respect to a CLIP-based perceptual loss. The abstraction degree is controlled by varying the number of strokes. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual components of the subject drawn.</td><td>Sketch Synthesis, Image-based Rendering, Vector Line Art Generation</td><td>VIS Full Paper</td><td>SIGGRAPH Invited Talks</td><td></td><td><a href="https://youtu.be/eu5YTZ9cwFk">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-siggraph-2.html">link</a></td></tr><tr><td>246</td><td>Joint Neural Phase Retrieval and Compression for Energy- and Computation-efficient Holography on the Edge</td><td>Yujie Wang, Praneeth Chakravarthula, Qi Sun, Baoquan Chen</td><td>Recent deep learning approaches have shown remarkable promise to enable high fidelity holographic displays. However, lightweight wearable display devices cannot afford the computation demand and energy consumption for hologram generation due to the limited onboard compute capability and battery life. On the other hand, if the computation is conducted entirely remotely on a cloud server, transmitting lossless hologram data is not only challenging but also result in prohibitively high latency and storage. In this work, by distributing the computation and optimizing the transmission, we propose the first framework that jointly generates and compresses high-quality phase-only holograms. Specifically, our framework asymmetrically separates the hologram generation process into high-compute remote encoding (on the server), and low-compute decoding (on the edge) stages. Our encoding enables light weight latent space data, thus faster and efficient transmission to the edge device. With our framework, we observed a reduction of 76% computation and consequently 83% in energy cost on edge devices, compared to the existing hologram generation methods. Our framework is robust to transmission and decoding errors, and approach high image fidelity for as low as 2 bits-per-pixel, and further reduced average bit-rates and decoding time for holographic videos.</td><td>Computer generated holography, neural hologram generation, hologram compression</td><td>VIS Full Paper</td><td>SIGGRAPH Invited Talks</td><td></td><td><a href="https://youtu.be/VDjM5zlypSo">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-siggraph-3.html">link</a></td></tr><tr><td>247</td><td>Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces</td><td>Nicholas Sharp, Alec Jacobson</td><td>Neural implicit representations, which encode a surface as the level set of a neural network applied to spatial coordinates, have proven to be remarkably effective for optimizing, compressing, and generating 3D geometry. Although these representations are easy to fit, it is not clear how to best evaluate geometric queries on the shape, such as intersecting against a ray or finding a closest point. The predominant approach is to encourage the network to have a signed distance property. However, this property typically holds only approximately, leading to robustness issues, and holds only at the conclusion of training, inhibiting the use of queries in loss functions. Instead, this work presents a new approach to perform queries directly on general neural implicit functions for a wide range of existing architectures. Our key tool is the application of range analysis to neural networks, using automatic arithmetic rules to bound the output of a network over a region; we conduct a study of range analysis on neural networks, and identify variants of affine arithmetic which are highly effective. We use the resulting bounds to develop geometric queries including ray casting, intersection testing, constructing spatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating bulk properties, and more. Our queries can be efficiently evaluated on GPUs, and offer concrete accuracy guarantees even on randomly-initialized networks, enabling their use in training objectives and beyond. We also show a preliminary application to inverse rendering.</td><td>implicit surfaces, neural networks, range analysis, geometry processing</td><td>VIS Full Paper</td><td>SIGGRAPH Invited Talks</td><td></td><td><a href="https://youtu.be/5DY5RaJI5Ds">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-siggraph-6.html">link</a></td></tr><tr><td>248</td><td>Sketch2Pose: estimating a 3D character pose from a bitmap sketch</td><td>Kirill Brodt, Mikhail Bessmeltsev</td><td>Artists frequently capture character poses via raster sketches, then use these drawings as a reference while posing a 3D character in a specialized 3D software --- a time-consuming process, requiring specialized 3D training and mental effort. We tackle this challenge by proposing the first system for automatically inferring a 3D character pose from a single bitmap sketch, producing poses consistent with viewer expectations. Algorithmically interpreting bitmap sketches is challenging, as they contain significantly distorted proportions and foreshortening. We address this by predicting three key elements of a drawing, necessary to disambiguate the drawn poses: 2D bone tangents, self-contacts, and bone foreshortening. These elements are then leveraged in an optimization inferring the 3D character pose consistent with the artist&#x27;s intent. Our optimization balances cues derived from artistic literature and perception research to compensate for distorted character proportions. We demonstrate a gallery of results on sketches of numerous styles. We validate our method via numerical evaluations, user studies, and comparisons to manually posed characters and previous work. Code and data for our paper are available at http://www-labs.iro.umontreal.ca/bmpix/sketch2pose/.</td><td>Character posing, rigged and skinned characters, sketch-based posing, character sketches</td><td>VIS Full Paper</td><td>SIGGRAPH Invited Talks</td><td></td><td><a href="https://youtu.be/xIj9PFjvTg8">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-siggraph-5.html">link</a></td></tr><tr><td>249</td><td>Umbrella Meshes: Volumetric Elastic Mechanisms for Freeform Shape Deployment</td><td>Yingying Ren, Uday Kusupati, Julian Panetta, Florin Isvoranu, Davide Pellis, Tian Chen, Mark Pauly</td><td>We present a computational inverse design framework for a new class of volumetric deployable structures that have compact rest states and deploy into bending-active 3D target surfaces. Umbrella meshes consist of elastic beams, rigid plates, and hinge joints that can be directly printed or assembled in a zero-energy fabrication state. During deployment, as the elastic beams of varying heights rotate from vertical to horizontal configurations, the entire structure transforms from a compact block into a target curved surface. Umbrella Meshes encode both intrinsic and extrinsic curvature of the target surface and in principle are free from the area expansion ratio bounds of past auxetic material systems.  We build a reduced physics-based simulation framework to accurately and efficiently model the complex interaction between the elastically deforming components. To determine the mesh topology and optimal shape parameters for approximating a given target surface, we propose an inverse design optimization algorithm initialized with conformal flattening. Our algorithm minimizes the structure&#x27;s strain energy in its deployed state and optimizes actuation forces so that the final deployed structure is in stable equilibrium close to the desired surface with few or no external constraints. We validate our approach by fabricating a series of physical models at various scales using different manufacturing techniques.</td><td>Deployable structure, physics-based simulation, numerical optimization, computational design, fabrication</td><td>VIS Full Paper</td><td>SIGGRAPH Invited Talks</td><td></td><td><a href="https://youtu.be/m-sz2RyRpKs">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-siggraph-4.html">link</a></td></tr><tr><td>250</td><td>Breaking Plausibility Without Breaking Presence - Evidence For The Multi-Layer Nature Of Plausibility</td><td>Larissa Brübach, Franziska Westermeier, Carolin Wienrich, Marc Erich Latoschik</td><td>A novel theoretical model recently introduced coherence and plausibility as the essential conditions of XR experiences, challenging contemporary presence-oriented concepts. This article reports on two experiments validating this model, which assumes coherence activation on three layers (cognition, perception, and sensation) as the potential sources leading to a condition of plausibility and from there to other XR qualia such as presence or body ownership. The experiments introduce and utilize breaks in plausibility (in analogy to breaks in presence): We induce incoherence on the perceptual and the cognitive layer simultaneously by a simulation of object behaviors that do not conform to the laws of physics, i.e., gravity. We show that this manipulation breaks plausibility and hence confirm that it results in the desired effects in the theorized condition space but that the breaks in plausibility did not affect presence. In addition, we show that a cognitive manipulation by a storyline framing is too weak to successfully counteract the strong bottom-up inconsistencies. Both results are in line with the predictions of the recently introduced three-layer model of coherence and plausibility, which incorporates well-known top-down and bottom-up rivalries and its theorized increased independence between plausibility and presence.</td><td>A novel theoretical model recently introduced coherence and plausibility as the essential conditions of XR experiences, challenging contemporary presence-oriented concepts. This article reports on two experiments validating this model, which assumes coherence activation on three layers (cognition, perception, and sensation) as the potential sources leading to a condition of plausibility and from there to other XR qualia such as presence or body ownership. The experiments introduce and utilize breaks in plausibility (in analogy to breaks in presence): We induce incoherence on the perceptual and the cognitive layer simultaneously by a simulation of object behaviors that do not conform to the laws of physics, i.e., gravity. We show that this manipulation breaks plausibility and hence confirm that it results in the desired effects in the theorized condition space but that the breaks in plausibility did not affect presence. In addition, we show that a cognitive manipulation by a storyline framing is too weak to successfully counteract the strong bottom-up inconsistencies. Both results are in line with the predictions of the recently introduced three-layer model of coherence and plausibility, which incorporates well-known top-down and bottom-up rivalries and its theorized increased independence between plausibility and presence.</td><td>VIS Full Paper</td><td>VR Invited Talks</td><td></td><td><a href="https://youtu.be/g8yVgAzt5TY">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-vr-9714117.html">link</a></td></tr><tr><td>251</td><td>Combining Real-World Constraints on User Behavior with Deep Neural Networks for Virtual Reality (VR) Biometrics</td><td>Robert Miller, Natasha Kholgade Banerjee, Sean Banerjee</td><td>Deep networks have demonstrated enormous potential for identification and authentication using behavioral biometrics in virtual reality (VR). However, existing VR behavioral biometrics datasets have small sample sizes which can make it challenging for deep networks to automatically learn features that characterize real-world user behavior and that may enable high success, e.g., high-level spatial relationships between headset and hand controller devices and underlying smoothness of trajectories despite noise. We provide an approach to perform behavioral biometrics using deep networks while incorporating spatial and smoothing constraints on input data to represent real-world behavior. We represent the input data to neural networks as a combination of scale- and translation-invariant device-centric position and orientation features, and displacement vectors representing spatial relationships between device pairs. We assess identification and authentication by including spatial relationships and by performing Gaussian smoothing of the position features. We evaluate our approach against baseline methods that use the raw data directly and that perform a global normalization of the data. By using displacement vectors, our work shows higher success over baseline methods in 36 out of 42 cases of analysis done by varying user sets and pairings of VR systems and sessions.</td><td>Virtual reality; Biometrics</td><td>VIS Full Paper</td><td>VR Invited Talks</td><td></td><td><a href="https://youtu.be/SlJF2kt-PbU">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-vr-9756791.html">link</a></td></tr><tr><td>252</td><td>Real-Time Gaze Tracking with Event-Driven Eye Segmentation</td><td>Yu Feng, Nathan Goulding-Hotta, Asif Khan, Hans Reyserhove, Yuhao Zhu</td><td>Gaze tracking is increasingly becoming an essential component in Augmented and Virtual Reality. Modern gaze tracking algorithms are heavyweight; they operate at most 5 Hz on mobile processors despite that near-eye cameras comfortably operate at a real-time rate (&gt; 30 Hz). This paper presents a real-time eye tracking algorithm that, on average, operates at 30 Hz on a mobile processor, achieves 0.1°–0.5° gaze accuracies, all the while requiring only 30K parameters, one to two orders of magnitude smaller than state-of-the-art eye tracking algorithms. The crux of our algorithm is an Auto ROI mode, which continuously predicts the Regions of Interest (ROIs) of near-eye images and judiciously processes only the ROIs for gaze estimation. To that end, we introduce a novel, lightweight ROI prediction algorithm by emulating an event camera. We discuss how a software emulation of events enables accurate ROI prediction without requiring special hardware. The code of our paper is available at https://github.com/horizon-research/edgaze.</td><td>Gaze, eye tracking, event camera, segmentation</td><td>VIS Full Paper</td><td>VR Invited Talks</td><td></td><td><a href="https://youtu.be/w8dbXGBl44c">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-vr-9756796.html">link</a></td></tr><tr><td>253</td><td>Mood-Driven Colorization of Virtual Indoor Scenes</td><td>Michael S Solah, Haikun Huang, Jiachuan Sheng, Tian Feng, Marc Pomplun, Lap-Fai Yu</td><td>One of the challenging tasks in virtual scene design for Virtual Reality (VR) is causing it to invoke a particular mood in viewers. The subjective nature of moods brings uncertainty to the purpose. We propose a novel approach to automatic adjustment of the colors of textures for objects in a virtual indoor scene, enabling it to match a target mood. A dataset of 25,000 images, including building/home interiors, was used to train a classifier with the features extracted via deep learning. It contributes to an optimization process that colorizes virtual scenes automatically according to the target mood. Our approach was tested on four different indoor scenes, and we conducted a user study demonstrating its efficacy through statistical analysis with the focus on the impact of the scenes experienced with a VR headset.</td><td>Virtual reality; Perception; Visualization design and evaluation methods</td><td>VIS Full Paper</td><td>VR Invited Talks</td><td></td><td><a href="https://youtu.be/CAl-JVTXvEk">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-vr-9714118.html">link</a></td></tr><tr><td>254</td><td>Omnidirectional Galvanic Vestibular Stimulation in Virtual Reality</td><td>Colin Groth, Jan-Philipp Tauscher, Nikkel Heesen, Max Hattenbach, Susana Castillo, Marcus Magnor</td><td>In this paper we propose omnidirectional galvanic vestibular stimulation (GVS) to mitigate cybersickness in virtual reality applications. One of the most accepted theories indicates that Cybersickness is caused by the visually induced impression of ego motion while physically remaining at rest. As a result of this sensory mismatch, people associate negative symptoms with VR and sometimes avoid the technology altogether. To reconcile the two contradicting sensory perceptions, we investigate GVS to stimulate the vestibular canals behind our ears with low-current electrical signals that are specifically attuned to the visually displayed camera motion. We describe how to calibrate and generate the appropriate GVS signals in real-time for pre-recorded omnidirectional videos exhibiting ego-motion in all three spatial directions. For validation, we conduct an experiment presenting real-world 360° videos shot from a moving first-person perspective in a VR head-mounted display. Our findings indicate that GVS is able to significantly reduce discomfort for cybersickness-susceptible VR users, creating a deeper and more enjoyable immersive experience for many people.</td><td>Galvanic Vestibular Stimulation, GVS, Virtual Reality, VR, 360 Videos, Cybersickness, Presence</td><td>VIS Full Paper</td><td>VR Invited Talks</td><td></td><td><a href="https://youtu.be/xm2y9WfQukA">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-vr-9714040.html">link</a></td></tr><tr><td>255</td><td>Comparing Direct and Indirect Methods of Audio Quality Evaluation in Virtual Reality Scenes of Varying Complexity</td><td>Thomas Robotham, Olli S. Rummukainen, Miriam Kurz, Marie Eckert, Emanuël A. P. Habets</td><td>Many quality evaluation methods are used to assess uni-modal audio or video content without considering perceptual, cognitive, and interactive aspects present in virtual reality (VR) settings. Consequently, little is known regarding the repercussions of the employed evaluation method, content, and subject behavior on the quality ratings in VR. This mixed between- and within-subjects study uses four subjective audio quality evaluation methods (viz. multiple-stimulus with and without reference for direct scaling, and rank-order elimination and pairwise comparison for indirect scaling) to investigate the contributing factors present in multi-modal 6-DoF VR on quality ratings of real-time audio rendering. For each between-subjects employed method, two sets of conditions in five VR scenes were evaluated within-subjects. The conditions targeted relevant attributes for binaural audio reproduction using scenes with various amounts of user interactivity. Our results show all referenceless methods produce similar results using both condition sets. However, rank-order elimination proved to be the fastest method, required the least amount of repetitive motion, and yielded the highest discrimination between spatial conditions. Scene complexity was found to be a main effect within results, with behavioral and task load index results implying more complex scenes and interactive aspects of 6-DoF VR can impede quality judgments.</td><td>Multi-modal, virtual reality, 6-Degrees-of-freedom, audio quality, direct scaling, indirect scaling, evaluation methods</td><td>VIS Full Paper</td><td>VR Invited Talks</td><td></td><td><a href="https://youtu.be/Ca5H76VdM14">link</a></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_v-vr-9714044.html">link</a></td></tr><tr><td>256</td><td>Analysis of the Design Space for Cybersecurity Visualizations in VizSec #3950</td><td>Adrian Komadina, Željka Mihajlović, Stjepan Groš</td><td>In this paper, we present our design study on developing an interactive visual firewall log analysis system in collaboration with an IT service provider.  We describe the human-centered design process, in which we additionally considered hedonic qualities by including the usage of personas, psychological need cards and interaction vocabulary. For the problem characterization we especially focus on the demands of the two main clusters of requirements: high-level overview and low-level analysis, represented by the two defined personas, namely information security officer and network analyst.  This resulted in the prototype of a visual analysis system consisting of two interlinked parts. One part addresses the needs for rather strategical tasks while also fulfilling the need for an appealing appearance and interaction. The other part rather addresses the requirements for operational tasks and aims to provide a high level of flexibility. We describe our design journey, the derived domain tasks and task abstractions as well as our visual design decisions, and present our final prototypes based on a usage scenario.  We also report on our capstone event, where we conducted an observed experiment and collected feedback from the information security officer. Finally, as a reflection, we propose the extension of a widely used design study process with a track for an additional focus on hedonic qualities.</td><td>Information visualization; Human and societal aspects of security and privacy</td><td>Associated Event</td><td>VizSec: Best Paper Announcement and Papers</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-vizsec-3950.html">link</a></td></tr><tr><td>257</td><td>Visualization Of Class Activation Maps To Explain AI Classification Of Network Packet Captures #7912</td><td>Igor Cherepanov, Alex Ulmer, Jonathan Geraldi Joewono, Jörn Kohlhammer</td><td>The classification of internet traffic has become increasingly important due to the rapid growth of today’s networks and application variety. The number of connections and the addition of new applications in our networks causes a vast amount of log data and complicates the search for common patterns by experts. Finding such patterns among specific classes of applications is necessary to fulfill various requirements in network analytics. Supervised deep learning methods learn features from raw data and achieve high accuracy in classification. However, these methods are very complex and are used as black-box models, which weakens the experts’ trust in these classifications. Moreover, by using them as a black-box, new knowledge cannot be obtained from the model predictions despite their excellent performance. Therefore, the explainability of the classifications is crucial. Besides increasing trust, the explanation can be used for model evaluation to gain new insights from the data and to improve the model. In this paper, we present a visual and interactive tool that combines the classification of network data with an explanation technique to form an interface between experts, algorithms, and data.</td><td>Human-centered computing, Visualization, User interface design, Interpretability, Network Classification, Convolutional Neural Networks</td><td>Associated Event</td><td>VizSec: Best Paper Announcement and Papers</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-vizsec-7912.html">link</a></td></tr><tr><td>258</td><td>Portola: A Hybrid Tree and Network Visualization Technique for Network Segmentation #7865</td><td>Kuhu Gupta, Aditeya Pandey, Larry Chan, Ambika Yadav, Brian Staats, Michelle Borkin</td><td>In this paper, we present research on the analysis of the design space for cybersecurity visualizations in VizSec. At the beginning of this research, we analyzed 17 survey papers in the field of cybersecurity visualization. Based on the analysis of the focus areas in each of these survey papers, we identified five key components of visual- ization design, i.e. Input Data, Security Tasks, Visual Encoding, Interactivity, and Evaluation. To show how research papers align with these components, we analyzed 60 papers published at the IEEE Symposium on Visualization for Cyber Security (VizSec) be- tween 2016 and 2021 in the context of the five identified components. As a result, each research paper was classified into several categories derived from the selected components of the visualization design. Our contributions are: (i) an analysis of the focus areas in survey papers on cybersecurity visualization and (ii) the classification of 60 research papers in the context of the selected components of the visualization design. Finally, we highlighted the main findings of the analysis and drew conclusions.</td><td>Visualization—Cybersecurity—Analysis—Survey—VizSec</td><td>Associated Event</td><td>VizSec: Best Paper Announcement and Papers</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-vizsec-7865.html">link</a></td></tr><tr><td>259</td><td>PRIVEE: A Visual Analytic Workflow for Proactive Privacy Risk Inspection of Open Data #3415</td><td>Kaustav Bhattacharjee, Akm Islam, Jaideep Vaidya, Aritra Dasgupta</td><td>Open data sets that contain personal information are susceptible to adversarial attacks even when anonymized. By performing low-cost joins on multiple datasets with shared attributes, malicious users of open data portals might get access to information that violates individuals&#x27; privacy. However, open data sets are primarily published using a release-and-forget model, whereby data owners and custodians have little to no cognizance of these privacy risks. We address this critical gap by developing a visual analytic solution that enables data defenders to gain awareness about the disclosure risks in local, joinable data neighborhoods. The solution is derived through a design study with data privacy researchers, where we initially play the role of a red team and engage in an ethical data hacking exercise based on privacy attack scenarios. We use this problem and domain characterization to develop a set of visual analytic interventions as a defense mechanism and realize them in PRIVEE, a visual risk inspection workflow that acts as a proactive monitor for data defenders. PRIVEE uses a combination of risk scores and associated interactive visualizations to let data defenders explore vulnerable joins and interpret risks at multiple levels of data granularity. We demonstrate how PRIVEE can help emulate the attack strategies and diagnose disclosure risks through two case studies with data privacy experts.</td><td>Human-centered computing, Visualization, Visualization application domains, Visual analytics;</td><td>Associated Event</td><td>VizSec: Best Paper Announcement and Papers</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-vizsec-3415.html">link</a></td></tr><tr><td>260</td><td>Visual Firewall Log Analysis - At the Border Between Analytical and Appealing #9019</td><td>Marija Schufrin, Hendrik Lücke-Tieke, Jörn Kohlhammer</td><td>Network security is critical for organizations to secure their network resources from intrusion and attacks. A security policy is a rule enforced in the network to allow or block network traffic. To write security policies, network analysts divide their networks into segments or parts with similar security needs. Segmentation makes writing security policies manageable and identifies more robust security policies for the network. Visualizations can help analysts to understand the segmented network and define security policies. We contribute Portola, a hybrid tree and network visualization technique to display a segmented computer network. Portola presents an overview of the segmentation as a hierarchy and displays connections within the network. Using Portola, analysts can explore a segmented network, identify nodes and connections of interest through exploratory network analysis, and drill down on elements of interest to reason about the patterns of relationships in the network. Through this work, we also discuss the goals of network analysts who work with segmented networks and discuss the lessons learned from the user-centered iterative design of Portola.</td><td>Human-centered computing, Visualization Techniques, Tree and Network Visualization</td><td>Associated Event</td><td>VizSec: Best Paper Announcement and Papers</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-vizsec-9019.html">link</a></td></tr><tr><td>261</td><td>Shifting winds: Gendered structures of academic mentorship</td><td>Jiabao Li, Houjiang Liu, Jilie Zeng, Di Wu, Ying Ding, Alec McGail</td><td>Every researcher alive today had their mentors, those who helped assimilate them into a life of scholarly work. And in turn they each had their mentors, and so on to the dawn of knowledge. In the same way, each researcher’s mentees take their perspectives and methods to future mentees, and to their mentees, etc. These comprise the roots and branches, respectively, of the academic tree of a single researcher. If we let these ancestors’ and descendants’ genders affect these trees like a “wind,” most curl nearly to the earth. We depict and describe the structure of these trees, and how this wind has changed over the decades. To set these trees growing upright again we visualize giving differential weight to male and female researchers.</td><td></td><td>Associated Event</td><td>VISAP: Papers 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1021.html">link</a></td></tr><tr><td>262</td><td>Diversity traces: an interactive lens on multi-racial families in America</td><td>Pedro Cruz</td><td>When looking at diversity from a racial perspective, homogenous communities are still the norm, as they remain siloed not only locally, but in their very own households as well. This visualization project comes as a celebration of the fringe couples and families who have a multi-racial identity, effectively embodying the intermingle of races, and dissolving the systemic barriers put on their very own existence. According to the census, there are only vestiges of these multi-racial families until 1960. More recently, there as been a surge of these families in the data, but they are still a rarity, still mere traces of diversity in America. In this visualization you can see every registered multi-racial couple in America, for recent periods in 1-5% samples of the population, and for older periods in 100% samples of the population. Each couple is represented as a colorful animated chromosome, enabling to see the races within each family, their ages, sexes, and children.</td><td></td><td>Associated Event</td><td>VISAP: Papers 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1030.html">link</a></td></tr><tr><td>263</td><td>#MeToo Anti-Network</td><td>Kim Albrecht, Catherine D&#x27;Ignazio, Matthew Battles, Nicole Martin</td><td>Cosmologists say that most of the universe is structured by antimatter. We postulate that social media is similarly structured by effects of the unobserved discourse and experience. The backbone of a movement such as #MeToo is not based on the most-liked and most-retweeted, but by the masses of unobserved tweets. Vast numbers of #MeToo tweets that had no retweets and no likes nonetheless constituted acts of quiet testimony or unassuming solidarity. Conventional measures of network science thus fail to capture the true relevance of #MeToo. As Black feminist Patricia Hill Collins says, &quot;Most activism is brought about by ordinary people like ourselves.&quot;  From a distance, the graphics appear as abstract diagrams, similar to Bridget Riley’s work. The beauty of each line contains a powerful request for a reordering of power within society. We present an opportunity to engage with each request—from individual people at individual moments within a collective movement that is not over. #MeToo is urgent, #InvisibleNoMore is urgent, #BelieveBlackWomen is urgent, #MMIWG2S is urgent, #SayHerName is urgent. We are still living in a crisis of sexual violence. So we invite you to ditch the networked metrics and listen.</td><td></td><td>Associated Event</td><td>VISAP: Papers 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1024.html">link</a></td></tr><tr><td>264</td><td>Quaran.tiles. Archiving expressive digital places from instagram during the COVID-19 pandemic</td><td>Andrea Benedetti, Beatrice Gobbo, Giacomo Flaim</td><td>During the spring of 2020, COVID-19 limited contact between people and prevented from meeting and aggregating in real places. Many had to stay at home, and others spent time in quarantine facilities. In this context, virtual aggregation has increased at the expense of in-person aggregation. Expressive geo-tagging, namely the practice of creating locations with fictitious names to express an emotional condition, became worthy of attention. Grounded on anecdotal evidence, fictitious digital locations on social media such as “Quarantine” began to proliferate, which, despite not having a name that could be traced back to an existing place, still carried geo-referenced information with them. Starting from this concept, we present the book Quaran.tiles, an archive of 364 expressive digital places collected from Instagram in April 2020 and enriched with information from Google Street View, which aims to give space and dimension to the resulting collection of fictitious and mingling user-generated places.</td><td></td><td>Associated Event</td><td>VISAP: Papers 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1041.html">link</a></td></tr><tr><td>265</td><td>The memory of street - Hong Kong: history, culture, memory and post-humanism</td><td>Wai Lam Noah Luk, Chunxi Liu</td><td>In this annotated portfolio, we will introduce a posthumanist idea of using micro-organism to define the mingling spaces. As a result, to explore and extend the current boundary of the cultural concerns of human beings. Alongside the philosophical discussion, we will also present a multidisciplinarily-fabricated installation to visualize the idea through 3D printing technology and biomedical experiments. (The geographical location of this project is based in Hong Kong).</td><td></td><td>Associated Event</td><td>VISAP: Papers 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1043.html">link</a></td></tr><tr><td>266</td><td>Tangled Tracks</td><td>Luiz Ludwig, Doris Kosminsky</td><td>The artwork Tangled Tracks is an installation that consists of a set of 16 red and white ceramic tiles with projections that register the real and virtual paths taken by the artist. As a way of exhibiting them, the tiles are available so the public can interact and rearrange them in their own way. In this manner, participants are invited to form their own paths, imagining and fabulating routes.</td><td></td><td>Associated Event</td><td>VISAP: Papers 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1019.html">link</a></td></tr><tr><td>267</td><td>ESSYS* Sharing #UC: An Emotion-driven Audiovisual Installation</td><td>Sérgio M. Rebelo, Mariana Seiça, Pedro Martins, João Bicker, Penousal Machado</td><td>We present ESSYS* Sharing #UC, an audiovisual installation artwork that reflects upon the emotional context related to the university and the city of Coimbra, based on the data shared about them on Twitter. The installation was presented in an urban art gallery of Círculo de Artes Plásticas de Coimbra during the summer and autumn of 2021. In the installation space, one may see a collection of typographic posters displaying the tweets and listening to an ever-changing ambient sound. The present audiovisuals are created by an autonomous computational creative approach, which employs a neural classifier to recognise the emotional context of a tweet and uses this resulting data as feedstock for the audiovisual generation. The installation’s space is designed to promote an approach and blend between the online and physical perceptions of the same location. We applied multiple experiments with the proposed approach to evaluate the capability and performance. Also, we conduct interview-based evaluation sessions to understand how the installation elements, especially poster designs, are experienced by people regarding diversity, expressiveness and possible employment in other commercial and social scenarios.</td><td></td><td>Associated Event</td><td>VISAP: Papers 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1027.html">link</a></td></tr><tr><td>268</td><td>Wind from Bamboo: A Chinese Handwriting Interactive Installation based on Human-AI Collaborative Font Design</td><td>Zhen Zeng, Jie Wang, Nan He</td><td>In the era of information, the feeling that the pen tip rubs against the paper is getting farther away, and the way of writing Chinese characters with strokes is gradually alienating. In response to this problem, this work tries to help people relive the touch of handwriting through the mingling of real and virtual experiences. The designer collaborated with AI to design a Chinese font that integrates bamboo leaves&#x27; shape and Chinese characters&#x27; structure. Based on the font, an interactive installation was set up to start a virtual Chinese poetry bamboo forest scene through real handwriting behavior.</td><td></td><td>Associated Event</td><td>VISAP: Papers 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1032.html">link</a></td></tr><tr><td>269</td><td>Under the Green: Visual data storytelling the process of urban CO2 neutralization by forests</td><td>Linqi Wang, Fengzhou Liang, Fang Liu, Boai Yang, Junyan Lv</td><td>We express the conflict between industrialization and ecological civilization through Cyber Aesthetics and interactive web pages. We popularize the originally cryptic knowledge of Forest Ecology to the public through common visual metaphors and interactive effects. With this work spreading online, we hope to attract more people to join the construction of ecological civilization and pay tribute to Forest Ecological Scientists. There are already some scientific research results on forest carbon fixation, and a large amount of scientific data has been generated. However, these achievements and data are highly specialized, detached from daily life, and subsequently receive rare public attention. The physical space humans depend on is strongly interconnected, and forests and cities seem separate but mingling. The production and living of people produce lots of greenhouse gases, which need to be consumed by forest plants through photosynthesis, fixing CO2 in the form of organic carbon in the soil and biomass to ensure the carbon cycle. Industrialization has led to excessive CO2 emissions, causing severe disturbances to the carbon cycle process. At the same time, nature is constantly warning humanity, accompanied by frequent occurrences of extreme weather. Therefore, natural forest conservation and plantation forest management are crucial for future ecological civilization.</td><td></td><td>Associated Event</td><td>VISAP: Papers 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1079.html">link</a></td></tr><tr><td>270</td><td>Supersynthesis: A Communal Synthesis</td><td>Amay Kataria</td><td>This pictorial presents the journey of a light and sound installation called Supersynthesis, which collects data from its users through an interactive digital interface and expresses it through the physical installation. It begins by going over historical works and methodologies that align with this project, goes over the design decisions behind the sculptural form and its software architecture, and finally analyzes its function through the lens of a ”performative object” to draw connections with the theme of Mingling Spaces.</td><td></td><td>Associated Event</td><td>VISAP: Papers 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1018.html">link</a></td></tr><tr><td>271</td><td>SkyGlyphs: Reflections on the Design of a Delightful Visualization</td><td>Bon Adriel Aseniero, Sheelagh Carpendale, George Fitzmaurice, Justin Matejka</td><td>In creating SkyGlyphs, our goal was to develop a data visualization that could possibly capture people’s attention and spark their curiosity to explore a dataset. This work was inspired by a mingling of research including serendipitous interactions, visualizations for public displays, and personal visualizations. SkyGlyphs is a nonconventional whimsical visualization, depicting datapoints as animated balloons in space. We designed it to encourage non-experts to casually browse the contents of a repository through visual interactions like linking and grouping of datapoints. Our contributions include SkyGlyphs’ representation and our design reflection that reveals a perspective on how to design delightful visualizations.</td><td></td><td>Associated Event</td><td>VISAP: Papers 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1013.html">link</a></td></tr><tr><td>272</td><td>Molecular Planets</td><td>Christoph Müller, Karsten Schatz, Florian Frieß, Michael Krone</td><td>The molecular world is always in motion – molecules are never stationary, their atoms are constantly vibrating due to thermal energies and other external forces. This ongoing motion is the reason that our exhibit is in the form of a mobile – a form of art already used by Alexander Calder, who believed that the mathematical laws of the universe could not be expressed by static art. The idea of mysterious forces holding the universe in balance inspired his mobiles. Likewise, the ever-moving elements of the molecular space are not only invisible, but their shapes are of purely theoretical nature. Visualisation makes the elegance and beauty of the molecular world visible in virtual space by representing molecular models as molecular surfaces portraying the interface between a protein and its environment. Our exhibit not only makes such visualisations transcend into our three-dimensional, tangible space, but also mingles all intermediate mathematical spaces that the idea of molecular surfaces traverse to reach their visual representation into one object. It therefore makes the visualisation process behind the idea of molecular surface maps more tangible by showing the metaphor of a planet being charted.</td><td></td><td>Associated Event</td><td>VISAP: Papers 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1089.html">link</a></td></tr><tr><td>273</td><td>SoftVoss &amp; OctoAnemone </td><td>Yin Yu</td><td>Human skin exchanges real-time energy, such as temperature, humidity, and pressure, from its surrounding space. What if our skin can listen? SoftVoss is a morphing artificial skin that changes its appearance by real-time sound.   SoftVoss brings sound material into a body architectural space that perfectly represents the show’s theme--Mingling Spaces. In general, skin as the outer layer of a body protects our inner body from the environment. SoftVoss responds to the space through a new dimension of senses: aural. As an artificial skin, SoftVoss becomes a medium for visual communication with the soundscape.   SoftVoss used the information of sound morphology–the transformation of sound material–to shape the feather-like wearables. The realization of SoftVoss has three main components: sound materials, control system, and soft structure. Sound material, captured by microphones, is the input data to control the piece. The four channels of sound materials activate the four layers of feathers using the real-time input. The sounds captured from different source directions activities the specific layer of the feather.   SoftVoss visualizes the sound information through a 3D morphogenesis of wearable art. This work received successful recognition at my solo exhibition at the GlassBox Gallery at the University of California, Santa Barbara, in 2021.</td><td></td><td>Associated Event</td><td>VISAP: Papers 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1052.html">link</a></td></tr><tr><td>274</td><td>Beyond Human Perception</td><td>María Castellanos Vicente</td><td>The artwork is a video installation that allows the audience to visualize and compare the reactions of humans and plants to a common stimulus; live music. Erasing boundaries into the communication and understanding between both living beings and by highlighting  the immediate reactions of plants to their surrounding changes.  The installation is the result of several sessions where the brain activity of humans was measured, through the EEG registered wave,  and measuring  the electrical oscillations that are happening into the plants, measured with a sensor developed by the artists, able to detect immediate changes in plants.  Through the use of mathematics,  by using the Fast Fourier Transform, humans data and plants data are able to compare each other. This data can also be displayed graphically thanks to an algorithm developed by the artists that allow the audience to see the data through the shape of little spheres that are moving within the geometric shape of torus. Each little sphere represent each data registered. The graphic representation of  human data and plant data can be seen simultaneously in a video allowing the audience to find patterns by comparing the both living beings reactions to the live music.  The video installation is composed by two synchronized videos. One video with the the concert for plants and humans, and the other one with the data visualization of two living beings responses during the performance.  Through this artistic research we wanted to know more about the secrets language of plants. To know more about the plants’ language and behavior will allow us to know more about nature, thus we could beMer understand our environment.  To have an impact in other fields such as climate change, which is a reality happening now; the more we know about our environment and the living organisms that are living with us on Earth, the more we can do to try to improve the situation. Plants could give us a lot of information that we cannot understand yet, but this can help formulate new questions.</td><td></td><td>Associated Event</td><td>VISAP: Papers 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1038.html">link</a></td></tr><tr><td>275</td><td>Affective, Hand-Sculpted Glyph Forms for Engaging and Expressive Scientific Visualization</td><td>Stephanie Zeller, Francesca Samsel, Lyn Bartram</td><td>As scientific data continues to grow in size, complexity, and density, the representation scope of three-dimensional spaces, data sampling methods, and transfer functions have improved in parallel, allowing visualization practitioners to produce richer multidimensional encodings. Glyphs, in particular, have become an essential encoding tool due to their versatile applications in co-located multivariate volumetric datasets. While prior work has been conducted investigating the perceptual attributes of computationally-generated three-dimensional glyph-forms for scientific visualization, their affective and expressive qualities have yet to be examined. Further, our prior work has demonstrated the benefits of artist hand-created glyph forms in contrast to commonly-used synthetic forms in increasing visual diversity, discrimination, and expressive association in complex environmental datasets. In order to begin to address this gap, we establish preliminary groundwork for an affective design space for hand-created glyph forms, produce a novel set of glyph-forms based on this design space, describe a non-verbal method for discovering affective classifications of glyph-forms adopted from current affect theory, and report the results of two studies that explore how these three-dimensional forms produce consistent affective responses across assorted study cohorts.</td><td></td><td>Associated Event</td><td>VISAP: Papers 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-visap-1072.html">link</a></td></tr><tr><td>276</td><td>Hybrid Image-/Data-Parallel Rendering Using Island Parallelism</td><td>Stefan Zellmann, Ingo Wald, Joao Barbosa, Serkan Demirci, Alper Sahistan, Ugur Gudukbay</td><td>In parallel ray tracing, techniques fall into one of two camps: image-parallel techniques aim at increasing frame rate by replicating scene data across nodes and splitting the rendering work across different ranks, and data-parallel techniques aim at increasing the size of the model that can be rendered by splitting the model across multiple ranks, but typically cannot scale much in frame rate. We propose and evaluate a hybrid approach that combines the advantages of both by splitting a set of N × M ranks into M islands of N ranks each and using data-parallel rendering within each island and image parallelism across islands. We discuss the integration of this concept into four wildly different parallel renderers and evaluate the efficacy of this approach based on multiple different data sets.</td><td></td><td>Associated Event</td><td>LDAV: Parallelization &amp; Progressiveness</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-ldav-1016.html">link</a></td></tr><tr><td>277</td><td>A Prototype for Pipeline-Composable Task-Based Visualization Algorithms</td><td>Marvin Petersen, Kilian Werner, Andrea Schnorr, Torsten Wolfgang Kuhlen, Christoph Garth</td><td>For next generation platforms, the paradigm of task-based parallelism has the potential to overcome some of the accompanying challenges. This paradigm has already been applied by the visualization community to specific algorithms and problems. However, one advantage of the task-based paradigm—the interleaving of work—should lead to better utilization of resources and ultimately lower execution times if the paradigm is applied to whole pipelines. In order to investigate this potential, we build a prototype framework for composable task-based parallel visualization algorithms. With this we explore the combination of a strictly task-based approach with the addition of a pipeline layer for visualization algorithms. This additional layer eases the composition of larger task-based parallel visualization applications without the need to explicitly define the exact connection in a task graph between algorithms. In this manner, task-based visualization algorithms can be designed towards a common interface, be easily combined, and still benefit from the advantages of the task-based paradigm across algorithm boundaries, such as latency hiding. We explore the design implications of this combination and show initial results of the scalability and the impact of task interleaving on the runtime of exemplary pipelines.</td><td></td><td>Associated Event</td><td>LDAV: Parallelization &amp; Progressiveness</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-ldav-1012.html">link</a></td></tr><tr><td>278</td><td>High-Quality Progressive Alignment of Large 3D Microscopy Data</td><td>Aniketh Venkat</td><td>Large-scale three-dimensional (3D) microscopy acquisitions frequently create terabytes of image data at high resolution and magnification. Imaging large specimens at high magnifications requires acquiring 3D overlapping image stacks as tiles arranged on a two-dimensional (2D) grid that must subsequently be aligned and fused into a single 3D volume. Due to their sheer size, aligning many overlapping gigabyte-sized 3D tiles in parallel and at full resolution is memory intensive and often I/O bound. Current techniques trade accuracy for scalability, perform alignment on subsampled images, and require additional postprocess algorithms to refine the alignment quality, usually with high computational requirements. One common solution to the memory problem is to subdivide the overlap region into smaller chunks (sub-blocks) and align the sub-block pairs in parallel, choosing the pair with the most reliable alignment to determine the global transformation. Yet aligning all sub-block pairs at full resolution remains computationally expensive. The key to quickly developing a fast, high-quality, low-memory solution is to identify a single or a small set of sub-blocks that give good alignment at full resolution without touching all the overlapping data. In this paper, we present a new iterative approach that leverages coarse resolution alignments to progressively refine and align only the promising candidates at finer resolutions, thereby aligning only a small user-defined number of sub-blocks at full resolution to determine the lowest error transformation between pairwise overlapping tiles. Our progressive approach is 2.6x faster than the state of the art, requires less than 450MB of peak RAM (per parallel thread), and offers a higher quality alignment without the need for additional postprocessing refinement steps to correct for alignment errors.</td><td></td><td>Associated Event</td><td>LDAV: Parallelization &amp; Progressiveness</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-ldav-1008.html">link</a></td></tr><tr><td>279</td><td>Distributed Hierarchical Contour Trees</td><td>Hamish Carr, Oliver Rübel, Gunther H Weber</td><td>Contour trees are a significant tool for data analysis as they capture both local and global variation. However, their utility has been limited by scalability, in particular for distributed computation and storage. We report a distributed data structure for storing the contour tree of a data set distributed on a cluster, based on a fan-in hierarchy, and an algorithm for computing it based on the boundary tree that represents only the superarcs of a contour tree that involve contours that cross boundaries between blocks. This allows us to limit the communication cost for contour tree computation to the complexity of the block boundaries rather than of the entire data set.</td><td></td><td>Associated Event</td><td>LDAV: Topology &amp; Ensembles</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-ldav-1013.html">link</a></td></tr><tr><td>280</td><td>Topological Analysis of Ensembles of Hydrodynamic Turbulent Flows, an Experimental Study</td><td>Florent Nauleau, Fabien Vivodtzev, Thibault Bridel-Bertomeu, Héloïse Beaugendre, Julien Tierny</td><td>This application paper presents a comprehensive experimental evaluation of the suitability of Topological Data Analysis (TDA) for the quantitative comparison of turbulent flows. Specifically, our study documents the usage of the persistence diagram of the maxima of flow enstrophy (an established vorticity indicator), for the topological representation of 180 ensemble members, generated by a coarse sampling of the parameter space of five numerical solvers. We document five main hypotheses reported by domain experts, describing their expectations regarding the variability of the flows generated by the distinct solver configurations. We contribute three evaluation protocols to assess the validation of the above hypotheses by two comparison measures: (i) a standard distance used in distance between persistence diagrams (the L2 -Wasserstein metric). Extensive experiments on the input ensemble demonstrate the superiority of the topological distance (ii) to report as close to each other flows which are expected to be similar by domain experts, due to the configuration of their vortices. Overall, the insights reported by our study bring an experimental evidence of the suitability of TDA for representing and comparing turbulent flows, thereby providing to the fluid dynamics community confidence for its usage in future work. Also, our flow data and evaluation protocols provide to the TDA community an application-approved benchmark for the evaluation and design of further topological distances.</td><td></td><td>Associated Event</td><td>LDAV: Topology &amp; Ensembles</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-ldav-1001.html">link</a></td></tr><tr><td>281</td><td>Angular-based Edge Bundled Parallel Coordinates Plot for the Visual Analysis of Large Ensemble Simulation Data</td><td>Keita Watanabe, Naohisa Sakamoto, Jorji Nonaka, Yasumitsu Maejima</td><td>With the continuous increase in the computational power and resources of modern high-performance computing (HPC) systems, large-scale ensemble simulations have become widely used in various fields of science and engineering, and especially in meteorological and climate science. It is widely known that the simulation outputs are large time-varying, multivariate, and multivalued datasets which pose a particular challenge to the visualization and analysis tasks. In this work, we focused on the widely used Parallel Coordinates Plot (PCP) to analyze the interrelations between different parameters, such as variables, among the members. However, PCP may suffer from visual cluttering and drawing performance with the increase on the data size to be analyzed, that is, the number of polylines. To overcome this problem, we present an extension to the PCP by adding B\&#x27;{e}zier curves connecting the angular distribution plots representing the mean and variance of the inclination of the line segments between parallel axes. The proposed Angular-based Parallel Coordinates Plot (APCP) is capable of presenting a simplified overview of the entire ensemble data set while maintaining the correlation information between the adjacent variables. To verify its effectiveness, we developed a visual analytics prototype system and evaluated by using a meteorological ensemble simulation output from the supercomputer Fugaku.</td><td></td><td>Associated Event</td><td>LDAV: Topology &amp; Ensembles</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-ldav-1006.html">link</a></td></tr><tr><td>282</td><td>A Data-Centric Methodology and Task Taxonomy for Time-Stamped Event Sequences</td><td>Yasara Peiris, Clara-Maria Barth, Elaine M. Huang, Jürgen Bernard</td><td>Task abstractions and taxonomic structures for tasks are useful for designers of interactive data analysis approaches, serving as design targets and evaluation criteria alike. For individual data types, dataset-specific taxonomic structures capture unique data characteristics, while being generalizable across application domains. The creation of dataset-centric but domain-agnostic taxonomic structures is difficult, especially if best practices for a focused data type are still missing, observing experts is not feasible, and means for reflection and generalization are scarce. We discovered this need for methodological support when working with time-stamped event sequences, a datatype that has not yet been fully systematically studied in visualization research. To address this shortcoming, we present a methodology that enables researchers to abstract tasks and build dataset-centric taxonomic structures in five phases (data collection, coding, task categorization, task synthesis, and action-target-(criterion) crosscut). We validate the methodology by applying it to time-stamped event sequences and present a task typology that uses triples as a novel language of description for tasks: (1) action, (2) data target, and (3) data criterion. We further evaluate the descriptive power of the typology with a real-world case on cybersecurity.</td><td></td><td>Workshop</td><td>BELIV: Paper Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-beliv-2070.html">link</a></td></tr><tr><td>283</td><td>Creative Visualisation Opportunities Workshops: A Case Study in Population Health</td><td>Mai Elshehaly, Kuldeep Sohal, Tom Lawton, Maria Bryant, Mark Mon-Williams</td><td>Population Health Management (PHM) relies on the analysis of data from several sources to account for the complex interaction of factors that contribute to the health and well-being of a population, while considering biases and inequalities across sub-populations. Visualisation is emerging as an essential tool for insight generation from data shared and linked across services including healthcare, education, housing, policing, etc. However, visualisation design is challenged by poor data connectivity and quality, high dimensionality and complexity of real-world routinely collected data, in addition to the heterogeneity of users’ backgrounds and tasks. The Creative Visualisation Opportunities (CVO) framework provides a structured approach for working with diverse communities of visualisation stakeholders and defines a set of participatory activities for the effective elicitation of requirements and visualisation design alternatives. We conducted three workshops, applying variations of the CVO framework, with over one hundred participants from the PHM domain, including clinicians, researchers, government and private sector representatives, and local communities. In this paper, we present the results of preliminary analysis of these activities and report on the perceived impact of visualisation in this domain from a stakeholders’ perspective. We report real-world successes and limitations of applying the framework in different formats (through online and in-person workshops), and reflect on lessons learned for task analysis and visualisation design in the PHM domain.</td><td></td><td>Workshop</td><td>BELIV: Paper Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-beliv-4877.html">link</a></td></tr><tr><td>284</td><td>How Do We Measure Trust in Visual Data Communication?</td><td>Hamza Elhamdadi, Aimen Gaba, Yea-Seul Kim, Cindy Xiong</td><td>Trust is fundamental to effective visual data communication between the visualization designer and the reader. Although personal experience and preference influence readers&#x27; trust in visualizations, visualization designers can leverage design techniques to create visualizations that evoke a ``calibrated trust,&quot; at which readers arrive after critically evaluating the information presented. To systematically understand what drives readers to engage in ``calibrated trust,&quot; we must first equip ourselves with reliable and valid methods for measuring trust. Computer science and data visualization researchers have not yet reached a consensus on a trust definition or metric, which are essential to building a comprehensive trust model in human-data interaction. On the other hand, social scientists and behavioral economists have developed and perfected metrics that can measure generalized and interpersonal trust, which the visualization community can reference, modify, and adapt for our needs. In this paper, we gather existing methods for evaluating trust from other disciplines and discuss how we might use them to measure, define, and model trust in data visualization research. Specifically, we discuss quantitative surveys from social sciences, trust games from behavioral economics, measuring trust through measuring belief updating, and measuring trust through perceptual methods. We assess the potential issues with these methods and consider how we can systematically apply them to visualization research.</td><td></td><td>Workshop</td><td>BELIV: Paper Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-beliv-5626.html">link</a></td></tr><tr><td>285</td><td>How Personality and Visual Channels Affect Insight Generation</td><td>Tomás Alves, Carlota Dias, Daniel Gonçalves, Joana Henriques-Calado, Sandra Gama</td><td>Gaining insight is considered one of the relevant purposes of visual data exploration, yet studies that categorize insights are rare. This paper reports on a study to understand if the categorization model used to describe insights and personality factors affect insight-based evaluations&#x27; findings. Participants completed a set of tasks with three hierarchical visualizations and then reported what insights they could gather from them. Results show that the insight categorization taxonomies produce different descriptions of insights based on the same corpus of responses. In addition, our findings suggest that the openness to experience trait positively influences the number of reported insights. Both these factors may create obstacles to the design of insight-based evaluations and, consequently, should be controlled in the experimental design. We discuss the study implications, lessons learned, and future work opportunities.</td><td></td><td>Workshop</td><td>BELIV: Paper Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-beliv-7504.html">link</a></td></tr><tr><td>286</td><td>Evaluating Situated Visualization with Eye Tracking</td><td>Kuno Kurzhals, Michael Becher, Nelusa Pathmanathan, Guido Reina</td><td>Augmented reality (AR) technology provides means for embedding visualization in a real-world context. Such techniques allow situated analyses of live data in their spatial domain. However, as existing techniques have to be adapted for this context and new approaches will be developed, the evaluation thereof poses new challenges for researchers. Apart from established performance measures, eye tracking has proven to be a valuable means to assess visualizations qualitatively and quantitatively. We discuss the challenges and opportunities of eye tracking for the evaluation of situated visualizations. We envision that an extension of gaze-based evaluation methodology into this field will provide new insights on how people perceive and interact with visualizations in augmented reality.</td><td></td><td>Workshop</td><td>BELIV: Paper Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-beliv-4009.html">link</a></td></tr><tr><td>287</td><td>Toward Inclusiveness and Accessibility in Visualization Research: Speculations on Challenges, Solution Strategies, and Calls for Action (Position Paper)</td><td>Katrin Angerbauer, Michael Sedlmair</td><td>Inclusion and accessibility in visualization research have gained increasing attention over recent years. However, many challenges still remain to be solved on the road toward a more inclusive, shared-experience-driven visualization design and evaluation process. In this position paper, discuss challenges and speculate about potential solutions, based on related work, our own research, as well as personal experiences. The goal of this paper is to start discussions on the role of accessibility and inclusiveness in visualization design and evaluation.</td><td></td><td>Workshop</td><td>BELIV: Paper Session 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-beliv-6205.html">link</a></td></tr><tr><td>288</td><td>Research Data Curation in Visualization</td><td>Dimitar Garkov, Christoph Müller, Matthias Braun, Daniel Weiskopf, Falk Schreiber</td><td>Research data curation is the act of carefully preparing research data and artifacts for sharing and long-term preservation. Research data management is centrally implemented and formally defined in a data management plan to enable data curation. In tandem, data curation and management facilitate research repeatability. In contrast to other research fields, data curation and management in visualization are not yet part of the researcher&#x27;s compendium. In this position paper, we discuss the unique challenges visualization faces and propose how data curation can be practically realized. We share eight lessons learned in managing data in two large research consortia, outline the larger curation workflow, and define the typical roles. We complement our lessons with minimum criteria for selecting a suitable data repository and five challenging scenarios that occur in practice. We conclude with a vision of how the visualization research community can pave the way for new curation standards.</td><td></td><td>Workshop</td><td>BELIV: Paper Session 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-beliv-7497.html">link</a></td></tr><tr><td>289</td><td>An Interdisciplinary Perspective on Evaluation and Experimental Design for Visual Text Analytics</td><td>Kostiantyn Kucher, Nicole Sultanum, Angel Daza, Vasiliki Simaki, Maria Skeppstedt, Barbara Plank, Jean-Daniel Fekete, Narges Mahyar</td><td>Appropriate evaluation and experimental design are fundamental for empirical sciences, particularly in data-driven fields. Due to the successes in computational modeling of languages, for instance, research outcomes are having an increasingly immediate impact on end users. As the gap in adoption by end users decreases, the need increases to ensure that tools and models developed by the research communities and practitioners are reliable, trustworthy, and supportive of the users in their goals. In this position paper, we focus on the issues of evaluating visual text analytic approaches. We take an interdisciplinary perspective from the visualization and natural language processing communities, as we argue that the design and validation of visual text analytics include concerns beyond computational or visual/interactive methods on their own. We identify four key groups of challenges for evaluating visual text analytic approaches (data ambiguity, experimental design, user trust, and big picture concerns) and provide suggestions for research opportunities from an interdisciplinary perspective.</td><td></td><td>Workshop</td><td>BELIV: Paper Session 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-beliv-3665.html">link</a></td></tr><tr><td>290</td><td>Position Paper: Are We Making Progress In Visualization Research?</td><td>Michael Correll</td><td>In this work I use a survey of senior visualization researchers and thinkers to ideate about the notion of progress in visualization research: how are we growing as a field, what are we building towards, and are our existing methods sufficient to get us there? My respondents discussed several potential challenges for visualization research in terms of knowledge formation: a lack of rigor in the methods used, a lack of applicability to actual communities of practice, and a lack of theoretical structures that incorporate everything that happens to people and to data both before and after the few seconds when a viewer looks at a value in a chart. Orienting the field around progress (if such a thing is even desirable, which is another point of contention) I believe will require drastic re-conceptions of what the field is, what it values, and how it is taught.</td><td></td><td>Workshop</td><td>BELIV: Paper Session 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-beliv-3875.html">link</a></td></tr><tr><td>291</td><td>Power Overwhelming: Quantifying the Energy Cost of Visualisation</td><td>Christoph Müller, Moritz Heinemann, Daniel Weiskopf, Thomas Ertl</td><td>GPUs are the power-hungry tool of many visualisation researchers. However, their energy consumption has mostly been investigated outside the visualisation community, albeit our algorithms can generate more complex workloads than compute kernels. Additionally, a raising number of web-based visualisations potentially makes consumers other than the GPU more relevant. We present measurement setups for quantifying the energy cost of visualisation, ranging from software sensors over external power meters and micro controller-based setups to using oscilloscopes. These setups cover energy consumption of GPUs, CPUs and other components of a computing system. Using raycasting of spherical glyphs, volume rendering and D3 visualisations as examples, we show that there are viable options for evaluating most kinds of visualisations. We conclude by stating the challenges to a broader application of these techniques and by making recommendations on how to overcome these.</td><td></td><td>Workshop</td><td>BELIV: Paper Session 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-beliv-7664.html">link</a></td></tr><tr><td>292</td><td>Paper 1: [Best Paper Award] PSEUDo: Interactive Pattern Search in Multivariate Time Series with Locality-Sensitive Hashing and Relevance Feedback</td><td>Yuncong Yu, Dylan Kruyff, Jiao Jiao, Tim Becker, Michael Behrisch</td><td></td><td></td><td>Associated Event</td><td>VDS: Opening / Keynote 1 + Paper Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-vds-1033.html">link</a></td></tr><tr><td>293</td><td>Paper 2: Motif-Based Visual Analysis of Dynamic Network</td><td>Eren Cakmak, Johannes Fuchs, Dominik Jäckle, Tobias Schreck, Ulrik Brandes, Daniel Keim</td><td></td><td></td><td>Associated Event</td><td>VDS: Opening / Keynote 1 + Paper Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-vds-1019.html">link</a></td></tr><tr><td>294</td><td>Paper 3: How Do Data Scientists Communicate Intermediate Results?</td><td>Yuren Pang, Ruotong Wang, Leilani Battle</td><td></td><td></td><td>Associated Event</td><td>VDS: Opening / Keynote 1 + Paper Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-vds-1025.html">link</a></td></tr><tr><td>295</td><td>Paper 4 BiaScope: Visual Unfairness Diagnosis for Graph Embeddings</td><td>Agapi Rissaki, Bruno Scarone, David Liu, Aditeya pandey, Brennan Klein, Tina Eliassi-Rad, Michelle Borkin</td><td></td><td></td><td>Associated Event</td><td>VDS: Opening / Keynote 1 + Paper Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-vds-1030.html">link</a></td></tr><tr><td>296</td><td>Paper 5: [Short Paper] Comparison of Computational Notebook Systems for Interactive Visual Analytics</td><td>Han Liu, Chris North</td><td></td><td></td><td>Associated Event</td><td>VDS: Paper Session 2 + Keynote 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-vds-1038.html">link</a></td></tr><tr><td>297</td><td>Paper 6: Interactive Visualization for Data Science Scripts </td><td>Rebecca Faust, Carlos Scheidegger, Katherine Isaacs, William Bernstein, Michael Sharp, Chris North</td><td></td><td></td><td>Associated Event</td><td>VDS: Paper Session 2 + Keynote 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-vds-1021.html">link</a></td></tr><tr><td>298</td><td>Paper 7: Communication Analysis through Visual Analytics: Current Practices, Challenges, and New Frontie</td><td>Maximilian Fischer, Frederik Dennig, Daniel Seebacher, Daniel Keim, Mennatallah El-Assady</td><td></td><td></td><td>Associated Event</td><td>VDS: Paper Session 2 + Keynote 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-vds-1017.html">link</a></td></tr><tr><td>299</td><td>ANNOTED: Taming Protein Beasts Through Interactive Visualization</td><td>Aaron Watters, Vikram Mulligan</td><td>ANNOTED is an interactive visualization designed to address the requirements of the Bio+MedViz Challenge 2022. Briefly, it is intended to permit visualization of proteins involved in disease, and to allow users to explore sites at which amino acid residues are modified in potentially disease-relevant ways. It is published as a public web application at https://aaronwatters.github.io/bioviz2022/challenge.html with open source code available at https://github.com/AaronWatters/bioviz2022 as a Github repository. </td><td></td><td>Associated Event</td><td>Bio+MedVis: Challenges</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-biovischallenge-1001.html">link</a></td></tr><tr><td>300</td><td>A simple pipeline for creating cover quality images of post-translational modifications to molecular structures in rare disease</td><td>Mo Rahman, Onno Faber, Pascale Marill, Otavio Good</td><td>Changes to molecular structure can have a large impact on biological function and are the basis for many diseases. Structural modifications to proteins, while not changes to the amino acid sequence, can alter their function and have been implicated in rare diseases. Visualizing these modifications on a 3D model helps understand how the characteristics of a molecule are affected. We present a simple pipeline to create high-quality rendered images of post-translational protein modifications. Data provided by CompOmics group at VIB and Ghent University in Ghent, Belgium is parsed using standard UNIX commands, then added to AlphaFold predicted structures using a newly developed viewer Dalton, and finally rendered to an image. Well-designed and simple to make visualizations demystify rare diseases and mechanisms of post-translational structural modifications, making it easier to conduct further research in these fields.</td><td></td><td>Associated Event</td><td>Bio+MedVis: Challenges</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-biovischallenge-1002.html">link</a></td></tr><tr><td>301</td><td>ProtoFold Neighborhood Inspector</td><td>Nicolas F. Chaves-de-Plaza, Klaus Hildebrandt, Anna Vilanova</td><td>Post-translational modifications (PTMs) affecting a protein&#x27;s residues (amino acids) can disturb its function, leading to illness. Whether or not a PTM is pathogenic depends on its type and the status of neighboring residues. In this paper, we present the ProtoFold Neighborhood Inspector (PFNI), a visualization system for analyzing residues neighborhoods. The main contribution is a visualization idiom, the Residue Constellation (RC), for identifying and comparing three-dimensional neighborhoods based on per-residue features and spatial characteristics. The RC leverages two-dimensional representations of the protein&#x27;s three-dimensional structure to overcome problems like occlusion, easing the analysis of neighborhoods that often have complicated spatial arrangements. Using the PFNI, we explored proteins’ structural PTM data, which allowed us to identify patterns in the distribution and quantity of per-neighborhood PTMs that might be related to their pathogenic status. In the following, we define the tasks that guided the development of the PFNI and describe the data sources we derived and used. Then, we introduce the PFNI and illustrate its usage through an example of an analysis workflow. We conclude by reflecting on preliminary findings obtained while using the tool on the provided data and future directions concerning the development of the PFNI.</td><td></td><td>Associated Event</td><td>Bio+MedVis: Challenges</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-biovischallenge-1003.html">link</a></td></tr><tr><td>302</td><td>(Re-design) EProM - Exploration of Protein Modifications </td><td>Daniël M. Bot, Jannes Peeters, Jan Aerts</td><td>We present EProM—a visual analysis interface for the exploration of protein modifications—as a contribution to the IEEE VIS 2022 Bio+MedVis Challenge. The interface targets researchers in bio-chemistry, proteomics, and precision medicine as its primary users. Observed modifications can be inspected from the protein’s primary, secondary, and tertiary structure, using a straightforward design and intuitive interactions. Modifications’ measurement uncertainty and relation to residues with identified pathogenic mutations are considered.</td><td></td><td>Associated Event</td><td>Bio+MedVis: Challenges</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-biovischallenge-1004.html">link</a></td></tr><tr><td>303</td><td>A Dashboard for the Visualisation of a Protein’s PTMs</td><td>Simon Hackl, Theresa Harbig, Caroline Jachmann, Mathias Witte Paz, Kay Nieselt</td><td>In our submission we address the redesign task of the Bio+MedVis Challenge 2022. The existing visualization shows all post-translational modifications (PTMs) identified on a protein as circles that are connected to their position on the protein’s primary sequence by a line. Overall, the figure communicates well that there can be significantly more PTMs on proteins than anticipated. However, the visualization suffers from overplotting in multiple dimensions, rendering a detailed interpretation impossible. First, on the x-axis adjacent positions may carry PTMs, which causes the circles to overlap. Second, on the y-axis the PTMs for a position are stacked. From the visualizations, it is not clear if some PTMs are not displayed due to space limitations. Moreover, there are more different PTMs than there are colors that can be differentiated and a legend is missing. Most importantly, the visualization is based only on the primary sequence. No (visual) relation between PTMs and the protein structure can be established.</td><td></td><td>Associated Event</td><td>Bio+MedVis: Challenges</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-biovischallenge-1007.html">link</a></td></tr><tr><td>304</td><td>Tightening the Loop in Mixed-Initiative ML Engineering and Domain Annotation using Active Learning and Visual Analytics</td><td></td><td></td><td></td><td>Workshop</td><td>BioMedical AI: Keynote and Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-biomedicalai-1053.html">link</a></td></tr><tr><td>305</td><td>Opening Access to Visual Exploration of Audiovisual Digital Biomarkers: an OpenDBM Analytics Tool</td><td></td><td></td><td></td><td>Workshop</td><td>BioMedical AI: Keynote and Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-biomedicalai-7627.html">link</a></td></tr><tr><td>306</td><td>An Interactive Interpretability System for Breast Cancer Screening with Deep Learning</td><td></td><td></td><td></td><td>Workshop</td><td>BioMedical AI: Session 2 and Panel</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-biomedicalai-4793.html">link</a></td></tr><tr><td>307</td><td>Kokiri: Random Forest-Based Cohort Comparison and Characterization</td><td></td><td></td><td></td><td>Workshop</td><td>BioMedical AI: Session 2 and Panel</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-biomedicalai-9146.html">link</a></td></tr><tr><td>308</td><td>K-Means Clustering: An Explorable Explainer</td><td>Yi Zhe Ang</td><td></td><td></td><td>Workshop</td><td>VISxAI: Opening, Keynote + Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-visxai-2556.html">link</a></td></tr><tr><td>309</td><td>Action as Information</td><td>Paschalis Bizopoulos</td><td></td><td></td><td>Workshop</td><td>VISxAI: Opening, Keynote + Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-visxai-7873.html">link</a></td></tr><tr><td>310</td><td>How is Real-World Gender Bias Reflected in Language Models?</td><td>Alexander Theus, Javier Rando, Rita Sevastjanova, Mennatallah El-Assady</td><td></td><td></td><td>Workshop</td><td>VISxAI: Session 2, Session 3, Closing</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-visxai-7361.html">link</a></td></tr><tr><td>311</td><td>Explaining Image Classifiers with Wavelets</td><td>Julius Hege, Stefan Kolek, Gitta Kutyniok</td><td></td><td></td><td>Workshop</td><td>VISxAI: Session 2, Session 3, Closing</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-visxai-6729.html">link</a></td></tr><tr><td>312</td><td>What should we watch tonight?</td><td>Ibrahim Al-Hazwani, Gabriela Morgenshtern, Yves Rutishauser, Mennatallah El-Assady, Jürgen Bernard</td><td></td><td></td><td>Workshop</td><td>VISxAI: Session 2, Session 3, Closing</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-visxai-7608.html">link</a></td></tr><tr><td>313</td><td>Poisoning Attacks and Subpopulation Susceptibility</td><td>Evan Rose, David Evans, Fnu Suya</td><td></td><td></td><td>Workshop</td><td>VISxAI: Session 2, Session 3, Closing</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-visxai-3404.html">link</a></td></tr><tr><td>314</td><td>Mapping Wikipedia with BERT and UMAP</td><td>Brandon Duderstadt, Andriy Mulyar</td><td></td><td></td><td>Workshop</td><td>VISxAI: Session 2, Session 3, Closing</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-visxai-8479.html">link</a></td></tr><tr><td>315</td><td>An Interactive Introduction to Causal Inference</td><td>Lucius Bynum, Falaah Arif Khan, Oleksandra Konopatska, Julia Stoyanovich, Joshua Loftus</td><td></td><td></td><td>Workshop</td><td>VISxAI: Session 2, Session 3, Closing</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-visxai-1801.html">link</a></td></tr><tr><td>316</td><td>Data Feel: Exploring Visual Effects in Video Games to Support Sensemaking Tasks</td><td>Hongwei Zhou, Angus Forbes</td><td></td><td></td><td>Workshop</td><td>VIS4DH: Paper Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4dh-1009.html">link</a></td></tr><tr><td>317</td><td>Multimodal analogs to infer humanities visualization requirements</td><td>Richard Brath</td><td></td><td></td><td>Workshop</td><td>VIS4DH: Paper Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4dh-1011.html">link</a></td></tr><tr><td>318</td><td>Boundaries, Extensions, and Challenges of Visualization for Humanities Data: Reflections on Three Cases</td><td>Rongqian Ma</td><td></td><td></td><td>Workshop</td><td>VIS4DH: Paper Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4dh-1016.html">link</a></td></tr><tr><td>319</td><td>Word Clouds in the Wild</td><td>Rebecca Hicke, Maanya Goenka, Eric Alexander</td><td></td><td></td><td>Workshop</td><td>VIS4DH: Paper Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4dh-1015.html">link</a></td></tr><tr><td>320</td><td>The Multiple Faces of Cultural Heritage: Towards an Integrated Visualization Platform for Tangible and Intangible Cultural Assets</td><td>Eva Mayr, Florian Windhager, Johannes Liem, Samuel Beck, Steffen Koch, Jakob Kusnick, Stefan Jänicke</td><td></td><td></td><td>Workshop</td><td>VIS4DH: Paper Session 2, Lab Talks 2, Closing</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4dh-1017.html">link</a></td></tr><tr><td>321</td><td>TU Darmstadt and Fraunhofer #1013</td><td>Jan Burmeister, Jilin Liao, Jieqing Yang, Qingtian Wei, Kexin Wang</td><td></td><td></td><td>Associated Event</td><td>VAST Challenge: Opening, Challenge 1, and Start of Challenge 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-vast-1013.html">link</a></td></tr><tr><td>322</td><td>giCentre, City, University of London #1017</td><td>Jo Wood</td><td>The VAST 2022 Challenge 1 put us in place of an urban planning analyst for the fictitious city Engagement, Ohio. Given a large, interlinked dataset of its citizens, businesses and activities, we were tasked to assemble a summary of the state of Engagement to serve as basis for future investments. We present a visual analytics application to interactively explore this complex dataset using a combination of dashboards and a 2D/3D map. Our application uses interactive visualizations to iteratively build and analyze subgroups, sync their abstract data with spatial positions, and detect clusters and patterns in the data.</td><td></td><td>Associated Event</td><td>VAST Challenge: Opening, Challenge 1, and Start of Challenge 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-vast-1017.html">link</a></td></tr><tr><td>323</td><td>SAS Institute #1004</td><td>Falko Schulz, Don Chapman, Steven Harenberg, Stu Sztukowski, Cheryl LeSaint</td><td></td><td></td><td>Associated Event</td><td>VAST Challenge: Challenges 2 and 3</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_a-vast-1004.html">link</a></td></tr><tr><td>324</td><td>Fast Merge Tree Computation via SYCL</td><td>Arnur Nigmetov, Dmitriy Morozov</td><td>A merge tree is a topological descriptor of a real-valued function. Merge trees are used in visualization and topological data analysis, either directly or as a means to another end: computing a 0-dimensional persistence diagram, identifying connected components, performing topological simplification, etc. Scientific computing relies more and more on GPUs to achieve fast, scalable computation. For efficiency, data analysis should take place at the same location as the main computation, which motivates interest in parallel algorithms and portable software for merge trees that can run not only on a CPU, but also on a GPU, or other types of accelerators. The SYCL standard defines a programming model that allows the same code, written in standard C++, to compile targets for multiple parallel backends (CPUs via OpenMP or TBB, NVIDIA GPUs via CUDA, AMD GPUs via ROCm, Intel GPUs via Level Zero, FPGAs). In this paper, we adapt the triplet merge tree algorithm to SYCL and compare our implementation with the \vtkm implementation, which is the only other implementation of merge trees for GPUs that we know of.</td><td>triplet merge tree, computations on GPU, SYCL</td><td>Workshop</td><td>TopoInVis: Opening, Keynote + Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-topoinvis-1026.html">link</a></td></tr><tr><td>325</td><td>Counterexamples expose gaps in the proof of time complexity for cover trees introduced in 2006</td><td>Yury Elkin, Vitaliy Kurlin</td><td>This paper is motivated by the k-nearest neighbors search: given an arbitrary metric space, and its finite subsets (a reference set R and a query set Q), design a fast algorithm to find all k-nearest neighbors in R for every point q in Q. In 2006, Beygelzimer, Kakade, and Langford introduced cover trees to justify a near-linear time complexity for the neighbor search in the sizes of Q,R.   Section 5.3 of Curtin&#x27;s PhD (2015) pointed out that the proof of this result was wrong.  The key step in the original proof attempted to show that the number of iterations can be estimated by multiplying the length of the longest root-to-leaf path in a cover tree by a constant factor. However, this estimate can miss many potential nodes in several branches of a cover tree, that should be considered during the neighbor search. The same argument was unfortunately repeated in several subsequent papers using cover trees from 2006.  This paper explicitly constructs challenging datasets that provide counterexamples to the past proofs of time complexity for the cover tree construction, the k-nearest neighbor search presented at ICML 2006, and the dual-tree search algorithm published in NIPS 2009.  The corrected near-linear time complexities with extra parameters are proved in another forthcoming paper by using a new compressed cover tree simplifying the original tree structure.</td><td>Cover tree, k-nearest neighbors, counterexample, compressed cover tree, dual-tree algorithms, parametrized time complexity</td><td>Workshop</td><td>TopoInVis: Opening, Keynote + Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-topoinvis-1010.html">link</a></td></tr><tr><td>326</td><td>Autoencoder-Aided Visualization of Collections of Morse Complexes</td><td>Jixian Li, Dan Van Boxel, Joshua Levine</td><td>Though analyzing a single scalar field using Morse complexes is well studied, there are few techniques for visualizing a collection of Morse complexes. We focus on analyses that are enabled by looking at a Morse complex as an embedded domain decomposition. Specifically, we target 2D scalar fields, and we encode the Morse complex through binary images of the boundaries of decomposition. Then we use image-based autoencoders to create a feature space for the Morse complexes. We apply additional dimensionality reduction methods to construct a scatterplot as a visual interface of the feature space. This allows us to investigate individual Morse complexes, as they relate to the collection, through interaction with the scatterplot. We demonstrate our approach using a synthetic data set, microscopy images, and time-varying vorticity magnitude fields of flow.  Through these, we show that our method can produce insights about structures within the collection of Morse complexes.</td><td>Morse complex, Dimensionality reduction, Autoencoders</td><td>Workshop</td><td>TopoInVis: Session 2, Early Career Lightning Talks + Best Paper Awards </td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-topoinvis-1004.html">link</a></td></tr><tr><td>327</td><td>A Deformation-based Edit Distance for Merge Trees</td><td>Florian Wetzels, Christoph Garth</td><td>In scientific visualization, scalar fields are often compared through edit distances between their merge trees. Typical tasks include ensemble analysis, feature tracking and symmetry or periodicity detection. Tree edit distances represent how one tree can be transformed into another through a sequence of simple edit operations: relabeling, insertion and deletion of nodes. In this paper, we present a new set of edit operations working directly on the merge tree as an geometrical or topological object: the represented operations are deformation retractions and inverse transformations on merge trees, which stands in contrast to other methods working on branch decomposition trees. We present a quartic time algorithm for the new edit distance, which is branch decomposition-independent and a metric on the set of all merge trees.</td><td>Scalar data, Topological data analysis, Merge trees, Edit distance</td><td>Workshop</td><td>TopoInVis: Session 2, Early Career Lightning Talks + Best Paper Awards </td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-topoinvis-1005.html">link</a></td></tr><tr><td>328</td><td>Reduced Connectivity for Local Bilinear Jacobi Sets</td><td>Daniel Klötzl, Tim Krake, Youjia Zhou, Jonathan Stober, Kathrin Schulte, Ingrid Hotz, Bei Wang, Daniel Weiskopf</td><td>We present a new topological connection method for the local bilinear computation of Jacobi sets that improves the visual representation while preserving the topological structure and geometric configuration. To this end, the topological structure of the local bilinear method is utilized, which is given by the nerve complex of the traditional piecewise linear method. Since the nerve complex consists of higher-dimensional simplices, the local bilinear method (visually represented by the 1-skeleton of the nerve complex) leads to clutter via crossings of line segments. Therefore, we propose a homotopy-equivalent representation that uses different collapses and edge contractions to remove such artifacts. Our new connectivity method is easy to implement, comes with only little overhead, and results in a less cluttered representation.</td><td>Human-centered computing - Visualization - Visualization techniques; Mathematics of computing - Discrete mathematics</td><td>Workshop</td><td>TopoInVis: Session 2, Early Career Lightning Talks + Best Paper Awards </td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-topoinvis-1002.html">link</a></td></tr><tr><td>329</td><td>Jacobi Set Driven Search for Flexible Fiber Surface Extraction</td><td>Mohit Sharma, Vijay Natarajan</td><td>Isosurfaces are an important tool for analysis and visualization of univariate scalar fields. Earlier works have demonstrated the presence of interesting isosurfaces at isovalues close to critical values. This motivated the development of efficient methods for computing individual components of isosurfaces restricted to a region of interest. Generalization of isosurfaces to fiber surfaces and critical points to Jacobi sets has resulted in new approaches for analyzing bivariate scalar fields. Unlike isosurfaces, there exists no output sensitive method for computing fiber surfaces. Existing methods traverse through all the tetrahedra in the domain. In this paper, we propose the use of the Jacobi set to identify fiber surface components of interest and present an output sensitive approach for its computation. The Jacobi edges are used to initiate the search towards seed tetrahedra that contain the fiber surface, thereby reducing the search space. This approach also leads to effective analysis of the bivariate field by supporting the identification of relevant fiber surfaces near Jacobi edges.</td><td>Human-centered computing—Visualization—Visualization techniques; Human-centered computing—Visualization application domains—Scientific visualization;</td><td>Workshop</td><td>TopoInVis: Session 2, Early Career Lightning Talks + Best Paper Awards </td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-topoinvis-1008.html">link</a></td></tr><tr><td>330</td><td>Discussion and Visualization of Distinguished Hyperbolic Trajectories as a Generalization of Critical Points to 2D Time-dependent Flow</td><td>Roxana Bujack</td><td>Classical vector field topology has proven to be a useful visualization technique for steady flow, but its straightforward application to time-dependent flows lacks physical meaning. Necessary requirements for physical meaningfulness include the results to be objective, i.e., independent of the frame of reference of the observer, and Lagrangian, i.e., that the generalized critical points are trajectories. We analyze whether the theoretical concept of distinguished hyperbolic trajectories provides a physically meaningful generalization to classical critical points and if the existing extraction algorithms correctly compute what has been defined mathematically. We show that both theory and algorithms constitute a significant improvement over previous methods. We further present a method to visualize a time-dependent flow field in the reference frames of distinguished trajectories. The result is easy to interpret because it makes these trajectories look like classical critical points for each instance in time, but it is meaningful because it is Lagrangian and objective.</td><td>visualization, vector field, flow, topology, Lagrangian, objective, time-dependent, distinguished hyperbolic trajectory</td><td>Workshop</td><td>TopoInVis: Session 2, Early Career Lightning Talks + Best Paper Awards </td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-topoinvis-1006.html">link</a></td></tr><tr><td>331</td><td>ClassMat: a Matrix of Small Multiples to Analyze the Topology of Multiclass Multidimensional Data</td><td>Michael Aupetit, Ahmed Ali, Abdelkader Baggag, Halima Bensmail</td><td>Data scientists often deal with multiclass multidimensional data. There is a need to support the exploration of such data with topological methods. We propose a new visualization metaphor for multiclass data and illustrate it with two complementary analytic approaches. We design ClassMat, a visualization matrix similar in spirit to the scatterplot matrix (SPLOM or pairs plot) but focused on pairs of classes rather than pairs of dimensions. We show how this visualization matrix can be used in two main multidimensional data visualization pipelines: visualization-then-topological-analysis and topological-analysis-then-visualization. In particular, we show it can support the analyst in detecting interferences between topological features of different classes.</td><td>Visualization; topological data analysis;</td><td>Workshop</td><td>TopoInVis: Session 2, Early Career Lightning Talks + Best Paper Awards </td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-topoinvis-1007.html">link</a></td></tr><tr><td>332</td><td>Untangling Force-Directed Layouts Using Persistent Homology</td><td>Bhavana Doppalapudi, Bei Wang, Paul Rosen</td><td>Force-directed layouts belong to a popular class of methods used to position nodes in a node-link diagram. However, they typically lack direct consideration of global structures, which can result in visual clutter and the overlap of unrelated structures. In this paper, we use the principles of persistent homology to untangle force-directed layouts thus mitigating these issues. First, we devise a new method to use 0-dimensional persistent homology to efficiently generate an initial graph layout. The approach results in faster convergence and better quality graph layouts. Second, we provide a new definition and an efficient algorithm for 1-dimensional persistent homology features (i.e., tunnels/cycles) on graphs. We provide users the ability to interact with the 1-dimensional features by highlighting them and adding cycle-emphasizing forces to the layout. Finally, we evaluate our approach with 32 synthetic and real-world graphs by computing various metrics, e.g., co-ranking, edge crossing, etc., to demonstrate the efficacy of our proposed method.</td><td>Force-directed layout, persistent homology, graph clustering, graph cycles</td><td>Workshop</td><td>TopoInVis: Session 2, Early Career Lightning Talks + Best Paper Awards </td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-topoinvis-1000.html">link</a></td></tr><tr><td>333</td><td>Exploring Cyclone Evolution with Hierarchical Features</td><td>Emma Nilsson, Jonas Lukasczyk, Wito Engelke, Talha Bin Masood, Gunilla Svensson, Rodrigo Caballero, Christoph Garth, Ingrid Hotz</td><td>The problem of tracking and visualizing cyclones is still an active area of climate research, since the nature of cyclones varies depending on geospatial location and temporal season, resulting in no clear mathematical definition. Thus, many cyclone tracking methods are tailored to specific datasets and therefore do not support general cyclone extraction across the globe. To address this challenge, we present a conceptual application for exploring cyclone evolution by organizing the extracted cyclone tracks into hierarchical groups. Our approach is based on extrema tracking, and the resulting tracks can be defined in a multi-scale structure by grouping the points based on a novel feature descriptor defined on the merge tree, so-called crown features. Consequently, multiple parameter settings can be visualized and explored in a level-of-detail approach, supporting experts to quickly gain insights on cyclonic formation and evolution. We describe a general cyclone exploration pipeline that consists of four modular building blocks: (1) an extrema tracking method, (2) multiple definitions of cyclones as groups of extrema, including crown features, (3) the correlation of cyclones based on the underlying tracking information, and (4) a hierarchical visualization of the resulting feature tracks and their spatial embedding, allowing exploration on a global and local scale. In order to be as flexible as possible, our pipeline allows for exchanging every module with different techniques, such as other tracking methods and cyclone definitions.</td><td>Human-centered computing—Visualization—Visualization design and evaluation methods; Human-centered computing—Visualization—Visualization application domains—Scientific visualization</td><td>Workshop</td><td>TopoInVis: Session 2, Early Career Lightning Talks + Best Paper Awards </td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-topoinvis-1016.html">link</a></td></tr><tr><td>334</td><td>Towards Benchmark Data Generation for Feature Tracking in Scalar Fields</td><td>Emma Nilsson, Jonas Lukasczyk, Talha Bin Masood, Christoph Garth, Ingrid Hotz</td><td>We describe a benchmark data generator for tracking methods for two- and three-dimensional time-dependent scalar fields. More and more topology-based tracking methods are presented in the visualization community, but the validation and evaluation of the tracking results are currently limited to qualitative visual approaches. We present a pipeline for creating different ground truth features that support evaluating tracking methods based on quantitative measures. In short, our approach randomly simulates a temporal point cloud with birth, death, split, merge, and continuation events, where the points are then used to derive a scalar field whose topological features correspond to the points. These scalar fields can be used as the input for different tracking methods, where the computed tracks can be compared against the ground truth feature evolution. This approach facilitates directly comparing the results of different tracking methods, independent of the initial feature characterization.</td><td>Human-centered computing—Visualization—Visualization design and evaluation methods; Human-centered computing—Visualization—Visualization application domains—Scientific visualization</td><td>Workshop</td><td>TopoInVis: Session 2, Early Career Lightning Talks + Best Paper Awards </td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-topoinvis-1017.html">link</a></td></tr><tr><td>335</td><td>Subject-Specific Brain Activity Analysis in fMRI Data Using Merge Trees</td><td>Farhan Rasheed, Daniel Jönsson, Emma Nilsson, Talha Bin Masood, Ingrid Hotz</td><td>We present a method for detecting patterns in time-varying functional magnetic resonance imaging (fMRI) data based on topological analysis. The oxygenated blood flow measured by fMRI is widely used as an indicator of brain activity. The signal is, however, prone to noise from various sources. Random brain activity, physiological noise, and noise from the scanner can reach a strength comparable to the signal itself. Thus, extracting the underlying signal is a challenging process typically approached by applying statistical methods. The goal of this work is to investigate the possibilities of recovering information from the signal using topological feature vectors directly based on the raw signal without medical domain priors. We utilize merge trees to define a robust feature vector capturing key features within a time step of fMRI data. We demonstrate how such a concise feature vector representation can be utilized for exploring the temporal development of brain activations, connectivity between these activations, and their relation to cognitive tasks.</td><td>fMRI data analysis, data abstraction, temporal data, feature detection, merge tree, computational topology-based techniques</td><td>Workshop</td><td>TopoInVis: Session 2, Early Career Lightning Talks + Best Paper Awards </td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-topoinvis-1011.html">link</a></td></tr><tr><td>336</td><td>Visual Exploration of Rheological Test Results from Soft Materials (Q+A)</td><td>Jonas Madsen, Lasse Sode, Julie Frost Dahl, Milena Corredig, Hans-Jörg Schulz</td><td></td><td></td><td>Workshop</td><td>TestVis: Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-testvis-1002.html">link</a></td></tr><tr><td>337</td><td>Interactive Analysis of Post-Silicon Validation Data</td><td>Andrés Lalama, Johannes Knittel, Steffen Koch, Daniel Weiskopf, Thomas Ertl, Sarah Rottacker, Raphaël Latty, Jochen Rivoir</td><td></td><td></td><td>Workshop</td><td>TestVis: Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-testvis-1000.html">link</a></td></tr><tr><td>338</td><td>Test Intelligence: How Modern Analyses and Visualizations in Teamscale Support Software Testing</td><td>Jakob Rott</td><td></td><td></td><td>Workshop</td><td>TestVis: Session 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-testvis-1001.html">link</a></td></tr><tr><td>339</td><td>FLoAT: Framework for Workflow Analysis, Visualization and Transformation</td><td>John Jacobson III, Michael Bentley, Cayden Lund, Ganesh Gopalakrishnan, Ignacio Laguna, Gregory L. Lee</td><td></td><td></td><td>Workshop</td><td>TestVis: Session 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-testvis-1004.html">link</a></td></tr><tr><td>340</td><td>Supporting Domain Characterization in Visualization Design Studies With the Critical Decision Method</td><td>Lena Cibulski, Evanthia Dimara, Setia Hermawati, Jörn Kohlhammer</td><td></td><td></td><td>Workshop</td><td>VisGuides: Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-visguides-1006.html">link</a></td></tr><tr><td>341</td><td>How should we design violin plots?</td><td>Pere-Pau Vázquez, Laia Viale Herrando, Elena Molina Lopez</td><td></td><td></td><td>Workshop</td><td>VisGuides: Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-visguides-1009.html">link</a></td></tr><tr><td>342</td><td>Semantic Color Mapping: A Pipeline for Assigning Meaningful Colors to Text</td><td>Mennatallah El-Assady, Rebecca Kehlbeck, Yannick Metz, Rita Sevastjanova, Fabian Sperrle, Thilo Spinner, Udo Schlegel</td><td></td><td></td><td>Workshop</td><td>VisGuides: Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-visguides-1005.html">link</a></td></tr><tr><td>343</td><td>Reflections and Considerations on Running Creative Visualization Learning Activities</td><td>Jonathan C Roberts, Benjamin Bach, Magdalena Boucher, Fanny Chevalier, Alexandra Diehl, Uta Hinrichs, Samuel Huron, Andy D Kirk, Søren Knudsen, Isabel Meirelles, Rebecca Noonan, Laura Pelchmann, Fateme Rajabiyazdi, Christina Stoiber</td><td></td><td></td><td>Workshop</td><td>VisGuides: Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-visguides-1007.html">link</a></td></tr><tr><td>344</td><td>Considering the Role of Guidelines in Visualization Design Practice</td><td>Paul Parsons, Prakash Chandra Shukla</td><td></td><td></td><td>Workshop</td><td>VisGuides: Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-visguides-1012.html">link</a></td></tr><tr><td>345</td><td>Trustworthy Visual Analytics in Clinical Gait Analysis: A Case Study for Patients with Cerebral Palsy  </td><td>Alexander Rind, Djordje Slijepcevic, Matthias Zeppelzauer, Fabian Unglaube, Andreas Kranzl, Brian Horsak</td><td></td><td></td><td>Workshop</td><td>TREX: Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-trex-8894.html">link</a></td></tr><tr><td>346</td><td>Understanding Systematic Miscalibration in Machine Learning Classifiers  </td><td>Markelle Kelly, Padhraic Smyth</td><td></td><td></td><td>Workshop</td><td>TREX: Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-trex-3846.html">link</a></td></tr><tr><td>347</td><td>How Do Algorithmic Fairness Metrics Align with Human Judgement? A Mixed-Initiative System for Contextualized Fairness Assessment  </td><td>Rares Constantin, Moritz Dück, Anton Alexandrov, Patrik Matosevic, Daphna Keidar, Mennatallah El-Assady</td><td></td><td></td><td>Workshop</td><td>TREX: Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-trex-6700.html">link</a></td></tr><tr><td>348</td><td>Kicking Analysts Out of the Meeting Room: Supporting Future Data-driven Decision Making with Intelligent Interactive Visualization Systems  </td><td>Yi Han</td><td></td><td></td><td>Workshop</td><td>TREX: Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-trex-9231.html">link</a></td></tr><tr><td>349</td><td>Standardized Process Models for Applying Artificial Intelligence to High-Risk Decision-Making: A Pediatric Neuro-Oncology Perspective  </td><td>Eric Prince, Todd Hankinson, Carsten Görg</td><td></td><td></td><td>Workshop</td><td>TREX: Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-trex-1111.html">link</a></td></tr><tr><td>350</td><td>Exploring Effectiveness of Explanations for Appropriate Trust: Lessons from Cognitive Psychology  </td><td>Ruben Verhagen, Siddharth Mehrotra, Mark Neerincx, Catholijn Jonker, Myrthe Tielman</td><td></td><td></td><td>Workshop</td><td>TREX: Session 1</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-trex-5208.html">link</a></td></tr><tr><td>351</td><td>Perception of Skill in Visual Problem Solving: An Analysis of Interactive Behaviors, Personality Traits, and the Dunning-Kruger Effect  </td><td>Bonnie Chen, Emily Wall</td><td></td><td></td><td>Workshop</td><td>TREX: Session 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-trex-6504.html">link</a></td></tr><tr><td>352</td><td>Using Processing Fluency as a Metric of Trust in Scatterplot Visualizations  </td><td>Hamza Elhamdadi, Lace Padilla, Cindy Xiong</td><td></td><td></td><td>Workshop</td><td>TREX: Session 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-trex-2365.html">link</a></td></tr><tr><td>353</td><td>Data Provenance Visualization in Brazilian Public Health Dashboards  </td><td>Johne Marcus Jarske, Lucia Filgueiras, Leandro Manuel Velloso, Tânia Letícia Letícia dos Santos, Jorge Rady de Almeida Júnior</td><td></td><td></td><td>Workshop</td><td>TREX: Session 2</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-trex-2027.html">link</a></td></tr><tr><td>354</td><td>Paper | Additional Perspectives on Data Equity</td><td>Jonathan Schwabish, Alice Feng</td><td></td><td></td><td>Workshop</td><td>Vis4Good: Opening + Keynote + 2 Papers</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4good-4497.html">link</a></td></tr><tr><td>355</td><td>Paper | Exploring and Explaining Climate Change: Exploranation as a Visualization Pedagogy for Societal Action</td><td>Lonni Besançon, Konrad Schönborn, Erik Sundén, Yin He, Samuel Rising, Peteer Westerdahl, Patric Ljung, Josef Wideström, Charles Hansen, Anders Ynnerman</td><td></td><td></td><td>Workshop</td><td>Vis4Good: Opening + Keynote + 2 Papers</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4good-4801.html">link</a></td></tr><tr><td>356</td><td>Paper | Envisioning Situated Visualizations of Environmental Footprints in an Urban Environment</td><td>Yvonne Jansen, Federica Bucchieri, Pierre Dragicevic, Martin Hachet, Morgane Koval, Léana Petiot, Arnaud Prouzeau, Dieter Schmalstieg, Lijie Yao, Petra Isenberg</td><td></td><td></td><td>Workshop</td><td>Vis4Good: Opening + Keynote + 2 Papers</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4good-9035.html">link</a></td></tr><tr><td>357</td><td>Paper | Can data visualizations change minds? Identifying mechanisms of elaborative thinking and persuasion</td><td>Douglas Markant, Milad Rogha, Alireza Karduni, Ryan Wesslen, Wenwen Dou</td><td></td><td></td><td>Workshop</td><td>Vis4Good: Opening + Keynote + 2 Papers</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4good-1760.html">link</a></td></tr><tr><td>358</td><td>Paper | Representing Marginalized Populations: Challenges in Anthropographics</td><td>Priya Dhawka, Helen Ai He, Wesley Willett</td><td></td><td></td><td>Workshop</td><td>Vis4Good: Opening + Keynote + 2 Papers</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4good-3322.html">link</a></td></tr><tr><td>359</td><td>Paper | Data Bricks Space Mission: Teaching Kids about Data with Physicalization</td><td>Lorenzo Ambrosini, Miriah Meyer</td><td></td><td></td><td>Workshop</td><td>Vis4Good: Opening + Keynote + 2 Papers</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4good-6296.html">link</a></td></tr><tr><td>360</td><td>Paper | Ten Challenges and Explainable Analogs of growth functions and distributions for statistical literacy and fluency</td><td>Georges Hattab</td><td></td><td></td><td>Workshop</td><td>Vis4Good: 7 Papers + 1 Poster + Closing</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4good-2457.html">link</a></td></tr><tr><td>361</td><td>How Do Captions Affect Visualization Reading?</td><td>Hazel Zhu, Shelly Cheng, eugene Wu</td><td></td><td></td><td>Workshop</td><td>VisComm: Opening, Presentations, and Late Breaking Work</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-viscomm-1006.html">link</a></td></tr><tr><td>362</td><td>Scoping the Future of Visualization Literacy: A Review</td><td>Mara Solen</td><td></td><td></td><td>Workshop</td><td>VisComm: Opening, Presentations, and Late Breaking Work</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-viscomm-1007.html">link</a></td></tr><tr><td>363</td><td>Negotiating visualization minimalism: a preliminary analysis of Twitter conversations</td><td>Prakash Chandra Shukla, Paul Parsons</td><td></td><td></td><td>Workshop</td><td>VisComm: Opening, Presentations, and Late Breaking Work</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-viscomm-1014.html">link</a></td></tr><tr><td>364</td><td>User Engagement with COVID-19 Visualizations on Twitter</td><td>Robert Kasumba, Saugat Pandey, Vishesh Patel, Micah Wolfson, Alvitta Ottley</td><td></td><td></td><td>Workshop</td><td>VisComm: Opening, Presentations, and Late Breaking Work</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-viscomm-1013.html">link</a></td></tr><tr><td>365</td><td>A Qualitative Evaluation and Taxonomy of Student Annotations on Bar Charts</td><td>Md Dilshadur Rahman, Ghulam Jilani Quadri, Paul Rosen</td><td></td><td></td><td>Workshop</td><td>VisComm: Opening, Presentations, and Late Breaking Work</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-viscomm-1005.html">link</a></td></tr><tr><td>366</td><td>Presentation by Francesca Samsel</td><td>Francesca Samsel, Jung Who Nam, Greg Abram, Mark Petersen</td><td></td><td></td><td>Workshop</td><td>Vis4Climate: Opening, Keynote &amp; Enlightening Session</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4climate-1012.html">link</a></td></tr><tr><td>367</td><td>Presentation by Helen-Nicole Kostis</td><td>AJ Christensen, Helen-Nicole Kostis, Mark SubbaRao, Greg Shirah, Horace Mitchell</td><td></td><td></td><td>Workshop</td><td>Vis4Climate: Opening, Keynote &amp; Enlightening Session</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4climate-1023.html">link</a></td></tr><tr><td>368</td><td>Presentation by Foroozan Danshzand</td><td>Foroozan Daneshzand, Charles Perin, Sheelagh Carpendale</td><td></td><td></td><td>Workshop</td><td>Vis4Climate: Opening, Keynote &amp; Enlightening Session</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4climate-1018.html">link</a></td></tr><tr><td>369</td><td>Presentation by Mark Subbarao</td><td>Mark SubbaRao</td><td></td><td></td><td>Workshop</td><td>Vis4Climate: Opening, Keynote &amp; Enlightening Session</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4climate-1021.html">link</a></td></tr><tr><td>370</td><td>Presentation by Jason Leigh</td><td>Jason Leigh, Mahdi Belcaid, Ryan Theriot, Nurit Kirshenbaum, Roderick S Tabalba Jr., Michael L. Rogers, Eva Moralez Peres, Kari Noe</td><td></td><td></td><td>Workshop</td><td>Vis4Climate: Opening, Keynote &amp; Enlightening Session</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4climate-1025.html">link</a></td></tr><tr><td>371</td><td>Presentation by Everardo Gonzalez</td><td>Valentin Buck, Flemming Stäbler, Everardo González, Christian Scharun</td><td></td><td></td><td>Workshop</td><td>Vis4Climate: Opening, Keynote &amp; Enlightening Session</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4climate-1009.html">link</a></td></tr><tr><td>372</td><td>Presentation by Kalina Borkiewicz</td><td>Kalina Borkiewicz, Stuart Levy, Jeffrey D Carpenter, Donna Cox, Robert Patterson, AJ Christensen</td><td></td><td></td><td>Workshop</td><td>Vis4Climate: Opening, Keynote &amp; Enlightening Session</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4climate-1019.html">link</a></td></tr><tr><td>373</td><td>Presentation by Daniel Sauter</td><td>Daniel Sauter, Timon McPhearson, Xinyue Peng, Christopher Kennedy</td><td></td><td></td><td>Workshop</td><td>Vis4Climate: Opening, Keynote &amp; Enlightening Session</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4climate-1006.html">link</a></td></tr><tr><td>374</td><td>Presentation by Marta Fereira</td><td>Marta Ferreira, Valentina Nisi, Nuno Jardim Nunes</td><td></td><td></td><td>Workshop</td><td>Vis4Climate: Opening, Keynote &amp; Enlightening Session</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4climate-1027.html">link</a></td></tr><tr><td>375</td><td>Presentation by Antoine Bertin</td><td>Antoine Bertin</td><td></td><td></td><td>Workshop</td><td>Vis4Climate: Opening, Keynote &amp; Enlightening Session</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4climate-1031.html">link</a></td></tr><tr><td>376</td><td>Presentation by Kel Elkins</td><td>Kel Elkins</td><td></td><td></td><td>Workshop</td><td>Vis4Climate: Opening, Keynote &amp; Enlightening Session</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4climate-1004.html">link</a></td></tr><tr><td>377</td><td>Presentation by Jason Haga</td><td>Jason Haga, Jason Leigh, Mores Prachyabrued</td><td></td><td></td><td>Workshop</td><td>Vis4Climate: Opening, Keynote &amp; Enlightening Session</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4climate-1026.html">link</a></td></tr><tr><td>378</td><td>Presentation by Cristina Tarquini</td><td>Cristina Tarquini</td><td></td><td></td><td>Workshop</td><td>Vis4Climate: Opening, Keynote &amp; Enlightening Session</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-vis4climate-1033.html">link</a></td></tr><tr><td>379</td><td>Why More Text is (Often) Better: Themes from Reader Preferences for Integration of Charts and Text</td><td>Chase Stokes, Marti Hearst</td><td></td><td></td><td>Workshop</td><td>NLVIZ: Opening, Keynote and Paper Session I</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-nlvis-1003.html">link</a></td></tr><tr><td>380</td><td>Using Large Language Models to Generate Engaging Captions for Data Visualizations</td><td>Ashley Liew, Klaus Mueller</td><td></td><td></td><td>Workshop</td><td>NLVIZ: Opening, Keynote and Paper Session I</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-nlvis-1014.html">link</a></td></tr><tr><td>381</td><td>Visualizing Suicide Risk Prediction in Clinical Language Models</td><td>Filip Dabek, Tim Oates, Peter Hoover, Jesus Caban</td><td></td><td></td><td>Workshop</td><td>NLVIZ: Paper Session II, Discussion and Closing</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-nlvis-1009.html">link</a></td></tr><tr><td>382</td><td>Summarizing text to embed qualitative data into visualizations</td><td>Richard Brath</td><td></td><td></td><td>Workshop</td><td>NLVIZ: Paper Session II, Discussion and Closing</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-nlvis-1006.html">link</a></td></tr><tr><td>383</td><td>WordStream Maker: A Lightweight End-to-end Visualization Platform for Qualitative Time-series Data</td><td>Huyen N. Nguyen, Tommy Dang, Kathleen A. Bowe</td><td></td><td></td><td>Workshop</td><td>NLVIZ: Paper Session II, Discussion and Closing</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-nlvis-1005.html">link</a></td></tr><tr><td>384</td><td>Towards Interactively Contextualizing Natural Language Input in Data Visualization Tools</td><td>Marcel Ruoff, Brad Myers, Alexander Maedche</td><td></td><td></td><td>Workshop</td><td>NLVIZ: Paper Session II, Discussion and Closing</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-nlvis-1004.html">link</a></td></tr><tr><td>385</td><td>NL2INTERFACE: Interactive Visualization Interface Generation from Natural Language Queries</td><td>Yiru Chen, Ryan Li, Austin Mac, Tianbao Xie, Tao Yu, eugene Wu</td><td></td><td></td><td>Workshop</td><td>NLVIZ: Paper Session II, Discussion and Closing</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-nlvis-1011.html">link</a></td></tr><tr><td>386</td><td>Speech-based Data Exploration for Diabetes Management among Older Adults</td><td>Vedangi Bhavsar, Utkarsha Nehe, Aditya Mandke, Srushti Sardeshmukh, Alark Joshi</td><td></td><td></td><td>Workshop</td><td>NLVIZ: Paper Session II, Discussion and Closing</td><td></td><td></td><td><a href="https://virtual.ieeevis.org/year/2022/paper_w-nlvis-1015.html">link</a></td></tr></table>